{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Learning Rate Scheduling\n",
    ":label:`sec_scheduler`\n",
    "\n",
    "So far we primarily focused on optimization *algorithms* for how to update the weight vectors rather than on the *rate* at which they are being updated. Nonetheless, adjusting the learning rate is often just as important as the actual algorithm. There are a number of aspects to consider:\n",
    "\n",
    "* Most obviously the *magnitude* of the learning rate matters. If it is too large, optimization diverges, if it is too small, it takes too long to train or we end up with a suboptimal result. We saw previously that the condition number of the problem matters (see e.g., :numref:`sec_momentum` for details). Intuitively it is the ratio of the amount of change in the least sensitive direction vs. the most sensitive one.\n",
    "* Secondly, the rate of decay is just as important. If the learning rate remains large we may simply end up bouncing around the minimum and thus not reach optimality. :numref:`sec_minibatch_sgd` discussed this in some detail and we analyzed performance guarantees in :numref:`sec_sgd`. In short, we want the rate to decay, but probably more slowly than $\\mathcal{O}(t^{-\\frac{1}{2}})$ which would be a good choice for convex problems.\n",
    "* Another aspect that is equally important is *initialization*. This pertains both to how the parameters are set initially (review :numref:`sec_numerical_stability` for details) and also how they evolve initially. This goes under the moniker of *warmup*, i.e., how rapidly we start moving towards the solution initially. Large steps in the beginning might not be beneficial, in particular since the initial set of parameters is random. The initial update directions might be quite meaningless, too.\n",
    "* Lastly, there are a number of optimization variants that perform cyclical learning rate adjustment. This is beyond the scope of the current chapter. We recommend the reader to review details in :cite:`Izmailov.Podoprikhin.Garipov.ea.2018`, e.g., how to obtain better solutions by averaging over an entire *path* of parameters.\n",
    "\n",
    "Given the fact that there is a lot of detail needed to manage learning rates, most deep learning frameworks have tools to deal with this automatically. In the current chapter we will review the effects that different schedules have on accuracy and also show how this can be managed efficiently via a *learning rate scheduler*. \n",
    "\n",
    "In DJL we will be referring to these as learning rate trackers.\n",
    "\n",
    "## Toy Problem\n",
    "\n",
    "We begin with a toy problem that is cheap enough to compute easily, yet sufficiently nontrivial to illustrate some of the key aspects. For that we pick a slightly modernized version of LeNet (`relu` instead of `sigmoid` activation, MaxPooling rather than AveragePooling), as applied to Fashion-MNIST. Moreover, we hybridize the network for performance. Since most of the code is standard we just introduce the basics without further detailed discussion. See :numref:`chap_cnn` for a refresher as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"9Mwikf\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "       if(!window.letsPlotCallQueue) {\n",
       "           window.letsPlotCallQueue = [];\n",
       "       }; \n",
       "       window.letsPlotCall = function(f) {\n",
       "           window.letsPlotCallQueue.push(f);\n",
       "       };\n",
       "       (function() {\n",
       "           var script = document.createElement(\"script\");\n",
       "           script.type = \"text/javascript\";\n",
       "           script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.4.0/js-package/distr/lets-plot.min.js\";\n",
       "           script.onload = function() {\n",
       "               window.letsPlotCall = function(f) {f();};\n",
       "               window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "               window.letsPlotCallQueue = [];\n",
       "               \n",
       "               \n",
       "           };\n",
       "           script.onerror = function(event) {\n",
       "               window.letsPlotCall = function(f) {};\n",
       "               window.letsPlotCallQueue = [];\n",
       "               var div = document.createElement(\"div\");\n",
       "               div.style.color = 'darkred';\n",
       "               div.textContent = 'Error loading Lets-Plot JS';\n",
       "               document.getElementById(\"9Mwikf\").appendChild(div);\n",
       "           };\n",
       "           var e = document.getElementById(\"9Mwikf\");\n",
       "           e.appendChild(script);\n",
       "       })();\n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%use @file[../djl.json]\n",
    "%use lets-plot\n",
    "@file:DependsOn(\"../D2J-1.0-SNAPSHOT.jar\")\n",
    "//import jp.live.ugai.d2j.attention.Chap10Utils\n",
    "import jp.live.ugai.d2j.util.GradDescUtils.plotGammas\n",
    "import jp.live.ugai.d2j.util.GradDescUtils.train2d\n",
    "import jp.live.ugai.d2j.util.GradDescUtils.showTrace2d\n",
    "import jp.live.ugai.d2j.util.TrainingChapter11.getDataCh11\n",
    "import jp.live.ugai.d2j.util.TrainingChapter11.plotLossEpoch\n",
    "import jp.live.ugai.d2j.util.TrainingChapter11.trainCh11\n",
    "import jp.live.ugai.d2j.util.TrainingChapter11.trainConciseCh11\n",
    "import jp.live.ugai.d2j.util.LossTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ai.djl.basicdataset.cv.classification.*;\n",
    "import ai.djl.metric.Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialBlock {\n",
       "\tConv2d\n",
       "\tReLU\n",
       "\tmaxPool2d\n",
       "\tConv2d\n",
       "\tbatchFlatten\n",
       "\tReLU\n",
       "\tLinear\n",
       "\tReLU\n",
       "\tLinear\n",
       "\tReLU\n",
       "\tLinear\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val net = SequentialBlock();\n",
    "\n",
    "net.add(Conv2d.builder()\n",
    "        .setKernelShape(Shape(5, 5))\n",
    "        .optPadding(Shape(2, 2))\n",
    "        .setFilters(1)\n",
    "        .build());\n",
    "net.add(Activation.reluBlock());\n",
    "net.add(Pool.maxPool2dBlock(Shape(2, 2), Shape(2, 2)));\n",
    "net.add(Conv2d.builder()\n",
    "        .setKernelShape(Shape(5, 5))\n",
    "        .setFilters(1)\n",
    "        .build());\n",
    "net.add(Blocks.batchFlattenBlock());\n",
    "net.add(Activation.reluBlock());\n",
    "net.add(Linear.builder().setUnits(120).build());\n",
    "net.add(Activation.reluBlock());\n",
    "net.add(Linear.builder().setUnits(84).build());\n",
    "net.add(Activation.reluBlock());\n",
    "net.add(Linear.builder().setUnits(10).build());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val batchSize = 256;\n",
    "val trainDataset = FashionMnist.builder()\n",
    "        .optUsage(Dataset.Usage.TRAIN)\n",
    "        .setSampling(batchSize, false)\n",
    "        .optLimit(Long.MAX_VALUE)\n",
    "        .build();\n",
    "\n",
    "val testDataset = FashionMnist.builder()\n",
    "        .optUsage(Dataset.Usage.TEST)\n",
    "        .setSampling(batchSize, false)\n",
    "        .optLimit(Long.MAX_VALUE)\n",
    "        .build();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "var trainLoss = listOf<Double>()\n",
    "var testAccuracy = listOf<Double>()\n",
    "var epochCount= listOf<Int>()\n",
    "var trainAccuracy= listOf<Double>()\n",
    "\n",
    "fun train(trainIter:RandomAccessDataset , testIter:RandomAccessDataset ,\n",
    "                             numEpochs: Int, trainer:Trainer) {\n",
    "    epochCount = List<Int>(numEpochs) { it+1 }\n",
    "    var avgTrainTimePerEpoch = 0.0\n",
    "    val evaluatorMetrics = mutableMapOf<String, List<Double>>()\n",
    "\n",
    "    trainer.setMetrics(Metrics());\n",
    "\n",
    "    EasyTrain.fit(trainer, numEpochs, trainIter, testIter);\n",
    "\n",
    "    var metrics = trainer.getMetrics();\n",
    "\n",
    "    trainer.getEvaluators().forEach{ evaluator -> \n",
    "                evaluatorMetrics.put(\"train_epoch_\" + evaluator.getName(), metrics.getMetric(\"train_epoch_\" + evaluator.getName()).map { it.value });\n",
    "                evaluatorMetrics.put(\"validate_epoch_\" + evaluator.getName(), metrics.getMetric(\"validate_epoch_\" + evaluator.getName()).map { it.value });\n",
    "            }\n",
    "\n",
    "    avgTrainTimePerEpoch = metrics.mean(\"epoch\");\n",
    "\n",
    "    trainLoss = evaluatorMetrics.get(\"train_epoch_SoftmaxCrossEntropyLoss\")!!\n",
    "    trainAccuracy = evaluatorMetrics!!.get(\"train_epoch_Accuracy\")!!\n",
    "    testAccuracy = evaluatorMetrics.get(\"validate_epoch_Accuracy\")!!\n",
    "\n",
    "    print(\"loss %.3f,\".format(trainLoss[numEpochs-1]))\n",
    "    print(\" train acc %.3f,\".format(trainAccuracy[numEpochs-1]))\n",
    "    println(\" test acc %.3f\".format(testAccuracy[numEpochs-1]))\n",
    "    println(\"%.1f examples/sec \".format(trainIter.size() / (avgTrainTimePerEpoch / Math.pow(10.0, 9.0))))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 2
   },
   "source": [
    "Let us have a look at what happens if we invoke this algorithm with default settings, such as a learning rate of $0.3$ and train for $30$ iterations. Note how the training accuracy keeps on increasing while progress in terms of test accuracy stalls beyond a point. The gap between both curves indicates overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.62, SoftmaxCrossEntropyLoss: 1.03\n",
      "Validating:  100% |████████████████████████████████████████|�███████                        |�████████              |ing:   89% |████████████████████████████████████    |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.79, SoftmaxCrossEntropyLoss: 0.56\n",
      "Validating:  100% |████████████████████████████████████████|��█████████████████████████████████     |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.82, SoftmaxCrossEntropyLoss: 0.49\n",
      "Validating:  100% |████████████████████████████████████████|█                   |��██████████████        |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.84, SoftmaxCrossEntropyLoss: 0.45\n",
      "Validating:  100% |████████████████████████████████████████|█                   |█████████████████████       |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.85, SoftmaxCrossEntropyLoss: 0.42\n",
      "Validating:  100% |████████████████████████████████████████||█████████████████████████████           |��██████   |\n",
      "Training:     31% |███████�█████                           | Accuracy: 0.85, SoftmaxCrossEntropyLoss: 0.41"
     ]
    }
   ],
   "source": [
    "val lr = 0.3f;\n",
    "val numEpochs = Integer.getInteger(\"MAX_EPOCH\", 10);\n",
    "\n",
    "val model = Model.newInstance(\"Modern LeNet\");\n",
    "model.setBlock(net);\n",
    "\n",
    "val loss = Loss.softmaxCrossEntropyLoss();\n",
    "val lrt = Tracker.fixed(lr);\n",
    "val sgd = Optimizer.sgd().setLearningRateTracker(lrt).build();\n",
    "\n",
    "val config = DefaultTrainingConfig(loss)\n",
    "        .optOptimizer(sgd) // Optimizer\n",
    "        .addEvaluator(Accuracy()) // Model Accuracy\n",
    "        .addTrainingListeners(*TrainingListener.Defaults.logging()); // Logging\n",
    "\n",
    "val trainer = model.newTrainer(config);\n",
    "trainer.initialize(Shape(1, 1, 28, 28));\n",
    "\n",
    "train(trainDataset, testDataset, numEpochs, trainer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"uUXBzc\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"y0\":[1.0269638,0.5645186,0.48789558,0.44628236,0.4202798,0.40463284,0.3886921,0.37738147,0.367311,0.35570827],\n",
       "\"x\":[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0],\n",
       "\"y1\":[0.6244,0.79191667,0.81983334,0.83503336,0.8462333,0.8516833,0.85723335,0.8603333,0.86401665,0.86843336],\n",
       "\"y2\":[0.7157,0.7913,0.8022,0.8131,0.801,0.8225,0.8217,0.8192,0.828,0.8184]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":400.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y0\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"red\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y1\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"blue\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y2\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"green\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"uUXBzc\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = mapOf(\"x\" to epochCount, \"y0\" to trainLoss, \"y1\" to trainAccuracy, \"y2\" to testAccuracy)\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine(color=\"red\") { x = \"x\"; y=\"y0\"}\n",
    "plot += geomLine(color=\"blue\") { x = \"x\" ; y=\"y1\"}\n",
    "plot += geomLine(color=\"green\") { x= \"x\" ; y=\"y2\"}\n",
    "plot + ggsize(500,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## Trackers\n",
    "\n",
    "One way of adjusting the learning rate is to set it explicitly at each step. We could adjust it downward after every epoch (or even after every minibatch), e.g., in a dynamic manner in response to how optimization is progressing. \n",
    "\n",
    "We, however, can't directly change the learning rate with the trainer after it has already been created. What we can do instead is create a tracker to do this for us.\n",
    "\n",
    "When invoked with the number of updates it returns the appropriate value of the learning rate. Let us define a simple one that sets the learning rate to $\\eta = \\eta_0 (t + 1)^{-\\frac{1}{2}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "class SquareRootTracker {\n",
    "    var lr : Float\n",
    "    constructor() {\n",
    "        lr = 0.1f\n",
    "    }\n",
    "    \n",
    "    constructor(learningRate: Float) {\n",
    "        lr = learningRate;        \n",
    "    }\n",
    "    fun getNewLearningRate(numUpdate: Int): Float {\n",
    "        return lr * Math.pow(numUpdate.toDouble() + 1, -0.5).toFloat()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This is not a drop in replacement for a standard Learning Rate Tracker (LRT). \n",
    "This is just a simple example to give a better understanding of how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "Let us plot its behavior over a range of values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"Nj4CJy\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"y0\":[0.10000000149011612,0.0707106813788414,0.05773502588272095,0.05000000074505806,0.04472136124968529,0.040824830532073975,0.03779644891619682,0.0353553406894207,0.03333333507180214,0.03162277862429619],\n",
       "\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":400.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y0\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"red\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"Nj4CJy\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tracker = SquareRootTracker()\n",
    "\n",
    "val epochs = List<Int>(numEpochs) { it }\n",
    "val learningRates = mutableListOf<Float>()\n",
    "for (i in 0 until numEpochs) {\n",
    "    learningRates.add(tracker.getNewLearningRate(i))\n",
    "}\n",
    "\n",
    "val data = mapOf(\"x\" to epochs, \"y0\" to  learningRates)\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine(color=\"red\") { x = \"x\"; y=\"y0\"}\n",
    "plot + ggsize(500,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "Now let us see how this plays out for training on Fashion-MNIST. We can't actually do it directly, but we can see how the curve would look theoretically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "This looks like it works quite a bit better than before. Two things stand out: the curve was rather more smooth than previously. Secondly, there was less overfitting. Unfortunately it is not a well-resolved question as to why certain strategies lead to less overfitting in *theory*. There is some argument that a smaller stepsize will lead to parameters that are closer to zero and thus simpler. However, this does not explain the phenomenon entirely since we do not really stop early but simply reduce the learning rate gently.\n",
    "\n",
    "## Policies\n",
    "\n",
    "While we cannot possibly cover the entire variety of learning rate trackers, we attempt to give a brief overview of popular policies below. Common choices are polynomial decay and piecewise constant schedules. Beyond that, cosine learning rate schedules have been found to work well empirically on some problems. Lastly, on some problems it is beneficial to warm up the optimizer prior to using large learning rates.\n",
    "\n",
    "### Factor Tracker\n",
    "\n",
    "One alternative to a polynomial decay would be a multiplicative one, that is $\\eta_{t+1} \\leftarrow \\eta_t \\cdot \\alpha$ for $\\alpha \\in (0, 1)$. To prevent the learning rate from decaying beyond a reasonable lower bound the update equation is often modified to $\\eta_{t+1} \\leftarrow \\mathop{\\mathrm{max}}(\\eta_{\\mathrm{min}}, \\eta_t \\cdot \\alpha)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "origin_pos": 13,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "class DemoFactorTracker {\n",
    "    val baseLr: Float\n",
    "    val stopFactorLr: Float\n",
    "    val factor: Float\n",
    "    constructor(factor: Float, stopFactorLr: Float, baseLr: Float) {\n",
    "        this.factor = factor;\n",
    "        this.stopFactorLr = stopFactorLr;\n",
    "        this.baseLr = baseLr;\n",
    "    }\n",
    "    constructor() {\n",
    "        factor = 1f\n",
    "        stopFactorLr =1e-7f\n",
    "        baseLr = 0.1f\n",
    "    }\n",
    "    fun getNewLearningRate(numUpdate: Int): Float {\n",
    "        return lr * Math.pow(numUpdate.toDouble() + 1, -0.5).toFloat()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"JJJqWc\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"y0\":[0.30000001192092896,0.2121320366859436,0.17320507764816284,0.15000000596046448,0.13416408002376556,0.12247449904680252,0.11338934302330017,0.1060660183429718,0.10000000894069672,0.09486833214759827,0.09045340865850449,0.08660253882408142,0.08320502936840057,0.08017837256193161,0.0774596706032753,0.07500000298023224,0.0727606862783432,0.0707106813788414,0.06882472336292267,0.06708204001188278,0.06546536833047867,0.06396021693944931,0.06255432218313217,0.06123724952340126,0.06000000238418579,0.05883484333753586,0.057735029608011246,0.056694671511650085,0.05570860207080841,0.054772257804870605,0.05388159304857254,0.0530330091714859,0.05222329869866371,0.05144957825541496,0.050709255039691925,0.05000000447034836,0.04931969568133354,0.04866642877459526,0.04803844541311264,0.04743416607379913,0.04685213044285774,0.046291008591651917,0.04574957489967346,0.04522670432925224,0.04472136124968529,0.04423258826136589,0.04375949874520302,0.04330126941204071,0.04285714775323868,0.04242641106247902],\n",
       "\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":400.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y0\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"red\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"JJJqWc\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tracker = DemoFactorTracker(0.9f, 1e-2f, 2f)\n",
    "\n",
    "val numEpochs = 50\n",
    "val epochs = List<Int>(numEpochs) { it }\n",
    "val learningRates = mutableListOf<Float>()\n",
    "for (i in 0 until numEpochs) {\n",
    "    learningRates.add(tracker.getNewLearningRate(i))\n",
    "}\n",
    "\n",
    "val data = mapOf(\"x\" to epochs, \"y0\" to  learningRates)\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine(color=\"red\") { x = \"x\"; y=\"y0\"}\n",
    "plot + ggsize(500,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "This can also be accomplished by a built-in scheduler in DJL via the `LearningRateTracker.factorTracker()` builder. It takes a few more parameters, such as warmup period, warmup mode (linear or constant), the maximum number of desired updates, etc.; Going forward we will use the built-in schedulers as appropriate and only explain their functionality here. \n",
    "\n",
    "### Multi Factor Scheduler\n",
    "\n",
    "A common strategy for training deep networks is to keep the learning rate piecewise constant and to decrease it by a given amount every so often. That is, given a set of times when to decrease the rate, such as $s = \\{5, 10, 20\\}$ decrease $\\eta_{t+1} \\leftarrow \\eta_t \\cdot \\alpha$ whenever $t \\in s$. Assuming that the values are halved at each step we can implement this as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "origin_pos": 15,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"bfFsyp\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"y0\":[0.5,0.5,0.5,0.5,0.5,0.5,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.125,0.125,0.125,0.125,0.125,0.125,0.125,0.125,0.125,0.125,0.125,0.125,0.125,0.125,0.125,0.125,0.125,0.125,0.125],\n",
       "\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":400.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y0\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"red\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"bfFsyp\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tracker = Tracker.multiFactor()\n",
    "        .setSteps(intArrayOf(5, 30))\n",
    "        .optFactor(0.5f)\n",
    "        .setBaseValue(0.5f)\n",
    "        .build()\n",
    "\n",
    "val numEpochs = 50;\n",
    "val epochs = List<Int>(numEpochs) { it }\n",
    "val learningRates = mutableListOf<Float>()\n",
    "for (i in 0 until numEpochs) {\n",
    "    learningRates.add(tracker.getNewValue(i))\n",
    "}\n",
    "\n",
    "val data = mapOf(\"x\" to epochs, \"y0\" to  learningRates)\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine(color=\"red\") { x = \"x\"; y=\"y0\"}\n",
    "plot + ggsize(500,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "The intuition behind this piecewise constant learning rate schedule is that one lets optimization proceed until a stationary point has been reached in terms of the distribution of weight vectors. Then (and only then) do we decrease the rate such as to obtain a higher quality proxy to a good local minimum. The example below shows how this can produce ever slightly better solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.88, SoftmaxCrossEntropyLoss: 0.32\n",
      "Validating:  100% |████████████████████████████████████████|��████████                  |�████████         |██████████████████ |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.89, SoftmaxCrossEntropyLoss: 0.31\n",
      "Validating:  100% |████████████████████████████████████████|��████████                  |�████████         |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.89, SoftmaxCrossEntropyLoss: 0.30\n",
      "Validating:  100% |████████████████████████████████████████|   |�██████████████████████            |�████████████  |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.89, SoftmaxCrossEntropyLoss: 0.29\n",
      "Validating:  100% |████████████████████████████████████████|�███████                        |�███████████████             |�████████████  |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.89, SoftmaxCrossEntropyLoss: 0.29\n",
      "Validating:  100% |████████████████████████████████████████|�███████████████             |��██████   |\n",
      "Training:     31% |███████�█████                           | Accuracy: 0.89, SoftmaxCrossEntropyLoss: 0.29"
     ]
    },
    {
     "data": {
      "text/html": [
       "   <div id=\"ykoUgC\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"y0\":[0.3150133,0.30573684,0.29998007,0.29469523,0.28994837,0.28555128,0.28127804,0.27750134,0.2734621,0.26951915],\n",
       "\"x\":[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0],\n",
       "\"y1\":[0.88276666,0.88558334,0.88848335,0.8902,0.89171666,0.8930167,0.8954833,0.89641666,0.89815,0.8999],\n",
       "\"y2\":[0.85,0.8491,0.8497,0.8475,0.8497,0.8474,0.8469,0.8462,0.8441,0.8439]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":400.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y0\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"red\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y1\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"blue\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y2\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"green\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"ykoUgC\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numEpochs = 10\n",
    "\n",
    "val model = Model.newInstance(\"Modern LeNet\");\n",
    "model.setBlock(net);\n",
    "\n",
    "val loss = Loss.softmaxCrossEntropyLoss();\n",
    "val sgd = Optimizer.sgd().setLearningRateTracker(tracker).build();\n",
    "\n",
    "val config = DefaultTrainingConfig(loss)\n",
    "        .optOptimizer(sgd) // Optimizer\n",
    "        .addEvaluator(Accuracy()) // Model Accuracy\n",
    "        .addTrainingListeners(*TrainingListener.Defaults.logging()); // Logging\n",
    "\n",
    "val trainer = model.newTrainer(config);\n",
    "trainer.initialize(Shape(1, 1, 28, 28));\n",
    "\n",
    "train(trainDataset, testDataset, numEpochs, trainer);\n",
    "val data = mapOf(\"x\" to epochCount, \"y0\" to trainLoss, \"y1\" to trainAccuracy, \"y2\" to testAccuracy)\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine(color=\"red\") { x = \"x\"; y=\"y0\"}\n",
    "plot += geomLine(color=\"blue\") { x = \"x\" ; y=\"y1\"}\n",
    "plot += geomLine(color=\"green\") { x= \"x\" ; y=\"y2\"}\n",
    "plot + ggsize(500,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"PikeY7\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"y0\":[0.3150133,0.30573684,0.29998007,0.29469523,0.28994837,0.28555128,0.28127804,0.27750134,0.2734621,0.26951915],\n",
       "\"x\":[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0],\n",
       "\"y1\":[0.88276666,0.88558334,0.88848335,0.8902,0.89171666,0.8930167,0.8954833,0.89641666,0.89815,0.8999],\n",
       "\"y2\":[0.85,0.8491,0.8497,0.8475,0.8497,0.8474,0.8469,0.8462,0.8441,0.8439]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":400.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y0\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"red\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y1\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"blue\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y2\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"green\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"PikeY7\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = mapOf(\"x\" to epochCount, \"y0\" to trainLoss, \"y1\" to trainAccuracy, \"y2\" to testAccuracy)\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine(color=\"red\") { x = \"x\"; y=\"y0\"}\n",
    "plot += geomLine(color=\"blue\") { x = \"x\" ; y=\"y1\"}\n",
    "plot += geomLine(color=\"green\") { x= \"x\" ; y=\"y2\"}\n",
    "plot + ggsize(500,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "### Cosine Tracker\n",
    "\n",
    "A rather perplexing heuristic was proposed by :cite:`Loshchilov.Hutter.2016`. It relies on the observation that we might not want to decrease the learning rate too drastically in the beginning and moreover, that we might want to \"refine\" the solution in the end using a very small learning rate. This results in a cosine-like tracker with the following functional form for learning rates in the range $t \\in [0, T]$.\n",
    "\n",
    "$$\\eta_t = \\eta_T + \\frac{\\eta_0 - \\eta_T}{2} \\left(1 + \\cos(\\pi t/T)\\right)$$\n",
    "\n",
    "Here $\\eta_0$ is the initial learning rate, $\\eta_T$ is the target rate at time $T$. Furthermore, for $t > T$ we simply pin the value to $\\eta_T$ without increasing it again. In the following example, we set the max update step $T = 20$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoCosineTracker {\n",
    "    var baseLr : Float\n",
    "    var finalLr : Float\n",
    "    var maxUpdate : Int\n",
    "    constructor() {\n",
    "        baseLr = 0.5f\n",
    "        finalLr =0.01f\n",
    "        maxUpdate =20\n",
    "    }\n",
    "    constructor(baseLr: Float, finalLr: Float, maxUpdate: Int) {\n",
    "        this.baseLr = baseLr;\n",
    "        this.finalLr = finalLr;\n",
    "        this.maxUpdate = maxUpdate;\n",
    "    }\n",
    "    fun getNewLearningRate(numUpdate: Int): Float {\n",
    "        if (numUpdate > maxUpdate) {\n",
    "            return finalLr;\n",
    "        }\n",
    "        // Scale the curve to smoothly transition\n",
    "        val step = (baseLr - finalLr) / 2 * (1 + Math.cos(Math.PI * numUpdate / maxUpdate));\n",
    "        return finalLr + step.toFloat()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "origin_pos": 19,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"jXfToM\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"y0\":[0.5,0.4969836473464966,0.48800885677337646,0.4732966125011444,0.45320916175842285,0.42824116349220276,0.3990073800086975,0.3662276566028595,0.33070915937423706,0.2933264374732971,0.2549999952316284,0.2166735678911209,0.17929084599018097,0.14377233386039734,0.11099261045455933,0.08175883442163467,0.05679083615541458,0.03670340031385422,0.021991152316331863,0.013016356155276299,0.009999999776482582,0.009999999776482582,0.009999999776482582,0.009999999776482582,0.009999999776482582,0.009999999776482582,0.009999999776482582,0.009999999776482582,0.009999999776482582,0.009999999776482582],\n",
       "\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":400.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y0\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"red\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"jXfToM\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tracker = DemoCosineTracker(0.5f, 0.01f, 20);\n",
    "\n",
    "val numEpochs = 30\n",
    "val epochs = List<Int>(numEpochs) { it }\n",
    "val learningRates = List<Float>(numEpochs) { tracker.getNewLearningRate(it) }\n",
    "val data = mapOf(\"x\" to epochs, \"y0\" to  learningRates)\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine(color=\"red\") { x = \"x\"; y=\"y0\"}\n",
    "plot + ggsize(500,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "In the context of computer vision this schedule *can* lead to improved results. Note, though, that such improvements are not guaranteed (as can be seen below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "origin_pos": 21,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.85, SoftmaxCrossEntropyLoss: 0.43\n",
      "Validating:  100% |████████████████████████████████████████|   |��█████████████████████████████████     |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.88, SoftmaxCrossEntropyLoss: 0.34\n",
      "Validating:  100% |████████████████████████████████████████|��████████                  |�████████         |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.89, SoftmaxCrossEntropyLoss: 0.31\n",
      "Validating:  100% |████████████████████████████████████████|�████████              |█    |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.89, SoftmaxCrossEntropyLoss: 0.29\n",
      "Validating:  100% |████████████████████████████████████████|�███████                        |�███████████████             |�████████████  |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.89, SoftmaxCrossEntropyLoss: 0.28\n",
      "Validating:  100% |████████████████████████████████████████|�██                      ||█████████████████████████████           |��███████████████████████|\n",
      "Training:     31% |███████�█████                           | Accuracy: 0.90, SoftmaxCrossEntropyLoss: 0.28"
     ]
    },
    {
     "data": {
      "text/html": [
       "   <div id=\"BGftGL\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"y0\":[0.42666405,0.3354828,0.30863822,0.29352456,0.28359166,0.2763648,0.27100238,0.26682925,0.2633914,0.26041993],\n",
       "\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0],\n",
       "\"y1\":[0.8464,0.87553334,0.8851,0.89068335,0.89491665,0.8975,0.89898336,0.90033334,0.9017,0.90283334],\n",
       "\"y2\":[0.8391,0.8469,0.8501,0.8539,0.8559,0.8579,0.8582,0.8576,0.8582,0.8584]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":400.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y0\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"red\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y1\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"blue\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y2\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"green\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"BGftGL\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cosineTracker = Tracker.cosine()\n",
    "                            .setBaseValue(0.5f)\n",
    "                            .optFinalValue(0.01f)\n",
    "                            .setMaxUpdates(20)\n",
    "                            .build();\n",
    "\n",
    "val loss = Loss.softmaxCrossEntropyLoss();\n",
    "val sgd = Optimizer.sgd().setLearningRateTracker(cosineTracker).build();\n",
    "\n",
    "val config = DefaultTrainingConfig(loss)\n",
    "        .optOptimizer(sgd) // Optimizer\n",
    "        .addEvaluator(Accuracy()) // Model Accuracy\n",
    "        .addTrainingListeners(*TrainingListener.Defaults.logging()); // Logging\n",
    "\n",
    "val trainer = model.newTrainer(config);\n",
    "trainer.initialize(Shape(1, 1, 28, 28));\n",
    "\n",
    "val numEpochs = 10\n",
    "val epochs = List<Int>(numEpochs) { it }\n",
    "train(trainDataset, testDataset, numEpochs, trainer)\n",
    "val data = mapOf(\"x\" to epochs, \"y0\" to trainLoss, \"y1\" to trainAccuracy, \"y2\" to testAccuracy)\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine(color=\"red\") { x = \"x\"; y=\"y0\"}\n",
    "plot += geomLine(color=\"blue\") { x = \"x\" ; y=\"y1\"}\n",
    "plot += geomLine(color=\"green\") { x= \"x\" ; y=\"y2\"}\n",
    "plot + ggsize(500,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "### Warmup\n",
    "\n",
    "In some cases initializing the parameters is not sufficient to guarantee a good solution. This particularly a problem for some advanced network designs that may lead to unstable optimization problems. We could address this by choosing a sufficiently small learning rate to prevent divergence in the beginning. Unfortunately this means that progress is slow. Conversely, a large learning rate initially leads to divergence.\n",
    "\n",
    "A rather simple fix for this dilemma is to use a warmup period during which the learning rate *increases* to its initial maximum and to cool down the rate until the end of the optimization process. For simplicity one typically uses a linear increase for this purpose. This leads to a schedule of the form indicated below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupTracker {\n",
    "    var baseLr: Float\n",
    "    var finalLr: Float\n",
    "    var maxUpdate: Int\n",
    "    var warmUpSteps : Int\n",
    "    var warmUpBeginValue : Float = 0.0f\n",
    "    var warmUpFinalValue : Float = 0.0f\n",
    "    \n",
    "    constructor() {\n",
    "        baseLr = 0.5f\n",
    "        finalLr = 0.01f\n",
    "        maxUpdate = 20\n",
    "        warmUpSteps =5\n",
    "    }\n",
    "    \n",
    "    constructor (baseLr: Float, finalLr: Float, maxUpdate: Int, warmUpSteps: Int) {\n",
    "        this.baseLr = baseLr;\n",
    "        this.finalLr = finalLr;\n",
    "        this.maxUpdate = maxUpdate;\n",
    "        this.warmUpSteps = 5;\n",
    "        this.warmUpBeginValue = 0f;\n",
    "    }\n",
    "    \n",
    "    fun getNewLearningRate(numUpdate: Int): Float {\n",
    "        if (numUpdate <= warmUpSteps) {\n",
    "            return getWarmUpValue(numUpdate);\n",
    "        }\n",
    "        if (numUpdate > maxUpdate) {\n",
    "            return finalLr;\n",
    "        }\n",
    "        // Scale the cosine curve to fit smoothly with the warmup steps\n",
    "        var step = (baseLr - finalLr) / 2 * (1 + \n",
    "            Math.cos(Math.PI * (numUpdate - warmUpSteps) / (maxUpdate - warmUpSteps)));\n",
    "        return finalLr + step.toFloat();\n",
    "    }\n",
    "    \n",
    "    fun getWarmUpValue(numUpdate: Int): Float {\n",
    "        // Linear warmup\n",
    "        return warmUpBeginValue + (baseLr - warmUpBeginValue) * numUpdate / warmUpSteps;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "origin_pos": 23,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"CMEris\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"y0\":[0.0,0.10000000149011612,0.20000000298023224,0.30000001192092896,0.4000000059604645,0.5,0.4946461617946625,0.4788186252117157,0.45320916175842285,0.41893699765205383],\n",
       "\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":400.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y0\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"red\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"CMEris\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tracker = CosineWarmupTracker(0.5f, 0.01f, 20, 5);\n",
    "\n",
    "val epochs = List<Int>(numEpochs) { it }\n",
    "val learningRates = List<Float>(numEpochs) { tracker.getNewLearningRate(it) }\n",
    "val data = mapOf(\"x\" to epochs, \"y0\" to  learningRates)\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine(color=\"red\") { x = \"x\"; y=\"y0\"}\n",
    "plot + ggsize(500,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "Note that the network converges better initially (in particular observe the performance during the first 5 epochs).\n",
    "\n",
    "Additionally, we still use a total of 20 max updates, but the 1st\n",
    "5 are dedicated to the warmup steps. The cosine curve will then be\n",
    "squeezed into the 15 steps relative to the earlier 20 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "origin_pos": 25,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.80, SoftmaxCrossEntropyLoss: 0.56\n",
      "Validating:  100% |████████████████████████████████████████|�█               |��█████████████████████████████████     |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.86, SoftmaxCrossEntropyLoss: 0.39\n",
      "Validating:  100% |████████████████████████████████████████|ting:   35% |███████████████                         |�█               |��█████████████████████████████████     |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.87, SoftmaxCrossEntropyLoss: 0.34\n",
      "Validating:  100% |████████████████████████████████████████|�███████                        |�████████              |��██████   |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.88, SoftmaxCrossEntropyLoss: 0.32\n",
      "Validating:  100% |████████████████████████████████████████||██          |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.89, SoftmaxCrossEntropyLoss: 0.31\n",
      "Validating:  100% |████████████████████████████████████████||�████████         |\n",
      "Training:     31% |███████�█████                           | Accuracy: 0.89, SoftmaxCrossEntropyLoss: 0.30"
     ]
    },
    {
     "data": {
      "text/html": [
       "   <div id=\"xtynuc\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"y0\":[0.5645259,0.38827857,0.34320375,0.32039315,0.30583546,0.29542467,0.28794155,0.28229326,0.2777384,0.2738804],\n",
       "\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0],\n",
       "\"y1\":[0.79936665,0.8581167,0.8728667,0.88138336,0.88575,0.89015,0.89275,0.89488333,0.8965333,0.8980167],\n",
       "\"y2\":[0.8235,0.8382,0.8434,0.8486,0.8502,0.8527,0.855,0.8556,0.8552,0.8568]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":400.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y0\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"red\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y1\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"blue\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y2\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"green\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"xtynuc\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cosineTracker = Tracker.cosine()\n",
    "        .setBaseValue(0.5f)\n",
    "        .optFinalValue(0.01f)\n",
    "        .setMaxUpdates(15)\n",
    "        .build();\n",
    "\n",
    "val warmupCosine = Tracker.warmUp()\n",
    "        .optWarmUpSteps(5)\n",
    "        .setMainTracker(cosineTracker)\n",
    "        .build();\n",
    "\n",
    "val loss = Loss.softmaxCrossEntropyLoss();\n",
    "val sgd = Optimizer.sgd().setLearningRateTracker(warmupCosine).build();\n",
    "\n",
    "val config = DefaultTrainingConfig(loss)\n",
    "        .optOptimizer(sgd) // Optimizer\n",
    "        .addEvaluator(Accuracy()) // Model Accuracy\n",
    "        .addTrainingListeners(*TrainingListener.Defaults.logging()); // Logging\n",
    "\n",
    "val trainer = model.newTrainer(config);\n",
    "trainer.initialize(Shape(1, 1, 28, 28));\n",
    "\n",
    "train(trainDataset, testDataset, numEpochs, trainer);\n",
    "val epochs = List<Int>(numEpochs) { it }\n",
    "val data = mapOf(\"x\" to epochs, \"y0\" to trainLoss, \"y1\" to trainAccuracy, \"y2\" to testAccuracy)\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine(color=\"red\") { x = \"x\"; y=\"y0\"}\n",
    "plot += geomLine(color=\"blue\") { x = \"x\" ; y=\"y1\"}\n",
    "plot += geomLine(color=\"green\") { x= \"x\" ; y=\"y2\"}\n",
    "plot + ggsize(500,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.88, SoftmaxCrossEntropyLoss: 0.33\n",
      "Validating:  100% |████████████████████████████████████████|�███████                        |�████████              |��██████   |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.89, SoftmaxCrossEntropyLoss: 0.29\n",
      "Validating:  100% |████████████████████████████████████████|�███████                        |�███████████████             |��██████   |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.90, SoftmaxCrossEntropyLoss: 0.28\n",
      "Validating:  100% |███████████�████████████████████████████|��████████                  |█████████████████████       |�████████████████████████████|\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.90, SoftmaxCrossEntropyLoss: 0.27\n",
      "Validating:  100% |████████████████████████████████████████|�████████████████                 |█████████████████████       |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.90, SoftmaxCrossEntropyLoss: 0.27\n",
      "Validating:  100% |████████████████████████████████████████|ing:   89% |████████████████████████████████████    |\n",
      "Training:     31% |███████�█████                           | Accuracy: 0.90, SoftmaxCrossEntropyLoss: 0.26"
     ]
    },
    {
     "data": {
      "text/html": [
       "   <div id=\"O1aEcH\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"y0\":[0.3269031,0.29022425,0.2792886,0.2728296,0.2683963,0.26503688,0.26230556,0.2599009,0.25785133,0.25598484],\n",
       "\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0],\n",
       "\"y1\":[0.87813336,0.89203334,0.89556664,0.8979667,0.89945,0.901,0.90218335,0.90323335,0.90398335,0.90473336],\n",
       "\"y2\":[0.8518,0.8541,0.8545,0.8544,0.8555,0.8566,0.8578,0.8583,0.8579,0.858]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":400.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y0\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"red\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y1\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"blue\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y2\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"color\":\"green\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"O1aEcH\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sgd = Optimizer.sgd().setLearningRateTracker(cosineTracker).build();\n",
    "\n",
    "val config = DefaultTrainingConfig(loss)\n",
    "        .optOptimizer(sgd) // Optimizer\n",
    "        .addEvaluator(Accuracy()) // Model Accuracy\n",
    "        .addTrainingListeners(*TrainingListener.Defaults.logging()); // Logging\n",
    "\n",
    "val trainer = model.newTrainer(config);\n",
    "trainer.initialize(Shape(1, 1, 28, 28));\n",
    "\n",
    "train(trainDataset, testDataset, numEpochs, trainer);\n",
    "val epochs = List<Int>(numEpochs) { it }\n",
    "val data = mapOf(\"x\" to epochs, \"y0\" to trainLoss, \"y1\" to trainAccuracy, \"y2\" to testAccuracy)\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine(color=\"red\") { x = \"x\"; y=\"y0\"}\n",
    "plot += geomLine(color=\"blue\") { x = \"x\" ; y=\"y1\"}\n",
    "plot += geomLine(color=\"green\") { x= \"x\" ; y=\"y2\"}\n",
    "plot + ggsize(500,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warmup can be applied to any scheduler (not just cosine). For a more detailed discussion of learning rate schedules and many more experiments see also :cite:Gotmare.Keskar.Xiong.ea.2018. In particular they find that a warmup phase limits the amount of divergence of parameters in very deep networks. This makes intuitively sense since we would expect significant divergence due to random initialization in those parts of the network that take the most time to make progress in the beginning.\n",
    "\n",
    "## Summary\n",
    "* Decreasing the learning rate during training can lead to improved accuracy and (most perplexingly) reduced overfitting of the model.\n",
    "* A piecewise decrease of the learning rate whenever progress has plateaued is effective in practice. Essentially this ensures that we converge efficiently to a suitable solution and only then reduce the inherent variance of the parameters by reducing the learning rate.\n",
    "* Cosine schedulers are popular for some computer vision problems.\\n\",\n",
    "* A warmup period before optimization can prevent divergence.\\n\",\n",
    "* Optimization serves multiple purposes in deep learning. Besides minimizing the training objective, different choices of optimization algorithms and learning rate scheduling can lead to rather different amounts of generalization and overfitting on the test set (for the same amount of training error).\\n\",\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Experiment with the optimization behavior for a given fixed learning rate. What is the best model you can obtain this way?\n",
    "1. How does convergence change if you change the exponent of the decrease in the learning rate?\n",
    "1. Apply the cosine scheduler to large computer vision problems, e.g., training ImageNet. How does it affect performance relative to other schedulers?\n",
    "1. How long should warmup last?\n",
    "1. Can you connect optimization and sampling? Start by using results from :cite:`Welling.Teh.2011` on Stochastic Gradient Langevin Dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
