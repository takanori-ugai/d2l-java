{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Adadelta\n",
    ":label:`sec_adadelta`\n",
    "\n",
    "Adadelta is yet another variant of AdaGrad. The main difference lies in the fact that it decreases the amount by which the learning rate is adaptive to coordinates. Moreover, traditionally it referred to as not having a learning rate since it uses the amount of change itself as calibration for future change. The algorithm was proposed in :cite:`Zeiler.2012`. It is fairly straightforward, given the discussion of previous algorithms so far. \n",
    "\n",
    "## The Algorithm\n",
    "\n",
    "In a nutshell Adadelta uses two state variables, $\\mathbf{s}_t$ to store a leaky average of the second moment of the gradient and $\\Delta\\mathbf{x}_t$ to store a leaky average of the second moment of the change of parameters in the model itself. Note that we use the original notation and naming of the authors for compatibility with other publications and implementations (there is no other real reason why one should use different Greek variables to indicate a parameter serving the same purpose in momentum, Adagrad, RMSProp, and Adadelta). The parameter du jour is $\\rho$. We obtain the following leaky updates:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\mathbf{s}_t & = \\rho \\mathbf{s}_{t-1} + (1 - \\rho) \\mathbf{g}_t^2, \\\\\n",
    "    \\mathbf{g}_t' & = \\sqrt{\\frac{\\Delta\\mathbf{x}_{t-1} + \\epsilon}{\\mathbf{s}_t + \\epsilon}} \\odot \\mathbf{g}_t, \\\\\n",
    "    \\mathbf{x}_t  & = \\mathbf{x}_{t-1} - \\mathbf{g}_t', \\\\\n",
    "    \\Delta \\mathbf{x}_t & = \\rho \\Delta\\mathbf{x}_{t-1} + (1 - \\rho) \\mathbf{x}_t^2.\n",
    "\\end{aligned}$$\n",
    "\n",
    "The difference to before is that we perform updates with the rescaled gradient $\\mathbf{g}_t'$ which is computed by taking the ratio between the average squared rate of change and the average second moment of the gradient. The use of $\\mathbf{g}_t'$ is purely for notational convenience. In practice we can implement this algorithm without the need to use additional temporary space for $\\mathbf{g}_t'$. As before $\\eta$ is a parameter ensuring nontrivial numerical results, i.e., avoiding zero step size or infinite variance. Typically we set this to $\\eta = 10^{-5}$. \n",
    "\n",
    "## Implementation\n",
    "\n",
    "Adadelta needs to maintain two state variables for each variable, $\\mathbf{s}_t$ and $\\Delta\\mathbf{x}_t$. This yields the following implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"UCSG99\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "       if(!window.letsPlotCallQueue) {\n",
       "           window.letsPlotCallQueue = [];\n",
       "       }; \n",
       "       window.letsPlotCall = function(f) {\n",
       "           window.letsPlotCallQueue.push(f);\n",
       "       };\n",
       "       (function() {\n",
       "           var script = document.createElement(\"script\");\n",
       "           script.type = \"text/javascript\";\n",
       "           script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.4.0/js-package/distr/lets-plot.min.js\";\n",
       "           script.onload = function() {\n",
       "               window.letsPlotCall = function(f) {f();};\n",
       "               window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "               window.letsPlotCallQueue = [];\n",
       "               \n",
       "               \n",
       "           };\n",
       "           script.onerror = function(event) {\n",
       "               window.letsPlotCall = function(f) {};\n",
       "               window.letsPlotCallQueue = [];\n",
       "               var div = document.createElement(\"div\");\n",
       "               div.style.color = 'darkred';\n",
       "               div.textContent = 'Error loading Lets-Plot JS';\n",
       "               document.getElementById(\"UCSG99\").appendChild(div);\n",
       "           };\n",
       "           var e = document.getElementById(\"UCSG99\");\n",
       "           e.appendChild(script);\n",
       "       })();\n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%use @file[../djl.json]\n",
    "%use lets-plot\n",
    "@file:DependsOn(\"../D2J-1.0-SNAPSHOT.jar\")\n",
    "//import jp.live.ugai.d2j.attention.Chap10Utils\n",
    "import jp.live.ugai.d2j.util.GradDescUtils.plotGammas\n",
    "import jp.live.ugai.d2j.util.GradDescUtils.train2d\n",
    "import jp.live.ugai.d2j.util.GradDescUtils.showTrace2d\n",
    "import jp.live.ugai.d2j.util.TrainingChapter11.getDataCh11\n",
    "import jp.live.ugai.d2j.util.TrainingChapter11.plotLossEpoch\n",
    "import jp.live.ugai.d2j.util.TrainingChapter11.trainCh11\n",
    "import jp.live.ugai.d2j.util.TrainingChapter11.trainConciseCh11\n",
    "import jp.live.ugai.d2j.util.LossTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun initAdadeltaStates(featureDimension: Int) : NDList {\n",
    "    val manager = NDManager.newBaseManager();\n",
    "    val sW = manager.zeros(Shape(featureDimension.toLong(), 1));\n",
    "    val sB = manager.zeros(Shape(1));\n",
    "    val deltaW = manager.zeros(Shape(featureDimension.toLong(), 1));\n",
    "    val deltaB = manager.zeros(Shape(1));\n",
    "    return NDList(sW, deltaW, sB, deltaB);\n",
    "}\n",
    "\n",
    "object Optimization {\n",
    "    fun adadelta(params: NDList, states: NDList,  hyperparams: Map<String, Float>) {\n",
    "        val rho = hyperparams.get(\"rho\")!!\n",
    "        val eps = 1e-5.toFloat()\n",
    "        for (i in 0 until params.size) {\n",
    "            val param = params.get(i);\n",
    "            val state = states.get(2 * i);\n",
    "            val delta = states.get(2 * i + 1);\n",
    "            // Update parameter, state, and delta\n",
    "            // In-place updates with the '__'i methods (ex. muli)\n",
    "            // state = rho * state + (1 - rho) * param.gradient^2\n",
    "            state.muli(rho).addi(param.getGradient().square().mul(1 - rho));\n",
    "            // rescaledGradient = ((delta + eps)^(1/2) / (state + eps)^(1/2)) * param.gradient\n",
    "           val rescaledGradient = delta.add(eps).sqrt()\n",
    "                .div(state.add(eps).sqrt()).mul(param.getGradient());\n",
    "            // param -= rescaledGradient\n",
    "            param.subi(rescaledGradient);\n",
    "            // delta = rho * delta + (1 - rho) * g^2\n",
    "            delta.muli(rho).addi(rescaledGradient.square().mul(1 - rho));\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 2
   },
   "source": [
    "Choosing $\\rho = 0.9$ amounts to a half-life time of 10 for each parameter update. This tends to work quite well. We get the following behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.244, 0.140 sec/epoch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "   <div id=\"mLjjqT\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":400.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"epoch\",\n",
       "\"y\":\"loss\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"data\":{\n",
       "\"loss\":[0.3976842761039734,0.33912256360054016,0.2988288104534149,0.27370399236679077,0.25891828536987305,0.2527175545692444,0.24852116405963898,0.24909700453281403,0.24529370665550232,0.24508105218410492,0.24452221393585205,0.24295805394649506,0.24726718664169312,0.24291753768920898,0.24448031187057495],\n",
       "\"epoch\":[0.13333333333333333,0.26666666666666666,0.4,0.5333333333333333,0.6666666666666666,0.8,0.9333333333333333,1.0666666666666667,1.2,1.3333333333333333,1.4666666666666666,1.6,1.7333333333333334,1.8666666666666667,2.0]\n",
       "},\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\"\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"mLjjqT\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val airfoil = getDataCh11(10, 1500);\n",
    "\n",
    "fun trainAdadelta(rho: Float, numEpochs: Int) : LossTime {\n",
    "    val featureDimension = airfoil.getColumnNames().size\n",
    "    val hyperparams = mutableMapOf<String, Float>()\n",
    "    hyperparams.put(\"rho\", rho)\n",
    "    return trainCh11(Optimization::adadelta, \n",
    "                                       initAdadeltaStates(featureDimension), \n",
    "                                       hyperparams, airfoil, \n",
    "                                       featureDimension, numEpochs)\n",
    "}\n",
    "\n",
    "val lossTime = trainAdadelta(0.9f, 2)\n",
    "plotLossEpoch(lossTime.loss, lossTime.epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "As usual, for a concise implementation, we simply create an instance of `adadelta` from the `Optimizer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "origin_pos": 5,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.67, L2Loss: 0.49\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.67, L2Loss: 0.48, L2Loss: 0.48\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.67, L2Loss: 0.47█         | Accuracy: 0.67, L2Loss: 0.47, L2Loss: 0.46\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.67, L2Loss: 0.45�██████████████       | Accuracy: 0.67, L2Loss: 0.46, L2Loss: 0.45\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.67, L2Loss: 0.44\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.67, L2Loss: 0.43███████████       | Accuracy: 0.67, L2Loss: 0.43\n",
      "loss: 0.371, 0.216 sec/epoch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "   <div id=\"7y9D3q\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":400.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"epoch\",\n",
       "\"y\":\"loss\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"data\":{\n",
       "\"loss\":[0.5029948949813843,0.4983053505420685,0.49371203780174255,0.48961225152015686,0.4851512312889099,0.4811341464519501,0.47774431109428406,0.47374072670936584,0.4701749384403229,0.46690401434898376,0.46405282616615295,0.4600073993206024,0.456693559885025,0.4539746642112732,0.45072537660598755,0.4477524757385254,0.4443097412586212,0.44141724705696106,0.43855229020118713,0.4354425370693207,0.43223002552986145,0.42900699377059937,0.4263855516910553,0.4233197569847107,0.42080506682395935,0.4176415205001831,0.4149017333984375,0.4120928645133972,0.4094136953353882,0.4068828821182251,0.40419408679008484,0.4014259874820709,0.3985999822616577,0.3963824510574341,0.3940349221229553,0.3917523920536041,0.38955312967300415,0.38675931096076965,0.38438525795936584,0.38227009773254395,0.3796766400337219,0.3775693476200104,0.37479138374328613,0.37265917658805847,0.3706628084182739],\n",
       "\"epoch\":[0.13333333333333333,0.26666666666666666,0.4,0.5333333333333333,0.6666666666666666,0.8,0.9333333333333333,1.0666666666666667,1.2,1.3333333333333333,1.4666666666666666,1.6,1.7333333333333334,1.8666666666666667,2.0,2.1333333333333333,2.2666666666666666,2.4,2.533333333333333,2.6666666666666665,2.8,2.933333333333333,3.066666666666667,3.2,3.3333333333333335,3.466666666666667,3.6,3.7333333333333334,3.8666666666666667,4.0,4.133333333333334,4.266666666666667,4.4,4.533333333333333,4.666666666666667,4.8,4.933333333333334,5.066666666666666,5.2,5.333333333333333,5.466666666666667,5.6,5.733333333333333,5.866666666666666,6.0]\n",
       "},\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\"\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"7y9D3q\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val adadelta = Optimizer.adadelta().optRho(0.9f).build();\n",
    "\n",
    "val lossTime = trainConciseCh11(adadelta, airfoil, 6);\n",
    "plotLossEpoch(lossTime.loss, lossTime.epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Adadelta has no learning rate parameter. Instead, it uses the rate of change in the parameters itself to adapt the learning rate. \n",
    "* Adadelta requires two state variables to store the second moments of gradient and the change in parameters. \n",
    "* Adadelta uses leaky averages to keep a running estimate of the appropriate statistics. \n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Adjust the value of $\\rho$. What happens?\n",
    "1. Show how to implement the algorithm without the use of $\\mathbf{g}_t'$. Why might this be a good idea?\n",
    "1. Is Adadelta really learning rate free? Could you find optimization problems that break Adadelta?\n",
    "1. Compare Adadelta to Adagrad and RMS prop to discuss their convergence behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
