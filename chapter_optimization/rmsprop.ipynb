{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# RMSProp\n",
    ":label:`sec_rmsprop`\n",
    "\n",
    "One of the key issues in :numref:`sec_adagrad` is that the learning rate decreases at a predefined schedule of effectively $\\mathcal{O}(t^{-\\frac{1}{2}})$. While this is generally appropriate for convex problems, it might not be ideal for nonconvex ones, such as those encountered in deep learning. Yet, the coordinate-wise adaptivity of Adagrad is highly desirable as a preconditioner.\n",
    "\n",
    ":cite:`Tieleman.Hinton.2012` proposed the RMSProp algorithm as a simple fix to decouple rate scheduling from coordinate-adaptive learning rates. The issue is that Adagrad accumulates the squares of the gradient $\\mathbf{g}_t$ into a state vector $\\mathbf{s}_t = \\mathbf{s}_{t-1} + \\mathbf{g}_t^2$. As a result $\\mathbf{s}_t$ keeps on growing without bound due to the lack of normalization, essentially linarly as the algorithm converges.\n",
    "\n",
    "One way of fixing this problem would be to use $\\mathbf{s}_t / t$. For reasonable distributions of $\\mathbf{g}_t$ this will converge. Unfortunately it might take a very long time until the limit behavior starts to matter since the procedure remembers the full trajectory of values. An alternative is to use a leaky average in the same way we used in the momentum method, i.e., $\\mathbf{s}_t \\leftarrow \\gamma \\mathbf{s}_{t-1} + (1-\\gamma) \\mathbf{g}_t^2$ for some parameter $\\gamma > 0$. Keeping all other parts unchanged yields RMSProp.\n",
    "\n",
    "## The Algorithm\n",
    "\n",
    "Let us write out the equations in detail.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\mathbf{s}_t & \\leftarrow \\gamma \\mathbf{s}_{t-1} + (1 - \\gamma) \\mathbf{g}_t^2, \\\\\n",
    "    \\mathbf{x}_t & \\leftarrow \\mathbf{x}_{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{s}_t + \\epsilon}} \\odot \\mathbf{g}_t.\n",
    "\\end{aligned}$$\n",
    "\n",
    "The constant $\\epsilon > 0$ is typically set to $10^{-6}$ to ensure that we do not suffer from division by zero or overly large step sizes. Given this expansion we are now free to control the learning rate $\\eta$ independently of the scaling that is applied on a per-coordinate basis. In terms of leaky averages we can apply the same reasoning as previously applied in the case of the momentum method. Expanding the definition of $\\mathbf{s}_t$ yields\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{s}_t & = (1 - \\gamma) \\mathbf{g}_t^2 + \\gamma \\mathbf{s}_{t-1} \\\\\n",
    "& = (1 - \\gamma) \\left(\\mathbf{g}_t^2 + \\gamma \\mathbf{g}_{t-1}^2 + \\gamma^2 \\mathbf{g}_{t-2} + \\ldots, \\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "As before in :numref:`sec_momentum` we use $1 + \\gamma + \\gamma^2 + \\ldots, = \\frac{1}{1-\\gamma}$. Hence the sum of weights is normalized to $1$ with a half-life time of an observation of $\\gamma^{-1}$. Let us visualize the weights for the past 40 timesteps for various choices of $\\gamma$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"Bt5dVp\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "       if(!window.letsPlotCallQueue) {\n",
       "           window.letsPlotCallQueue = [];\n",
       "       }; \n",
       "       window.letsPlotCall = function(f) {\n",
       "           window.letsPlotCallQueue.push(f);\n",
       "       };\n",
       "       (function() {\n",
       "           var script = document.createElement(\"script\");\n",
       "           script.type = \"text/javascript\";\n",
       "           script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.4.0/js-package/distr/lets-plot.min.js\";\n",
       "           script.onload = function() {\n",
       "               window.letsPlotCall = function(f) {f();};\n",
       "               window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "               window.letsPlotCallQueue = [];\n",
       "               \n",
       "               \n",
       "           };\n",
       "           script.onerror = function(event) {\n",
       "               window.letsPlotCall = function(f) {};\n",
       "               window.letsPlotCallQueue = [];\n",
       "               var div = document.createElement(\"div\");\n",
       "               div.style.color = 'darkred';\n",
       "               div.textContent = 'Error loading Lets-Plot JS';\n",
       "               document.getElementById(\"Bt5dVp\").appendChild(div);\n",
       "           };\n",
       "           var e = document.getElementById(\"Bt5dVp\");\n",
       "           e.appendChild(script);\n",
       "       })();\n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%use @file[../djl.json]\n",
    "%use lets-plot\n",
    "@file:DependsOn(\"../D2J-1.0-SNAPSHOT.jar\")\n",
    "//import jp.live.ugai.d2j.attention.Chap10Utils\n",
    "import jp.live.ugai.d2j.util.GradDescUtils.plotGammas\n",
    "import jp.live.ugai.d2j.util.GradDescUtils.train2d\n",
    "import jp.live.ugai.d2j.util.GradDescUtils.showTrace2d\n",
    "import jp.live.ugai.d2j.util.TrainingChapter11.getDataCh11\n",
    "import jp.live.ugai.d2j.util.TrainingChapter11.trainCh11\n",
    "import jp.live.ugai.d2j.util.TrainingChapter11.trainConciseCh11\n",
    "import jp.live.ugai.d2j.util.LossTime\n",
    "import org.jetbrains.letsPlot.intern.Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "   <div id=\"ZSieGX\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0],\n",
       "\"y1\":[1.0,0.949999988079071,0.9024999773502351,0.8573749677240853,0.814506209117175,0.7737808889516455,0.7350918352798762,0.6983372347529049,0.6634203646904311,0.6302493385473225,0.5987368641067988,0.5688000137639592,0.5403600062951367,0.5133419995387867,0.4876748934423338,0.46329114295667934,0.44012658028598456,0.41812024602496767,0.39721422873933754,0.37735351256720806,0.35848583244044324,0.3405615365449369,0.3235334556578802,0.3073567790181668,0.29198893640328016,0.2773894861023368,0.26352000849047963,0.25034400492455233,0.23782680169399162,0.22593545877417562,0.2146386831421063,0.2039067464263085,0.19371140667423523,0.18402583403130354,0.17482454013597948,0.16608331104510957,0.15777914351298675,0.14989018445646346,0.14239567344681003,0.13527588807698082],\n",
       "\"y2\":[1.0,0.8999999761581421,0.8099999570846563,0.7289999420642869,0.6560999304771451,0.5904899217867893,0.5314409155297335,0.47829681130622137,0.43046711877211463,0.3874203966317673,0.34867834773176853,0.313810504645452,0.2824294466990814,0.2541864952955305,0.22876783970569914,0.2058910502808789,0.18530194034396585,0.16677174189162672,0.15009456372631588,0.13508510377515104,0.12157659017695607,0.10941892826064868,0.09847703282583327,0.08862932719537452,0.07976639236274924,0.07178975122469533,0.06461077439062475,0.05814969541112137,0.052334724483612455,0.047101250787494144,0.042391124585763405,0.038152011116503896,0.034336809095238674,0.030903127367061484,0.027812813893567365,0.025031531841101472,0.0225283780601931,0.0202755397170554,0.018247985261943322,0.01642318630068312],\n",
       "\"y3\":[1.0,0.800000011920929,0.6400000190734865,0.5120000228881839,0.40960002441406307,0.32768002441406324,0.2621440234375009,0.20971522187500097,0.16777218000000105,0.13421774600000108,0.10737419840000108,0.08589936000000105,0.068719489024001,0.05497559203840095,0.04398047428608089,0.03518437995315282,0.02814750438195275,0.0225180038411066,0.018014403341320803,0.014411522887805065,0.011529218482042794,0.009223374923073228,0.00737870004840978,0.005902960126688783,0.004722368171719795,0.0037778945936708516,0.003022315719972694,0.0024178526120069662,0.0019342821184286222,0.0015474257178013375,0.001237940592687822,9.903524889076595E-4,7.922820029320493E-4,6.338256117903769E-4,5.070604969880916E-4,4.056484036351054E-4,3.2451872774379013E-4,2.596149860635968E-4,2.0769199194572925E-4,1.6615359603246489E-4],\n",
       "\"y4\":[1.0,0.699999988079071,0.4899999833106996,0.34299998247623475,0.2400999836444859,0.16806998568892528,0.11764898797869733,0.0823542901826029,0.05764800214608239,0.040353600815039935,0.028247520089475547,0.019773263725896204,0.01384128437241167,0.009688898895687201,0.006782229111480366,0.004747560297185784,0.00332329215143472,0.0023263044663875744,0.0016284130987395918,0.0011398891497055173,7.979223912053246E-4,5.585456643317511E-4,3.9098195837384257E-4,2.7368736620082167E-4,1.915811530779675E-4,1.3410680487075194E-4,9.387476181084867E-5,6.57123321485197E-5,4.599863172061175E-5,3.2199041656081804E-5,2.2539328775414773E-5,1.5777529874100605E-5,1.104427072378761E-5,7.73098937499336E-6,5.411692470334777E-6,3.788184664721943E-6,2.65172922014668E-6,1.8562104224916002E-6,1.2993472736163675E-6,9.095430760420307E-7]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":600.0,\n",
       "\"height\":400.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y1\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y2\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y3\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y4\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"ZSieGX\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val manager = NDManager.newBaseManager();\n",
    "\n",
    "val gammas = listOf(0.95f, 0.9f, 0.8f, 0.7f)\n",
    "\n",
    "val timesND = manager.arange(40f);\n",
    "val times = timesND.toFloatArray().toList()\n",
    "plotGammas(times, gammas, 600, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 2
   },
   "source": [
    "## Implementation from Scratch\n",
    "\n",
    "As before we use the quadratic function $f(\\mathbf{x})=0.1x_1^2+2x_2^2$ to observe the trajectory of RMSProp. Recall that in :numref:`sec_adagrad`, when we used Adagrad with a learning rate of 0.4, the variables moved only very slowly in the later stages of the algorithm since the learning rate decreased too quickly. Since $\\eta$ is controlled separately this does not happen with RMSProp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tablesaw not supporting for contour and meshgrids, will update soon\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "   <div id=\"qjHigr\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":400.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"data\":{\n",
       "\"x\":[-5.0,-3.735095500946045,-2.952556848526001,-2.372981071472168,-1.915252447128296,-1.5430707931518555,-1.2364221811294556,-0.9826861619949341,-0.773052453994751,-0.6008374691009521,-0.4606166183948517,-0.34775713086128235,-0.2581668198108673,-0.18816684186458588,-0.13443559408187866,-0.09399157762527466,-0.06419359147548676,-0.04274466633796692,-0.027690961956977844,-0.017411664128303528,-0.010598676279187202],\n",
       "\"y\":[-2.0,-0.7350891828536987,-0.2781260907649994,-0.0977412760257721,-0.03101300448179245,-0.008698521181941032,-0.0021012965589761734,-4.214039072394371E-4,-6.628691335208714E-5,-7.4053350545000285E-6,-4.7147386794677004E-7,-6.137724994914606E-9,2.4778135099268184E-10,-2.394717757425724E-11,3.734967196633576E-12,-8.160755057828872E-13,2.3209808640983765E-13,-8.213590398098436E-14,3.5081841403238057E-14,-1.7692332923138418E-14,1.0362189418696745E-14]\n",
       "},\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\"\n",
       "},{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"data\":{\n",
       "\"x\":[-5.0,-3.735095500946045,-2.952556848526001,-2.372981071472168,-1.915252447128296,-1.5430707931518555,-1.2364221811294556,-0.9826861619949341,-0.773052453994751,-0.6008374691009521,-0.4606166183948517,-0.34775713086128235,-0.2581668198108673,-0.18816684186458588,-0.13443559408187866,-0.09399157762527466,-0.06419359147548676,-0.04274466633796692,-0.027690961956977844,-0.017411664128303528,-0.010598676279187202],\n",
       "\"y\":[-2.0,-0.7350891828536987,-0.2781260907649994,-0.0977412760257721,-0.03101300448179245,-0.008698521181941032,-0.0021012965589761734,-4.214039072394371E-4,-6.628691335208714E-5,-7.4053350545000285E-6,-4.7147386794677004E-7,-6.137724994914606E-9,2.4778135099268184E-10,-2.394717757425724E-11,3.734967196633576E-12,-8.160755057828872E-13,2.3209808640983765E-13,-8.213590398098436E-14,3.5081841403238057E-14,-1.7692332923138418E-14,1.0362189418696745E-14]\n",
       "},\n",
       "\"size\":3.0,\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"point\"\n",
       "},{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y\",\n",
       "\"z\":\"z\"\n",
       "},\n",
       "\"stat\":\"contour\",\n",
       "\"data\":{\n",
       "\"..group..\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\n",
       "\"x\":[-5.5,-5.415886415223286,-5.243925412246457,-5.209157377010628,-5.2,-5.184602710261304,-4.968999514320472,-4.9,-4.7875502270784995,-4.726683276672434,-4.6000000000000005,-4.482130217216026,-4.399842434846678,-4.300000000000001,-4.235258017345872,-4.020958987796334,-4.0,-3.985980360304798,-3.9120130883186226,-3.72433843378243,-3.7,-3.668770918871484,-3.4566002098230144,-3.4000000000000004,-3.32884162615805,-3.186717264973591,-3.1000000000000005,-2.9931743714008716,-2.914630419137296,-2.8000000000000007,-2.661627361727152,-2.6402783201793003,-2.500000000000001,-2.363597331976745,-2.3340649502765967,-2.2000000000000006,-2.084521455672371,-2.0103573095167584,-1.9000000000000008,-1.8029820983593394,-1.690380322041456,-1.600000000000001,-1.5189080677501146,-1.374015041084201,-1.3000000000000007,-1.2322253894186233,-1.061147596583023,-1.0000000000000009,-0.9428571544462585,-0.7516688349889327,-0.7000000000000011,-0.650723434849537,-0.44547405113122274,-0.40000000000000124,-0.3557411242342914,-0.14246283551047423,-0.10000000000000142,-0.05782371452229018,0.15746112699027837,0.1999999999999984,-5.5,-5.35833143635674,-5.339599601647751,-5.2,-5.087543845776977,-5.003716490982683,-4.9,-4.815393648167339,-4.67038628227462,-4.6000000000000005,-4.541857703736205,-4.3395566928065925,-4.300000000000001,-4.266912286816435,-4.011177010229006,-4.0,-3.9905330748451853,-3.871428044082161,-3.710703390661358,-3.7,-3.6878369430555766,-3.4281116180965956,-3.4000000000000004,-3.3683848954124525,-3.144216958098359,-3.1000000000000005,-3.050786095848182,-2.859000172192557,-2.8000000000000007,-2.735010225468655,-2.572441587967952,-2.500000000000001,-2.4210277264750686,-2.2845210762803774,-2.2000000000000006,-2.10880969859939,-1.9952181993115192,-1.9000000000000008,-1.798327749802993,-1.704512089016963,-1.600000000000001,-1.4895541899442186,-1.4123813850560927,-1.3000000000000007,-1.18246194146265,-1.1188043161176253,-1.0000000000000009,-0.8770243703514398,-0.8237587868255813,-0.7000000000000011,-0.5732154369177032,-0.5272221210135246,-0.40000000000000124,-0.27100977703681917,-0.2291711394396323,-0.10000000000000142,0.02961761883006009,0.07041765952543688,0.1999999999999984],\n",
       "\"y\":[-0.46842851596219237,-0.519858019029108,-0.625,-0.6364467212632856,-0.6394616153012282,-0.6442466121733705,-0.711249392900589,-0.7326923001271028,-0.7655622161518763,-0.7833540958405418,-0.8203845960066436,-0.8526627715200321,-0.8751969564416524,-0.9025383946987298,-0.9190725216823397,-0.9738012652545835,-0.9791538749749846,-0.9824754503809969,-1.0,-1.030423042228037,-1.0343684295290394,-1.0390363514106449,-1.0707502622787672,-1.0792104970467717,-1.0889479673024383,-1.1083965812169878,-1.1202631647649564,-1.1335320357489107,-1.1432880239216194,-1.1575263322968232,-1.1729657978410608,-1.1753479002241245,-1.1909999996423721,-1.20449666497093,-1.207418812154255,-1.2206841874566245,-1.230651819590463,-1.237053363104053,-1.2465789341612865,-1.2537276229491736,-1.2620245974481816,-1.268684201334652,-1.2736350846876419,-1.2824811986447502,-1.287000018515085,-1.2902817367732775,-1.2985655042712223,-1.3015263355092,-1.3035714430578225,-1.3104139562638353,-1.312263152316997,-1.31340429356192,-1.318157436085973,-1.3192105191318613,-1.3196764052928622,-1.3219214556119079,-1.3223684359537926,-1.3222796431528616,-1.3218264087378486,-1.3217368525894064,-1.9283548877123864,-1.9479142954459245,-1.950500497940311,-1.96977425534879,-1.98442980722122,-1.9953543862716465,-2.00887096456943,-2.019242060209174,-2.037017147156725,-2.045645138429057,-2.0523221296702556,-2.07555413399176,-2.0800967769276713,-2.083640358520544,-2.111028737213743,-2.1122257591064293,-2.1131663435564807,-2.125,-2.1383792383266966,-2.139270223073057,-2.1402038211805294,-2.160139522620744,-2.1622972822672613,-2.164518880734435,-2.1802711976229476,-2.183378330356366,-2.1865173801897733,-2.1987502152406955,-2.2025134704402975,-2.2062372181641825,-2.2155519849599394,-2.219702702519056,-2.223715341906165,-2.230651345350471,-2.234945923492715,-2.2389878767507634,-2.244022749139398,-2.2482432364612013,-2.2520903127462595,-2.255640111271203,-2.2595946414245143,-2.2630572625697276,-2.2654767313201143,-2.269000035282728,-2.271922573171688,-2.273505395147031,-2.2764594180358424,-2.278719537060702,-2.2796984835319747,-2.28197299588371,-2.2834807038528724,-2.2840276512669035,-2.285540562626478,-2.2862377787039785,-2.2864639242995386,-2.287162118264147,-2.287022023537577,-2.286977925593202,-2.2868378689965687]\n",
       "},\n",
       "\"color\":\"red\",\n",
       "\"position\":\"identity\",\n",
       "\"binwidth\":7.0,\n",
       "\"geom\":\"contour\"\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"qjHigr\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var eta = 0.4f;\n",
    "var gamma = 0.9f;\n",
    "\n",
    "val rmsProp2d = { state: List<Float> -> \n",
    "    var x1 = state[0]\n",
    "    var x2 = state[1]\n",
    "    var s1 = state[2]\n",
    "    var s2 = state[3]\n",
    "    var g1 = 0.2f * x1\n",
    "    var g2 = 4.0f * x2\n",
    "    var eps = 1e-6.toFloat()\n",
    "    s1 = gamma * s1 + (1 - gamma) * g1 * g1;\n",
    "    s2 = gamma * s2 + (1 - gamma) * g2 * g2;\n",
    "    x1 -= eta / Math.sqrt(s1.toDouble() + eps).toFloat() * g1\n",
    "    x2 -= eta / Math.sqrt(s2.toDouble() + eps).toFloat() * g2;\n",
    "    listOf(x1, x2, s1, s2)\n",
    "}\n",
    "\n",
    "val f2d = fun(x1:Float, x2: Float) : Float {\n",
    "    return 0.1f * x1 * x1 + 2 * x2 * x2;\n",
    "}\n",
    "\n",
    "showTrace2d(f2d, train2d(rmsProp2d, 20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RmsProp Gradient Descent 2D.](https://d2l-java-resources.s3.amazonaws.com/img/chapter_optim-rmsprop-gd2d.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "Next, we implement RMSProp to be used in a deep network. This is equally straightforward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun initRmsPropStates(featureDimension: Int) : NDList {\n",
    "    val manager = NDManager.newBaseManager();\n",
    "    val sW = manager.zeros(Shape(featureDimension.toLong(), 1));\n",
    "    val sB = manager.zeros(Shape(1));\n",
    "    return NDList(sW, sB);\n",
    "}\n",
    "\n",
    "object Optimization {\n",
    "    fun rmsProp(params:NDList , states:NDList , hyperparams:Map<String, Float> ) {\n",
    "        val gamma = hyperparams.get(\"gamma\")!!\n",
    "        val eps = 1e-6.toFloat()\n",
    "        for (i in 0 until params.size) {\n",
    "            val param = params.get(i);\n",
    "            val state = states.get(i);\n",
    "            // Update parameter and state\n",
    "            // state = gamma * state + (1 - gamma) * param.gradient^(1/2)\n",
    "            state.muli(gamma).addi(param.getGradient().square().mul(1 - gamma));\n",
    "            // param -= lr * param.gradient / sqrt(s + eps)\n",
    "            param.subi(param.getGradient().mul(hyperparams.get(\"lr\")).div(state.add(eps).sqrt()));\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "We set the initial learning rate to 0.01 and the weighting term $\\gamma$ to 0.9. That is, $\\mathbf{s}$ aggregates on average over the past $1/(1-\\gamma) = 10$ observations of the square gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.245, 0.159 sec/epoch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jp.live.ugai.d2j.util.LossTime@1ec7d8b3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val airfoil = getDataCh11(10, 1500);\n",
    "\n",
    "fun trainRmsProp(lr: Float, gamma: Float, numEpochs: Int) : LossTime {\n",
    "    var featureDimension = airfoil.getColumnNames().size\n",
    "    var hyperparams = mutableMapOf<String, Float>();\n",
    "    hyperparams.put(\"lr\", lr);\n",
    "    hyperparams.put(\"gamma\", gamma)\n",
    "    return trainCh11(Optimization::rmsProp, \n",
    "                                       initRmsPropStates(featureDimension), \n",
    "                                       hyperparams, airfoil, \n",
    "                                       featureDimension, numEpochs)\n",
    "}\n",
    "\n",
    "trainRmsProp(0.01f, 0.9f, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "## Concise Implementation\n",
    "\n",
    "Since RMSProp is a rather popular algorithm it is also available in `Optimizer`. We create an instance of `RmsProp` and set its learning rate and optional `gamma1` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.67, L2Loss: 0.28�██████████████████████████████████   | Accuracy: 0.67, L2Loss: 0.29, L2Loss: 0.28\n",
      "loss: 0.246, 0.267 sec/epoch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jp.live.ugai.d2j.util.LossTime@5bb3131b"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrt = Tracker.fixed(0.01f);\n",
    "val rmsProp = Optimizer.rmsprop().optLearningRateTracker(lrt).optRho(0.9f).build();\n",
    "\n",
    "trainConciseCh11(rmsProp, airfoil, 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "## Summary\n",
    "\n",
    "* RMSProp is very similar to Adagrad insofar as both use the square of the gradient to scale coefficients.\n",
    "* RMSProp shares with momentum the leaky averaging. However, RMSProp uses the technique to adjust the coefficient-wise preconditioner.\n",
    "* The learning rate needs to be scheduled by the experimenter in practice.\n",
    "* The coefficient $\\gamma$ determines how long the history is when adjusting the per-coordinate scale.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. What happens experimentally if we set $\\gamma = 1$? Why?\n",
    "1. Rotate the optimization problem to minimize $f(\\mathbf{x}) = 0.1 (x_1 + x_2)^2 + 2 (x_1 - x_2)^2$. What happens to the convergence?\n",
    "1. Try out what happens to RMSProp on a real machine learning problem, such as training on Fashion-MNIST. Experiment with different choices for adjusting the learning rate.\n",
    "1. Would you want to adjust $\\gamma$ as optimization progresses? How sensitive is RMSProp to this?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
