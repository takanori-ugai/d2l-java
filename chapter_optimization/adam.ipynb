{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Adam\n",
    ":label:`sec_adam`\n",
    "\n",
    "In the discussions leading up to this section we encountered a number of techniques for efficient optimization. Let us recap them in detail here:\n",
    "\n",
    "* We saw that :numref:`sec_sgd` is more effective than Gradient Descent when solving optimization problems, e.g., due to its inherent resilience to redundant data. \n",
    "* We saw that :numref:`sec_minibatch_sgd` affords significant additional efficiency arising from vectorization, using larger sets of observations in one minibatch. This is the key to efficient multi-machine, multi-GPU and overall parallel processing. \n",
    "* :numref:`sec_momentum` added a mechanism for aggregating a history of past gradients to accelerate convergence.\n",
    "* :numref:`sec_adagrad` used per-coordinate scaling to allow for a computationally efficient preconditioner. \n",
    "* :numref:`sec_rmsprop` decoupled per-coordinate scaling from a learning rate adjustment. \n",
    "\n",
    "Adam :cite:`Kingma.Ba.2014` combines all these techniques into one efficient learning algorithm. As expected, this is an algorithm that has become rather popular as one of the more robust and effective optimization algorithms to use in deep learning. It is not without issues, though. In particular, :cite:`Reddi.Kale.Kumar.2019` show that there are situations where Adam can diverge due to poor variance control. In a follow-up work :cite:`Zaheer.Reddi.Sachan.ea.2018` proposed a hotfix to Adam, called Yogi which addresses these issues. More on this later. For now let us review the Adam algorithm. \n",
    "\n",
    "## The Algorithm\n",
    "\n",
    "One of the key components of Adam is that it uses exponential weighted moving averages (also known as leaky averaging) to obtain an estimate of both the momentum and also the second moment of the gradient. That is, it uses the state variables\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\mathbf{v}_t & \\leftarrow \\beta_1 \\mathbf{v}_{t-1} + (1 - \\beta_1) \\mathbf{g}_t, \\\\\n",
    "    \\mathbf{s}_t & \\leftarrow \\beta_2 \\mathbf{s}_{t-1} + (1 - \\beta_2) \\mathbf{g}_t^2.\n",
    "\\end{aligned}$$\n",
    "\n",
    "Here $\\beta_1$ and $\\beta_2$ are nonnegative weighting parameters. Common choices for them are $\\beta_1 = 0.9$ and $\\beta_2 = 0.999$. That is, the variance estimate moves *much more slowly* than the momentum term. Note that if we initialize $\\mathbf{v}_0 = \\mathbf{s}_0 = 0$ we have a significant amount of bias initially towards smaller values. This can be addressed by using the fact that $\\sum_{i=0}^t \\beta^i = \\frac{1 - \\beta^t}{1 - \\beta}$ to re-normalize terms. Correspondingly the normalized state variables are given by \n",
    "\n",
    "$$\\hat{\\mathbf{v}}_t = \\frac{\\mathbf{v}_t}{1 - \\beta_1^t} \\text{ and } \\hat{\\mathbf{s}}_t = \\frac{\\mathbf{s}_t}{1 - \\beta_2^t}.$$\n",
    "\n",
    "Armed with the proper estimates we can now write out the update equations. First, we rescale the gradient in a manner very much akin to that of RMSProp to obtain\n",
    "\n",
    "$$\\mathbf{g}_t' = \\frac{\\eta \\hat{\\mathbf{v}}_t}{\\sqrt{\\hat{\\mathbf{s}}_t} + \\epsilon}.$$\n",
    "\n",
    "Unlike RMSProp our update uses the momentum $\\hat{\\mathbf{v}}_t$ rather than the gradient itself. Moreover, there is a slight cosmetic difference as the rescaling happens using $\\frac{1}{\\sqrt{\\hat{\\mathbf{s}}_t} + \\epsilon}$ instead of $\\frac{1}{\\sqrt{\\hat{\\mathbf{s}}_t + \\epsilon}}$. The former works arguably slightly better in practice, hence the deviation from RMSProp. Typically we pick $\\epsilon = 10^{-6}$ for a good trade-off between numerical stability and fidelity. \n",
    "\n",
    "Now we have all the pieces in place to compute updates. This is slightly anticlimactic and we have a simple update of the form\n",
    "\n",
    "$$\\mathbf{x}_t \\leftarrow \\mathbf{x}_{t-1} - \\mathbf{g}_t'.$$\n",
    "\n",
    "Reviewing the design of Adam its inspiration is clear. Momentum and scale are clearly visible in the state variables. Their rather peculiar definition forces us to debias terms (this could be fixed by a slightly different initialization and update condition). Second, the combination of both terms is pretty straightforward, given RMSProp. Last, the explicit learning rate $\\eta$ allows us to control the step length to address issues of convergence. \n",
    "\n",
    "## Implementation \n",
    "\n",
    "Implementing Adam from scratch is not very daunting. For convenience we store the timestep counter $t$ in the `hyperparams` dictionary. Beyond that all is straightforward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"2jsP2E\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "       if(!window.letsPlotCallQueue) {\n",
       "           window.letsPlotCallQueue = [];\n",
       "       }; \n",
       "       window.letsPlotCall = function(f) {\n",
       "           window.letsPlotCallQueue.push(f);\n",
       "       };\n",
       "       (function() {\n",
       "           var script = document.createElement(\"script\");\n",
       "           script.type = \"text/javascript\";\n",
       "           script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.4.0/js-package/distr/lets-plot.min.js\";\n",
       "           script.onload = function() {\n",
       "               window.letsPlotCall = function(f) {f();};\n",
       "               window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "               window.letsPlotCallQueue = [];\n",
       "               \n",
       "               \n",
       "           };\n",
       "           script.onerror = function(event) {\n",
       "               window.letsPlotCall = function(f) {};\n",
       "               window.letsPlotCallQueue = [];\n",
       "               var div = document.createElement(\"div\");\n",
       "               div.style.color = 'darkred';\n",
       "               div.textContent = 'Error loading Lets-Plot JS';\n",
       "               document.getElementById(\"2jsP2E\").appendChild(div);\n",
       "           };\n",
       "           var e = document.getElementById(\"2jsP2E\");\n",
       "           e.appendChild(script);\n",
       "       })();\n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%use @file[../djl-pytorch.json]\n",
    "%use lets-plot\n",
    "@file:DependsOn(\"../D2J-1.0-SNAPSHOT.jar\")\n",
    "@file:DependsOn(\"../pytorch-native-cu116-1.12.1-20220814.211140-1.jar\")\n",
    "@file:DependsOn(\"../pytorch-native-cu116-1.12.1-20220814.211140-1-linux-x86_64.jar\")\n",
    "//import jp.live.ugai.d2j.attention.Chap10Utils\n",
    "import jp.live.ugai.d2j.util.GradDescUtils.plotGammas\n",
    "import jp.live.ugai.d2j.util.GradDescUtils.train2d\n",
    "import jp.live.ugai.d2j.util.GradDescUtils.showTrace2d\n",
    "import jp.live.ugai.d2j.util.TrainingChapter11.getDataCh11\n",
    "import jp.live.ugai.d2j.util.TrainingChapter11.plotLossEpoch\n",
    "import jp.live.ugai.d2j.util.TrainingChapter11.trainCh11\n",
    "import jp.live.ugai.d2j.util.TrainingChapter11.trainConciseCh11\n",
    "import jp.live.ugai.d2j.util.LossTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun initAdamStates(featureDimension: Int): NDList {\n",
    "    val manager = NDManager.newBaseManager();\n",
    "    val vW = manager.zeros(Shape(featureDimension.toLong(), 1));\n",
    "    val vB = manager.zeros(Shape(1));\n",
    "    val sW = manager.zeros(Shape(featureDimension.toLong(), 1));\n",
    "    val sB = manager.zeros(Shape(1));\n",
    "    return NDList(vW, sW, vB, sB)\n",
    "}\n",
    "\n",
    "object Optimization {\n",
    "    fun adam(params:NDList , states:NDList , hyperparams:MutableMap<String, Float> ) {\n",
    "        val beta1 = 0.9f;\n",
    "        val beta2 = 0.999f;\n",
    "        val eps = 1e-6.toFloat()\n",
    "        val time = hyperparams.get(\"time\")!!\n",
    "        for (i in 0 until params.size) {\n",
    "            val param = params.get(i);\n",
    "            val velocity = states.get(2 * i);\n",
    "            val state = states.get(2 * i + 1);\n",
    "            // Update parameter, velocity, and state\n",
    "            // velocity = beta1 * v + (1 - beta1) * param.gradient\n",
    "            velocity.muli(beta1).addi(param.getGradient().mul(1 - beta1));\n",
    "            // state = beta2 * state + (1 - beta2) * param.gradient^2\n",
    "            state.muli(beta2).addi(param.getGradient().square().mul(1 - beta2));\n",
    "            // vBiasCorr = velocity / ((1 - beta1)^(time))\n",
    "            val vBiasCorr = velocity.div(1 - Math.pow(beta1.toDouble(), time.toDouble()));\n",
    "            // sBiasCorr = state / ((1 - beta2)^(time))\n",
    "            val sBiasCorr = state.div(1 - Math.pow(beta2.toDouble(), time.toDouble()))\n",
    "            // param -= lr * vBiasCorr / (sBiasCorr^(1/2) + eps)\n",
    "            param.subi(vBiasCorr.mul(hyperparams.get(\"lr\")).div(sBiasCorr.sqrt().add(eps)));\n",
    "        }\n",
    "        hyperparams.put(\"time\", time + 1);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 2
   },
   "source": [
    "We are ready to use Adam to train the model. We use a learning rate of $\\eta = 0.01$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load PyTorch native library\n",
      "ai.djl.engine.EngineException: Failed to load PyTorch native library\n",
      "\tat ai.djl.pytorch.engine.PtEngine.newInstance(PtEngine.java:82)\n",
      "\tat ai.djl.pytorch.engine.PtEngineProvider.getEngine(PtEngineProvider.java:40)\n",
      "\tat ai.djl.engine.Engine.getEngine(Engine.java:181)\n",
      "\tat ai.djl.engine.Engine.getInstance(Engine.java:136)\n",
      "\tat ai.djl.ndarray.NDManager.newBaseManager(NDManager.java:115)\n",
      "\tat Line_9.initAdamStates(Line_9.jupyter-kts:2)\n",
      "\tat Line_10.trainAdam(Line_10.jupyter-kts:9)\n",
      "\tat Line_10.<init>(Line_10.jupyter-kts:14)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.evalWithConfigAndOtherScriptsResults(BasicJvmScriptEvaluator.kt:105)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.invoke$suspendImpl(BasicJvmScriptEvaluator.kt:47)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.invoke(BasicJvmScriptEvaluator.kt)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmReplEvaluator.eval(BasicJvmReplEvaluator.kt:49)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.InternalEvaluatorImpl$eval$resultWithDiagnostics$1.invokeSuspend(InternalEvaluatorImpl.kt:103)\n",
      "\tat kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\n",
      "\tat kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:106)\n",
      "\tat kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:284)\n",
      "\tat kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:85)\n",
      "\tat kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:59)\n",
      "\tat kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source)\n",
      "\tat kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:38)\n",
      "\tat kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.InternalEvaluatorImpl.eval(InternalEvaluatorImpl.kt:103)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl$execute$1$result$1.invoke(CellExecutorImpl.kt:71)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl$execute$1$result$1.invoke(CellExecutorImpl.kt:69)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.withHost(repl.kt:635)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl.execute(CellExecutorImpl.kt:69)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.CellExecutor$DefaultImpls.execute$default(CellExecutor.kt:15)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl$evalEx$1.invoke(repl.kt:444)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl$evalEx$1.invoke(repl.kt:433)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.withEvalContext(repl.kt:397)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.evalEx(repl.kt:433)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.eval(repl.kt:485)\n",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.ProtocolKt$shellMessagesHandler$2$res$1.invoke(protocol.kt:321)\n",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.ProtocolKt$shellMessagesHandler$2$res$1.invoke(protocol.kt:320)\n",
      "\tat org.jetbrains.kotlinx.jupyter.JupyterExecutorImpl$runExecution$execThread$1.invoke(execution.kt:33)\n",
      "\tat org.jetbrains.kotlinx.jupyter.JupyterExecutorImpl$runExecution$execThread$1.invoke(execution.kt:31)\n",
      "\tat kotlin.concurrent.ThreadsKt$thread$thread$1.run(Thread.kt:30)\n",
      "Caused by: java.lang.UnsatisfiedLinkError: /home/hisako/.djl.ai/pytorch/1.12.1-SNAPSHOT-20220814-cu116-linux-x86_64/libtorch_cuda_cu.so: libcublas-2854e16e.so.11: cannot open shared object file: No such file or directory\n",
      "\tat java.base/java.lang.ClassLoader$NativeLibrary.load0(Native Method)\n",
      "\tat java.base/java.lang.ClassLoader$NativeLibrary.load(ClassLoader.java:2445)\n",
      "\tat java.base/java.lang.ClassLoader$NativeLibrary.loadLibrary(ClassLoader.java:2501)\n",
      "\tat java.base/java.lang.ClassLoader.loadLibrary0(ClassLoader.java:2700)\n",
      "\tat java.base/java.lang.ClassLoader.loadLibrary(ClassLoader.java:2630)\n",
      "\tat java.base/java.lang.Runtime.load0(Runtime.java:768)\n",
      "\tat java.base/java.lang.System.load(System.java:1837)\n",
      "\tat ai.djl.pytorch.jni.LibUtils.loadNativeLibrary(LibUtils.java:356)\n",
      "\tat ai.djl.pytorch.jni.LibUtils.loadLibTorch(LibUtils.java:163)\n",
      "\tat ai.djl.pytorch.jni.LibUtils.loadLibrary(LibUtils.java:76)\n",
      "\tat ai.djl.pytorch.engine.PtEngine.newInstance(PtEngine.java:53)\n",
      "\t... 40 more\n"
     ]
    }
   ],
   "source": [
    "val airfoil = getDataCh11(10, 1500);\n",
    "\n",
    "fun trainAdam(lr: Float, time: Float, numEpochs: Int) : LossTime {\n",
    "    val featureDimension = airfoil.getColumnNames().size\n",
    "    val hyperparams = mutableMapOf<String, Float>()\n",
    "    hyperparams.put(\"lr\", lr);\n",
    "    hyperparams.put(\"time\", time);\n",
    "    return trainCh11(Optimization::adam, \n",
    "                                       initAdamStates(featureDimension), \n",
    "                                       hyperparams, airfoil, \n",
    "                                       featureDimension, numEpochs);\n",
    "}\n",
    "\n",
    "val lossTime = trainAdam(0.01f, 1.0f, 2);\n",
    "plotLossEpoch(lossTime.loss, lossTime.epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "A more concise implementation is straightforward since `adam` is one of the algorithms provided as part of the DJL optimization library.\n",
    "\n",
    "We will set the learning rate to 0.01f to remain consistent with the previous section.\n",
    "However, you typically won't need to set this yourself as the default will usually work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "null\n",
      "java.lang.NullPointerException\n",
      "\tat Line_11.<init>(Line_11.jupyter-kts:4)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.evalWithConfigAndOtherScriptsResults(BasicJvmScriptEvaluator.kt:105)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.invoke$suspendImpl(BasicJvmScriptEvaluator.kt:47)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.invoke(BasicJvmScriptEvaluator.kt)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmReplEvaluator.eval(BasicJvmReplEvaluator.kt:49)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.InternalEvaluatorImpl$eval$resultWithDiagnostics$1.invokeSuspend(InternalEvaluatorImpl.kt:103)\n",
      "\tat kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\n",
      "\tat kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:106)\n",
      "\tat kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:284)\n",
      "\tat kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:85)\n",
      "\tat kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:59)\n",
      "\tat kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source)\n",
      "\tat kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:38)\n",
      "\tat kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.InternalEvaluatorImpl.eval(InternalEvaluatorImpl.kt:103)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl$execute$1$result$1.invoke(CellExecutorImpl.kt:71)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl$execute$1$result$1.invoke(CellExecutorImpl.kt:69)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.withHost(repl.kt:635)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl.execute(CellExecutorImpl.kt:69)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.CellExecutor$DefaultImpls.execute$default(CellExecutor.kt:15)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl$evalEx$1.invoke(repl.kt:444)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl$evalEx$1.invoke(repl.kt:433)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.withEvalContext(repl.kt:397)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.evalEx(repl.kt:433)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.eval(repl.kt:485)\n",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.ProtocolKt$shellMessagesHandler$2$res$1.invoke(protocol.kt:321)\n",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.ProtocolKt$shellMessagesHandler$2$res$1.invoke(protocol.kt:320)\n",
      "\tat org.jetbrains.kotlinx.jupyter.JupyterExecutorImpl$runExecution$execThread$1.invoke(execution.kt:33)\n",
      "\tat org.jetbrains.kotlinx.jupyter.JupyterExecutorImpl$runExecution$execThread$1.invoke(execution.kt:31)\n",
      "\tat kotlin.concurrent.ThreadsKt$thread$thread$1.run(Thread.kt:30)\n"
     ]
    }
   ],
   "source": [
    "val lrt = Tracker.fixed(0.01f);\n",
    "val adam = Optimizer.adam().optLearningRateTracker(lrt).build();\n",
    "\n",
    "val lossTime = trainConciseCh11(adam, airfoil, 2);\n",
    "plotLossEpoch(lossTime.loss, lossTime.epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## Yogi\n",
    "\n",
    "One of the problems of Adam is that it can fail to converge even in convex settings when the second moment estimate in $\\mathbf{s}_t$ blows up. As a fix :cite:`Zaheer.Reddi.Sachan.ea.2018` proposed a refined update (and initialization) for $\\mathbf{s}_t$. To understand what's going on, let us rewrite the Adam update as follows:\n",
    "\n",
    "$$\\mathbf{s}_t \\leftarrow \\mathbf{s}_{t-1} + (1 - \\beta_2) \\left(\\mathbf{g}_t^2 - \\mathbf{s}_{t-1}\\right).$$\n",
    "\n",
    "Whenever $\\mathbf{g}_t^2$ has high variance or updates are sparse, $\\mathbf{s}_t$ might forget past values too quickly. A possible fix for this is to replace $\\mathbf{g}_t^2 - \\mathbf{s}_{t-1}$ by $\\mathbf{g}_t^2 \\odot \\mathop{\\mathrm{sgn}}(\\mathbf{g}_t^2 - \\mathbf{s}_{t-1})$. Now the magnitude of the update no longer depends on the amount of deviation. This yields the Yogi updates\n",
    "\n",
    "$$\\mathbf{s}_t \\leftarrow \\mathbf{s}_{t-1} + (1 - \\beta_2) \\mathbf{g}_t^2 \\odot \\mathop{\\mathrm{sgn}}(\\mathbf{g}_t^2 - \\mathbf{s}_{t-1}).$$\n",
    "\n",
    "The authors furthermore advise to initialize the momentum on a larger initial batch rather than just initial pointwise estimate. We omit the details since they are not material to the discussion and since even without this convergence remains pretty good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "object Optimization {\n",
    "    fun yogi(params: NDList , states: NDList , hyperparams: MutableMap<String, Float> ) {\n",
    "        val beta1 = 0.9f;\n",
    "        val beta2 = 0.999f;\n",
    "        val eps = 1e-3.toFloat()\n",
    "        val time = hyperparams.get(\"time\")!!\n",
    "        for (i in 0 until params.size) {\n",
    "            val param = params.get(i);\n",
    "            val velocity = states.get(2 * i);\n",
    "            val state = states.get(2 * i + 1);\n",
    "            // Update parameter, velocity, and state\n",
    "            // velocity = beta1 * v + (1 - beta1) * param.gradient\n",
    "            velocity.muli(beta1).addi(param.getGradient().mul(1 - beta1));\n",
    "            /* Rewritten Update */\n",
    "            // state = state + (1 - beta2) * sign(param.gradient^2 - state) \n",
    "            //         * param.gradient^2\n",
    "            state.addi(param.getGradient().square().sub(state).sign().mul(1 - beta2));\n",
    "            // vBiasCorr = velocity / ((1 - beta1)^(time))\n",
    "            val vBiasCorr = velocity.div(1 - Math.pow(beta1.toDouble(), time.toDouble()));\n",
    "            // sBiasCorr = state / ((1 - beta2)^(time))\n",
    "            val sBiasCorr = state.div(1 - Math.pow(beta2.toDouble(), time.toDouble()));\n",
    "            // param -= lr * vBiasCorr / (sBiasCorr^(1/2) + eps)\n",
    "            param.subi(vBiasCorr.mul(hyperparams.get(\"lr\")).div(sBiasCorr.sqrt().add(eps)));\n",
    "        }\n",
    "        hyperparams.put(\"time\", time + 1);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load PyTorch native library\n",
      "ai.djl.engine.EngineException: Failed to load PyTorch native library\n",
      "\tat ai.djl.pytorch.engine.PtEngine.newInstance(PtEngine.java:82)\n",
      "\tat ai.djl.pytorch.engine.PtEngineProvider.getEngine(PtEngineProvider.java:40)\n",
      "\tat ai.djl.engine.Engine.getEngine(Engine.java:181)\n",
      "\tat ai.djl.engine.Engine.getInstance(Engine.java:136)\n",
      "\tat ai.djl.ndarray.NDManager.newBaseManager(NDManager.java:115)\n",
      "\tat Line_9.initAdamStates(Line_9.jupyter-kts:2)\n",
      "\tat Line_13.trainYogi(Line_13.jupyter-kts:9)\n",
      "\tat Line_13.<init>(Line_13.jupyter-kts:14)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.evalWithConfigAndOtherScriptsResults(BasicJvmScriptEvaluator.kt:105)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.invoke$suspendImpl(BasicJvmScriptEvaluator.kt:47)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.invoke(BasicJvmScriptEvaluator.kt)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmReplEvaluator.eval(BasicJvmReplEvaluator.kt:49)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.InternalEvaluatorImpl$eval$resultWithDiagnostics$1.invokeSuspend(InternalEvaluatorImpl.kt:103)\n",
      "\tat kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\n",
      "\tat kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:106)\n",
      "\tat kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:284)\n",
      "\tat kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:85)\n",
      "\tat kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:59)\n",
      "\tat kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source)\n",
      "\tat kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:38)\n",
      "\tat kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.InternalEvaluatorImpl.eval(InternalEvaluatorImpl.kt:103)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl$execute$1$result$1.invoke(CellExecutorImpl.kt:71)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl$execute$1$result$1.invoke(CellExecutorImpl.kt:69)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.withHost(repl.kt:635)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl.execute(CellExecutorImpl.kt:69)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.CellExecutor$DefaultImpls.execute$default(CellExecutor.kt:15)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl$evalEx$1.invoke(repl.kt:444)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl$evalEx$1.invoke(repl.kt:433)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.withEvalContext(repl.kt:397)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.evalEx(repl.kt:433)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.eval(repl.kt:485)\n",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.ProtocolKt$shellMessagesHandler$2$res$1.invoke(protocol.kt:321)\n",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.ProtocolKt$shellMessagesHandler$2$res$1.invoke(protocol.kt:320)\n",
      "\tat org.jetbrains.kotlinx.jupyter.JupyterExecutorImpl$runExecution$execThread$1.invoke(execution.kt:33)\n",
      "\tat org.jetbrains.kotlinx.jupyter.JupyterExecutorImpl$runExecution$execThread$1.invoke(execution.kt:31)\n",
      "\tat kotlin.concurrent.ThreadsKt$thread$thread$1.run(Thread.kt:30)\n",
      "Caused by: java.lang.UnsatisfiedLinkError: /home/hisako/.djl.ai/pytorch/1.12.1-SNAPSHOT-20220814-cu116-linux-x86_64/libtorch_cuda_cu.so: libcublas-2854e16e.so.11: cannot open shared object file: No such file or directory\n",
      "\tat java.base/java.lang.ClassLoader$NativeLibrary.load0(Native Method)\n",
      "\tat java.base/java.lang.ClassLoader$NativeLibrary.load(ClassLoader.java:2445)\n",
      "\tat java.base/java.lang.ClassLoader$NativeLibrary.loadLibrary(ClassLoader.java:2501)\n",
      "\tat java.base/java.lang.ClassLoader.loadLibrary0(ClassLoader.java:2700)\n",
      "\tat java.base/java.lang.ClassLoader.loadLibrary(ClassLoader.java:2630)\n",
      "\tat java.base/java.lang.Runtime.load0(Runtime.java:768)\n",
      "\tat java.base/java.lang.System.load(System.java:1837)\n",
      "\tat ai.djl.pytorch.jni.LibUtils.loadNativeLibrary(LibUtils.java:356)\n",
      "\tat ai.djl.pytorch.jni.LibUtils.loadLibTorch(LibUtils.java:163)\n",
      "\tat ai.djl.pytorch.jni.LibUtils.loadLibrary(LibUtils.java:76)\n",
      "\tat ai.djl.pytorch.engine.PtEngine.newInstance(PtEngine.java:53)\n",
      "\t... 40 more\n"
     ]
    }
   ],
   "source": [
    "val airfoil = getDataCh11(10, 1500);\n",
    "\n",
    "fun trainYogi(lr: Float, time: Float, numEpochs: Int) : LossTime {\n",
    "    val featureDimension = airfoil.getColumnNames().size\n",
    "    val hyperparams = mutableMapOf<String, Float>();\n",
    "    hyperparams.put(\"lr\", lr)\n",
    "    hyperparams.put(\"time\", time)\n",
    "    return trainCh11(Optimization::yogi, \n",
    "                                       initAdamStates(featureDimension), \n",
    "                                       hyperparams, airfoil, \n",
    "                                       featureDimension, numEpochs);\n",
    "}\n",
    "\n",
    "val lossTime = trainYogi(0.01f, 1.0f, 2)\n",
    "plotLossEpoch(lossTime.loss, lossTime.epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Adam combines features of many optimization algorithms into a fairly robust update rule. \n",
    "* Created on the basis of RMSProp, Adam also uses EWMA on the minibatch stochastic gradient\n",
    "* Adam uses bias correction to adjust for a slow startup when estimating momentum and a second moment. \n",
    "* For gradients with significant variance we may encounter issues with convergence. They can be amended by using larger minibatches or by switching to an improved estimate for $\\mathbf{s}_t$. Yogi offers such an alternative. \n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Adjust the learning rate and observe and analyze the experimental results.\n",
    "1. Can you rewrite momentum and second moment updates such that it does not require bias correction?\n",
    "1. Why do you need to reduce the learning rate $\\eta$ as we converge?\n",
    "1. Try to construct a case for which Adam diverges and Yogi converges?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
