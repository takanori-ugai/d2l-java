{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Text Preprocessing\n",
    ":label:`sec_text_preprocessing`\n",
    "\n",
    "We have reviewed and evaluated\n",
    "statistical tools \n",
    "and prediction challenges\n",
    "for sequence data.\n",
    "Such data can take many forms.\n",
    "Specifically,\n",
    "as we will focus on\n",
    "in many chapters of the book,\n",
    "text is one of the most popular examples of sequence data.\n",
    "For example,\n",
    "an article can be simply viewed as a sequence of words, or even a sequence of characters.\n",
    "To facilitate our future experiments\n",
    "with sequence data,\n",
    "we will dedicate this section\n",
    "to explain common preprocessing steps for text.\n",
    "Usually, these steps are:\n",
    "\n",
    "1. Load text as strings into memory.\n",
    "1. Split strings into tokens (e.g., words and characters).\n",
    "1. Build a table of vocabulary to map the split tokens to numerical indices.\n",
    "1. Convert text into sequences of numerical indices so they can be manipulated by models easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%use @file[../djl.json]\n",
    "// %load ../utils/djl-imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## Reading the Dataset\n",
    "\n",
    "To get started we load text from H. G. Wells' [*The Time Machine*](http://www.gutenberg.org/ebooks/35).\n",
    "This is a fairly small corpus of just over 30000 words, but for the purpose of what we want to illustrate this is just fine.\n",
    "More realistic document collections contain many billions of words.\n",
    "The following function reads the dataset into a list of text lines, where each line is a string.\n",
    "For simplicity, here we ignore punctuation and capitalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# text lines: 3221\n",
      "the time machine by h g wells\n",
      "twinkled and his usually pale face was flushed and animated the\n"
     ]
    }
   ],
   "source": [
    "import java.net.URL\n",
    "import java.io.BufferedReader\n",
    "import java.io.InputStreamReader\n",
    "import java.util.Locale\n",
    "fun readTimeMachine(): Array<String> {\n",
    "    val url = URL(\"http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt\")\n",
    "    var lines: Array<String>\n",
    "    BufferedReader(InputStreamReader(url.openStream())).use { inp ->\n",
    "        lines = inp.readLines().toTypedArray()\n",
    "    }\n",
    "    for (i in lines.indices) {\n",
    "        lines[i] = lines[i].replace(\"[^A-Za-z]+\".toRegex(), \" \").trim().lowercase(Locale.getDefault())\n",
    "    }\n",
    "    return lines\n",
    "}\n",
    "\n",
    "val lines = readTimeMachine()\n",
    "println(\"# text lines: \" + lines.size);\n",
    "println(lines[0])\n",
    "println(lines[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "The following `tokenize` function\n",
    "takes an array (`lines`) as the input,\n",
    "where each element is a text sequence (e.g., a text line).\n",
    "Each text sequence is split into a list of tokens.\n",
    "A *token* is the basic unit in text.\n",
    "In the end,\n",
    "a list of token lists are returned,\n",
    "where each token is a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[the, time, machine, by, h, g, wells]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[i]\n",
      "[]\n",
      "[]\n",
      "[the, time, traveller, for, so, it, will, be, convenient, to, speak, of, him]\n",
      "[was, expounding, a, recondite, matter, to, us, his, grey, eyes, shone, and]\n",
      "[twinkled, and, his, usually, pale, face, was, flushed, and, animated, the]\n"
     ]
    }
   ],
   "source": [
    "fun tokenize(lines: Array<String>, token: String) : List<List<String>> {\n",
    "    // Split text lines into word or character tokens.\n",
    "//    String[][] output = new String[lines.length][];\n",
    "    val output = mutableListOf<List<String>>()\n",
    "    if (token == \"word\") {\n",
    "        for (i in 0 until lines.size) {\n",
    "            output.add(lines[i].split(\" \"))\n",
    "        }\n",
    "    } else if (token == \"char\") {\n",
    "        for (i in 0 until lines.size) {\n",
    "            output.add(lines[i].split(\"\"))\n",
    "        }\n",
    "    } else {\n",
    "        throw Exception(\"ERROR: unknown token type: \" + token);\n",
    "    }\n",
    "    return output; \n",
    "}\n",
    "\n",
    "val tokens = tokenize(lines, \"word\");\n",
    "for (i in 0 until 11) {\n",
    "    println(tokens[i]);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "## Vocabulary\n",
    "\n",
    "The string type of the token is inconvenient to be used by models, which take numerical inputs.\n",
    "Now let us build a dictionary (HashMap), often called *vocabulary* as well, to map string tokens into numerical indices starting from 0.\n",
    "To do so, we first count the unique tokens in all the documents from the training set,\n",
    "namely a *corpus*,\n",
    "and then assign a numerical index to each unique token according to its frequency.\n",
    "Rarely appeared tokens are often removed to reduce the complexity.\n",
    "Any token that does not exist in the corpus or has been removed is mapped into a special unknown token “&lt;unk&gt;”.\n",
    "We optionally add a list of reserved tokens, such as\n",
    "“&lt;pad&gt;” for padding,\n",
    "“&lt;bos&gt;” to present the beginning for a sequence, and “&lt;eos&gt;” for the end of a sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(tokens: List<List<String>>, minFreq: Int, reservedTokens: List<String>) {\n",
    "    // The index for the unknown token is 0\n",
    "    var unk: Int= 0\n",
    "    var tokenFreqs: kotlin.collections.List<kotlin.Pair<String, Int>>\n",
    "    var idxToToken: MutableList<String> = mutableListOf()\n",
    "    var tokenToIdx: MutableMap<String, Int> = mutableMapOf()\n",
    "\n",
    "    init {\n",
    "        // Sort according to frequencies\n",
    "        val counter = countCorpus2D(tokens)\n",
    "        tokenFreqs = counter.toList().sortedBy { (_, value) -> value}\n",
    "\n",
    "        val uniqTokens: MutableSet<String> = mutableSetOf()\n",
    "        uniqTokens.add(\"<unk>\")\n",
    "        uniqTokens.addAll(reservedTokens)\n",
    "        uniqTokens.addAll(tokenFreqs.filter{(key, value) -> \n",
    "            value >= minFreq && !uniqTokens.contains(key)\n",
    "        }.map { it.first })\n",
    "        for (token in uniqTokens) {\n",
    "            idxToToken.add(token)\n",
    "            tokenToIdx[token] = idxToToken.size - 1\n",
    "        }\n",
    "    }\n",
    "\n",
    "    fun length(): Int {\n",
    "        return idxToToken.size\n",
    "    }\n",
    "\n",
    "    fun getIdxs(tokens: List<String>): List<Int> {\n",
    "        return tokens.map { getIdx(it) }\n",
    "    }\n",
    "\n",
    "    fun getIdx(token: String): Int {\n",
    "        return tokenToIdx.getOrDefault(token, unk)\n",
    "    }\n",
    "}\n",
    "\n",
    "    fun countCorpus(tokens: List<String>): Map<String, Int> {\n",
    "        /* Count token frequencies. */\n",
    "        var counter = tokens.groupingBy { it }.eachCount()\n",
    "        return counter\n",
    "    }\n",
    "\n",
    "    fun countCorpus2D(tokens: List<List<String>>): Map<String, Int> {\n",
    "        /* Flatten a list of token lists into a list of tokens */\n",
    "        return countCorpus(tokens.flatten().filter{ it != \"\"})\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "We construct a vocabulary using the time machine dataset as the corpus. \n",
    "Then we print the first few frequent tokens with their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<unk>, 0) (h, 1) (g, 2) (recondite, 3) (twinkled, 4) (radiance, 5) (incandescent, 6) (lights, 7) (lilies, 8) (bubbles, 9) "
     ]
    }
   ],
   "source": [
    "//println(tokens)\n",
    "val vocab = Vocab(tokens, 0, listOf<String>());\n",
    "for (i in 0 until 10) {\n",
    "    val token = vocab.idxToToken.get(i)\n",
    "    print(\"(\" + token + \", \" + vocab.tokenToIdx.get(token) + \") \")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "Now we can convert each text line into a list of numerical indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:[the, time, machine, by, h, g, wells]\n",
      "Indices:[4579, 4561, 4528, 4540, 1, 2, 4142]\n",
      "Words:[twinkled, and, his, usually, pale, face, was, flushed, and, animated, the]\n",
      "Indices:[4, 4577, 4555, 3161, 4181, 4466, 4573, 2399, 4577, 3162, 4579]\n"
     ]
    }
   ],
   "source": [
    "for (i in intArrayOf(0,10)) {\n",
    "    println(\"Words:\" + tokens[i])\n",
    "    println(\"Indices:\" + vocab.getIdxs(tokens[i]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## Putting All Things Together\n",
    "\n",
    "Using the above functions, we package everything into the `loadCorpusTimeMachine` function, which returns `corpus`, a list of token indices, and `vocab`, the vocabulary of the time machine corpus.\n",
    "The modifications we did here are:\n",
    "i) we tokenize text into characters, not words, to simplify the training in later sections;\n",
    "ii) `corpus` is a single list, not a list of token lists, since each text line in the time machine dataset is not necessarily a sentence or a paragraph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170580\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "fun  loadCorpusTimeMachine(maxTokens: Int) : kotlin.Pair<List<Int>, Vocab> {\n",
    "    /* Return token indices and the vocabulary of the time machine dataset. */\n",
    "    val lines = readTimeMachine()\n",
    "    val tokens = tokenize(lines, \"char\");\n",
    "    val vocab = Vocab(tokens, 0, listOf<String>());\n",
    "    // Since each text line in the time machine dataset is not necessarily a\n",
    "    // sentence or a paragraph, flatten all the text lines into a single list\n",
    "    var corpus = tokens.flatten().filter{it !=\"\"}.map{vocab.getIdx(it)}\n",
    "    if (maxTokens > 0) {\n",
    "        corpus = corpus.subList(0, maxTokens);\n",
    "    }\n",
    "    return kotlin.Pair(corpus, vocab);\n",
    "}\n",
    "\n",
    "val corpusVocabPair = loadCorpusTimeMachine(-1);\n",
    "val corpus = corpusVocabPair.first\n",
    "val vocab = corpusVocabPair.second\n",
    "\n",
    "println(corpus.size)\n",
    "println(vocab.length())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Text is an important form of sequence data.\n",
    "* To preprocess text, we usually split text into tokens, build a vocabulary to map token strings into numerical indices, and convert text data into token indices for  models to manipulate.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Tokenization is a key preprocessing step. It varies for different languages. Try to find another three commonly used methods to tokenize text.\n",
    "1. In the experiment of this section, tokenize text into words and vary the `minFreq` arguments of the `Vocab` instance. How does this affect the vocabulary size?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
