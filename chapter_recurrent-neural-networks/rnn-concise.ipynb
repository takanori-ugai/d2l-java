{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Concise Implementation of Recurrent Neural Networks\n",
    ":label:`sec_rnn-concise`\n",
    "\n",
    "While :numref:`sec_rnn_scratch` was instructive to see how RNNs are implemented,\n",
    "this is not convenient or fast.\n",
    "This section will show how to implement the same language model more efficiently\n",
    "using functions provided by high-level APIs\n",
    "of a deep learning framework.\n",
    "We begin as before by reading the time machine dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"6NyNWi\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "       if(!window.letsPlotCallQueue) {\n",
       "           window.letsPlotCallQueue = [];\n",
       "       }; \n",
       "       window.letsPlotCall = function(f) {\n",
       "           window.letsPlotCallQueue.push(f);\n",
       "       };\n",
       "       (function() {\n",
       "           var script = document.createElement(\"script\");\n",
       "           script.type = \"text/javascript\";\n",
       "           script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.4.0/js-package/distr/lets-plot.min.js\";\n",
       "           script.onload = function() {\n",
       "               window.letsPlotCall = function(f) {f();};\n",
       "               window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "               window.letsPlotCallQueue = [];\n",
       "               \n",
       "               \n",
       "           };\n",
       "           script.onerror = function(event) {\n",
       "               window.letsPlotCall = function(f) {};\n",
       "               window.letsPlotCallQueue = [];\n",
       "               var div = document.createElement(\"div\");\n",
       "               div.style.color = 'darkred';\n",
       "               div.textContent = 'Error loading Lets-Plot JS';\n",
       "               document.getElementById(\"6NyNWi\").appendChild(div);\n",
       "           };\n",
       "           var e = document.getElementById(\"6NyNWi\");\n",
       "           e.appendChild(script);\n",
       "       })();\n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%use @file[../djl.json]\n",
    "%use lets-plot\n",
    "@file:DependsOn(\"../D2J-1.0-SNAPSHOT.jar\")\n",
    "//import jp.live.ugai.d2j.attention.Chap10Utils\n",
    "import jp.live.ugai.d2j.timemachine.TimeMachine\n",
    "import jp.live.ugai.d2j.timemachine.Vocab\n",
    "import jp.live.ugai.d2j.timemachine.RNNModelScratch\n",
    "import jp.live.ugai.d2j.SeqDataLoader\n",
    "import jp.live.ugai.d2j.util.StopWatch\n",
    "import jp.live.ugai.d2j.util.Accumulator\n",
    "import jp.live.ugai.d2j.util.Training\n",
    "import kotlin.random.Random\n",
    "import kotlin.Pair\n",
    "import kotlin.collections.List\n",
    "// %load ../utils/djl-imports\n",
    "// %load ../utils/plot-utils\n",
    "// %load ../utils/PlotUtils.java\n",
    "\n",
    "// %load ../utils/Accumulator.java\n",
    "// %load ../utils/Animator.java\n",
    "// %load ../utils/Functions.java\n",
    "// %load ../utils/StopWatch.java\n",
    "// %load ../utils/Training.java\n",
    "// %load ../utils/timemachine/Vocab.java\n",
    "// %load ../utils/timemachine/RNNModelScratch.java\n",
    "// %load ../utils/timemachine/TimeMachine.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ai.djl.training.dataset.Record;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val manager = NDManager.newBaseManager();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataset in DJL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DJL, the ideal and concise way of dealing with datasets, is to use the built-in datasets that can easily wrap around existing NDArrays or to create your own dataset that extends from the `RandomAccessDataset` class. For this section, we will be implementing our own. For more information on creating your own dataset in DJL, you can refer to: https://djl.ai/docs/development/how_to_use_dataset.html\n",
    "\n",
    "Our implementation of `TimeMachineDataset` will be a concise replacement of the `SeqDataLoader` class previously created. Using a dataset in DJL format, will allow us to use already built-in functions so we don't have to implement most things from scratch. We have to implement a Builder, a prepare function which will contain the process to save the data to the TimeMachineDataset object, and finally a get function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeMachineDataset(builder: Builder) : RandomAccessDataset(builder) {\n",
    "    var vocab: Vocab? = null\n",
    "    private var data: NDArray\n",
    "    private var labels: NDArray\n",
    "    private val numSteps: Int\n",
    "    private val maxTokens: Int\n",
    "    private val batchSize: Int\n",
    "    private val manager: NDManager?\n",
    "    private var prepared: Boolean\n",
    "\n",
    "    init {\n",
    "        numSteps = builder.numSteps\n",
    "        maxTokens = builder.maxTokens\n",
    "        batchSize = builder.sampler.batchSize\n",
    "        manager = builder.manager\n",
    "        data = manager!!.create(Shape(0, 35), DataType.INT32)\n",
    "        labels = manager.create(Shape(0, 35), DataType.INT32)\n",
    "        prepared = false\n",
    "    }\n",
    "\n",
    "    override fun get(manager: NDManager, index: Long): Record {\n",
    "        val X = data[NDIndex(\"{}\", index)]\n",
    "        val Y = labels[NDIndex(\"{}\", index)]\n",
    "        return Record(NDList(X), NDList(Y))\n",
    "    }\n",
    "\n",
    "    override fun availableSize(): Long {\n",
    "        return data.shape[0]\n",
    "    }\n",
    "\n",
    "    override fun prepare(progress: Progress?) {\n",
    "        if (prepared) {\n",
    "            return\n",
    "        }\n",
    "        var corpusVocabPair = TimeMachine.loadCorpusTimeMachine(maxTokens)\n",
    "        val corpus: List<Int> = corpusVocabPair.first\n",
    "        vocab = corpusVocabPair.second\n",
    "\n",
    "        // Start with a random offset (inclusive of `numSteps - 1`) to partition a\n",
    "        // sequence\n",
    "        val offset: Int = Random.nextInt(numSteps)\n",
    "        val numTokens = ((corpus.size - offset - 1) / batchSize) * batchSize\n",
    "        var Xs = manager!!.create(corpus.subList(offset, offset + numTokens).toIntArray())\n",
    "        var Ys = manager.create(corpus.subList(offset + 1, offset + 1 + numTokens).toIntArray())\n",
    "        Xs = Xs.reshape(Shape(batchSize.toLong(), -1))\n",
    "        Ys = Ys.reshape(Shape(batchSize.toLong(), -1))\n",
    "        val numBatches = Xs.shape[1].toInt() / numSteps\n",
    "        val xNDList = NDList()\n",
    "        val yNDList = NDList()\n",
    "        var i = 0\n",
    "        while (i < numSteps * numBatches) {\n",
    "            val X = Xs[NDIndex(\":, {}:{}\", i, i + numSteps)]\n",
    "            val Y = Ys[NDIndex(\":, {}:{}\", i, i + numSteps)]\n",
    "            xNDList.add(X)\n",
    "            yNDList.add(Y)\n",
    "            i += numSteps\n",
    "        }\n",
    "        data = NDArrays.concat(xNDList)\n",
    "        xNDList.close()\n",
    "        labels = NDArrays.concat(yNDList)\n",
    "        yNDList.close()\n",
    "        prepared = true\n",
    "    }\n",
    "\n",
    "    class Builder : BaseBuilder<Builder>() {\n",
    "        var numSteps = 0\n",
    "        var maxTokens = 0\n",
    "        var manager: NDManager? = null\n",
    "        override fun self(): Builder {\n",
    "            return this\n",
    "        }\n",
    "\n",
    "        fun setSteps(steps: Int): Builder {\n",
    "            numSteps = steps\n",
    "            return this\n",
    "        }\n",
    "\n",
    "        fun setMaxTokens(maxTokens: Int): Builder {\n",
    "            this.maxTokens = maxTokens\n",
    "            return this\n",
    "        }\n",
    "\n",
    "        fun setManager(manager: NDManager): Builder {\n",
    "            this.manager = manager\n",
    "            return this\n",
    "        }\n",
    "\n",
    "        fun build(): TimeMachineDataset {\n",
    "            return TimeMachineDataset(this)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently we will update our code from the previous section for the functions `predictCh8`, `trainCh8`, `trainEpochCh8`, and `gradClipping` to include the dataset logic and also allow the functions to accept an `AbstractBlock` from DJL instead of just accepting `RNNModelScratch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** Generate new characters following the `prefix`. */\n",
    "fun predictCh8(\n",
    "    prefix: String,\n",
    "    numPreds: Int,\n",
    "    net: Any,\n",
    "    vocab: Vocab,\n",
    "    device: Device,\n",
    "    manager: NDManager\n",
    "): String {\n",
    "    val outputs: MutableList<Int> = ArrayList()\n",
    "    outputs.add(vocab.getIdx(\"\" + prefix[0]))\n",
    "    val getInput = {\n",
    "        manager.create(outputs[outputs.size - 1])\n",
    "            .toDevice(device, false)\n",
    "            .reshape(Shape(1, 1))\n",
    "    }\n",
    "    if (net is RNNModelScratch) {\n",
    "        val castedNet = net\n",
    "        var state: NDList = castedNet.beginState(1, device)\n",
    "        for (c in prefix.substring(1).toCharArray()) { // Warm-up period\n",
    "            state = castedNet.forward(getInput(), state).second\n",
    "            outputs.add(vocab.getIdx(\"\" + c))\n",
    "        }\n",
    "        var y: NDArray\n",
    "        for (i in 0 until numPreds) {\n",
    "            val pair = castedNet.forward(getInput(), state)\n",
    "            y = pair.first\n",
    "            state = pair.second\n",
    "            outputs.add(y.argMax(1).reshape(Shape(1)).getLong(0L).toInt())\n",
    "        }\n",
    "    } else {\n",
    "        val castedNet = net as AbstractBlock\n",
    "        var state: NDList? = null\n",
    "        for (c in prefix.substring(1).toCharArray()) { // Warm-up period\n",
    "            state = if (state == null) {\n",
    "                // Begin state\n",
    "                castedNet\n",
    "                    .forward(\n",
    "                        ParameterStore(manager, false),\n",
    "                        NDList(getInput()),\n",
    "                        false\n",
    "                    )\n",
    "                    .subNDList(1)\n",
    "            } else {\n",
    "                castedNet\n",
    "                    .forward(\n",
    "                        ParameterStore(manager, false),\n",
    "                        NDList(getInput()).addAll(state),\n",
    "                        false\n",
    "                    )\n",
    "                    .subNDList(1)\n",
    "            }\n",
    "            outputs.add(vocab.getIdx(\"\" + c))\n",
    "        }\n",
    "        var y: NDArray\n",
    "        for (i in 0 until numPreds) {\n",
    "            val pair = castedNet.forward(\n",
    "                ParameterStore(manager, false),\n",
    "                NDList(getInput()).addAll(state),\n",
    "                false\n",
    "            )\n",
    "            y = pair[0]\n",
    "            state = pair.subNDList(1)\n",
    "            outputs.add(y.argMax(1).reshape(Shape(1)).getLong(0L).toInt())\n",
    "        }\n",
    "    }\n",
    "    val output = StringBuilder()\n",
    "    for (i in outputs) {\n",
    "        output.append(vocab.idxToToken[i])\n",
    "    }\n",
    "    return output.toString()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** Clip the gradient. */\n",
    "fun gradClipping(net: Any, theta: Int, manager: NDManager) {\n",
    "    var result = 0.0\n",
    "    val params: NDList\n",
    "    if (net is RNNModelScratch) {\n",
    "        params = net.params\n",
    "    } else {\n",
    "        params = NDList()\n",
    "        for (pair in (net as AbstractBlock).parameters) {\n",
    "            params.add(pair.value.array)\n",
    "        }\n",
    "    }\n",
    "    for (p in params) {\n",
    "        val gradient = p.gradient.stopGradient()\n",
    "        gradient.attach(manager)\n",
    "        result += gradient.pow(2).sum().getFloat()\n",
    "    }\n",
    "    val norm = Math.sqrt(result)\n",
    "    if (norm > theta) {\n",
    "        for (param in params) {\n",
    "            val gradient = param.gradient\n",
    "            gradient.muli(theta / norm)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** Train a model within one epoch. */\n",
    "fun trainEpochCh8(\n",
    "    net: Any,\n",
    "    dataset: RandomAccessDataset,\n",
    "    loss: Loss,\n",
    "    updater: (Int, NDManager) -> Unit,\n",
    "    device: Device,\n",
    "    useRandomIter: Boolean,\n",
    "    manager: NDManager\n",
    "): Pair<Double, Double> {\n",
    "    val watch = StopWatch()\n",
    "    watch.start()\n",
    "    val metric = Accumulator(2) // Sum of training loss, no. of tokens\n",
    "    manager.newSubManager().use { childManager ->\n",
    "        var state: NDList? = null\n",
    "        for (batch in dataset.getData(childManager)) {\n",
    "            var X = batch.data.head().toDevice(device, true)\n",
    "            val Y = batch.labels.head().toDevice(device, true)\n",
    "            if (state == null || useRandomIter) {\n",
    "                // Initialize `state` when either it is the first iteration or\n",
    "                // using random sampling\n",
    "                if (net is RNNModelScratch) {\n",
    "                    state = net.beginState(X.shape.shape[0].toInt(), device)\n",
    "                }\n",
    "            } else {\n",
    "                for (s in state) {\n",
    "                    s.stopGradient()\n",
    "                }\n",
    "            }\n",
    "            state?.attach(childManager)\n",
    "            var y = Y.transpose().reshape(Shape(-1))\n",
    "            X = X.toDevice(device, false)\n",
    "            y = y.toDevice(device, false)\n",
    "            Engine.getInstance().newGradientCollector().use { gc ->\n",
    "                val yHat: NDArray\n",
    "                if (net is RNNModelScratch) {\n",
    "                    val pairResult = net.forward(X, state!!)\n",
    "                    yHat = pairResult.first\n",
    "                    state = pairResult.second\n",
    "                } else {\n",
    "                    val pairResult: NDList\n",
    "                    pairResult = if (state == null) {\n",
    "                        // Begin state\n",
    "                        (net as AbstractBlock)\n",
    "                            .forward(\n",
    "                                ParameterStore(manager, false),\n",
    "                                NDList(X),\n",
    "                                true\n",
    "                            )\n",
    "                    } else {\n",
    "                        (net as AbstractBlock)\n",
    "                            .forward(\n",
    "                                ParameterStore(manager, false),\n",
    "                                NDList(X).addAll(state),\n",
    "                                true\n",
    "                            )\n",
    "                    }\n",
    "                    yHat = pairResult[0]\n",
    "                    state = pairResult.subNDList(1)\n",
    "                }\n",
    "                val l = loss.evaluate(NDList(y), NDList(yHat)).mean()\n",
    "                gc.backward(l)\n",
    "                metric.add(floatArrayOf(l.getFloat() * y.size(), y.size().toFloat()))\n",
    "            }\n",
    "            gradClipping(net, 1, childManager)\n",
    "            updater(1, childManager) // Since the `mean` function has been invoked\n",
    "        }\n",
    "    }\n",
    "    return Pair(Math.exp((metric.get(0) / metric.get(1)).toDouble()), metric.get(1) / watch.stop())\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** Train a model. */\n",
    "fun trainCh8(\n",
    "    net: Any,\n",
    "    dataset: RandomAccessDataset,\n",
    "    vocab: Vocab,\n",
    "    lr: Float,\n",
    "    numEpochs: Int,\n",
    "    device: Device,\n",
    "    useRandomIter: Boolean,\n",
    "    manager: NDManager?\n",
    ") {\n",
    "    val loss = SoftmaxCrossEntropyLoss()\n",
    "//    val animator = Animator()\n",
    "    val updater: (Int, NDManager) -> Unit = if (net is RNNModelScratch) {\n",
    "        { batchSize: Int, subManager: NDManager ->\n",
    "            Training.sgd(net.params, lr.toFloat(), batchSize, subManager)\n",
    "        }\n",
    "    } else {\n",
    "        { batchSize: Int, subManager: NDManager ->\n",
    "            // Already initialized net\n",
    "            val castedNet = net as AbstractBlock\n",
    "            val model: Model = Model.newInstance(\"model\")\n",
    "            model.block = castedNet\n",
    "            val lrt: Tracker = Tracker.fixed(lr)\n",
    "            val sgd: Optimizer = Optimizer.sgd().setLearningRateTracker(lrt).build()\n",
    "            val config: DefaultTrainingConfig = DefaultTrainingConfig(loss)\n",
    "                .optOptimizer(sgd) // Optimizer (loss function)\n",
    "                .optInitializer(\n",
    "                    NormalInitializer(0.01f),\n",
    "                    Parameter.Type.WEIGHT\n",
    "                ) // setting the initializer\n",
    "                .optDevices(Engine.getInstance().getDevices(1)) // setting the number of GPUs needed\n",
    "                .addEvaluator(Accuracy()) // Model Accuracy\n",
    "                .addTrainingListeners(*TrainingListener.Defaults.logging()) // Logging\n",
    "            val trainer: Trainer = model.newTrainer(config)\n",
    "            trainer.step()\n",
    "        }\n",
    "    }\n",
    "    val predict: (String) -> String =\n",
    "        { prefix ->\n",
    "            predictCh8(prefix, 50, net, vocab, device, manager!!)\n",
    "        }\n",
    "    // Train and predict\n",
    "    var ppl = 0.0\n",
    "    var speed = 0.0\n",
    "    for (epoch in 0 until numEpochs) {\n",
    "        val pair = trainEpochCh8(net, dataset, loss, updater, device, useRandomIter, manager!!)\n",
    "        ppl = pair.first\n",
    "        speed = pair.second\n",
    "        if ((epoch + 1) % 10 == 0) {\n",
    "//            animator.add(epoch + 1, ppl.toFloat(), \"\")\n",
    "//            animator.show()\n",
    "            println(\"${epoch + 1} : $ppl\")\n",
    "        }\n",
    "    }\n",
    "    println(\n",
    "        \"perplexity: %.1f, %.1f tokens/sec on %s%n\".format(ppl, speed, device.toString())\n",
    "    )\n",
    "    println(predict(\"time traveller\"))\n",
    "    println(predict(\"traveller\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will leverage the dataset that we just created and assign the required parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val batchSize = 32\n",
    "val numSteps = 35\n",
    "\n",
    "    val dataset: TimeMachineDataset = TimeMachineDataset.Builder()\n",
    "        .setManager(manager)\n",
    "        .setMaxTokens(10000)\n",
    "        .setSampling(batchSize, false)\n",
    "        .setSteps(numSteps)\n",
    "        .build()\n",
    "    dataset.prepare()\n",
    "    val vocab = dataset.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 3
   },
   "source": [
    "## Defining the Model\n",
    "\n",
    "High-level APIs provide implementations of recurrent neural networks.\n",
    "We construct the recurrent neural network layer `rnn_layer` with a single hidden layer and 256 hidden units.\n",
    "In fact, we have not even discussed yet what it means to have multiple layers---this will happen in :numref:`sec_deep_rnn`.\n",
    "For now, suffice it to say that multiple layers simply amount to the output of one layer of RNN being used as the input for the next layer of RNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "    val numHiddens = 256\n",
    "    val rnnLayer = RNN.builder()\n",
    "        .setNumLayers(1)\n",
    "        .setStateSize(numHiddens)\n",
    "        .optReturnState(true)\n",
    "        .optBatchFirst(false)\n",
    "        .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "Initializing the hidden state is straightforward.\n",
    "We invoke the member function `beginState` _(In DJL we don't have to run `beginState` to later specify the resulting state the first time we run `forward`, as this logic is ran by DJL the first time we do `forward` but we will create it here for demonstration purposes)_.\n",
    "This returns a list (`state`)\n",
    "that contains\n",
    "an initial hidden state\n",
    "for each example in the minibatch,\n",
    "whose shape is\n",
    "(number of hidden layers, batch size, number of hidden units).\n",
    "For some models \n",
    "to be introduced later \n",
    "(e.g., long short-term memory),\n",
    "such a list also\n",
    "contains other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(1, 32, 256)\n"
     ]
    }
   ],
   "source": [
    "fun beginState(batchSize: Int, numLayers: Int, numHiddens: Int): NDList {\n",
    "    return NDList(manager.zeros(Shape(numLayers.toLong(), batchSize.toLong(), numHiddens.toLong())))\n",
    "}\n",
    "\n",
    "val state = beginState(batchSize, 1, numHiddens)\n",
    "    println(state.size)\n",
    "    println(state[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "With a hidden state and an input,\n",
    "we can compute the output with\n",
    "the updated hidden state.\n",
    "It should be emphasized that\n",
    "the \"output\" (`Y`) of `rnnLayer`\n",
    "does *not* involve computation of output layers:\n",
    "it refers to \n",
    "the hidden state at *each* time step,\n",
    "and they can be used as the input\n",
    "to the subsequent output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "Besides,\n",
    "the updated hidden state (`stateNew`) returned by `rnnLayer`\n",
    "refers to the hidden state\n",
    "at the *last* time step of the minibatch.\n",
    "It can be used to initialize the \n",
    "hidden state for the next minibatch within an epoch\n",
    "in sequential partitioning.\n",
    "For multiple hidden layers,\n",
    "the hidden state of each layer will be stored\n",
    "in this variable (`stateNew`).\n",
    "For some models \n",
    "to be introduced later \n",
    "(e.g., long short-term memory),\n",
    "this variable also\n",
    "contains other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 32, 256)\n",
      "(1, 32, 256)\n"
     ]
    }
   ],
   "source": [
    "    val X = manager.randomUniform(0.0f, 1.0f, Shape(numSteps.toLong(), batchSize.toLong(), vocab!!.length().toLong()))\n",
    "\n",
    "    val input = NDList(X, state[0])\n",
    "    rnnLayer.initialize(manager, DataType.FLOAT32, *input.shapes)\n",
    "    val forwardOutput = rnnLayer.forward(ParameterStore(manager, false), input, false)\n",
    "    val Y = forwardOutput[0]\n",
    "    val stateNew = forwardOutput[1]\n",
    "\n",
    "    println(Y.shape)\n",
    "    println(stateNew.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "Similar to :numref:`sec_rnn_scratch`,\n",
    "we define an `RNNModel` class \n",
    "for a complete RNN model.\n",
    "Note that `rnnLayer` only contains the hidden recurrent layers, we need to create a separate output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(private val rnnLayer: RecurrentBlock, vocabSize: Int) : AbstractBlock() {\n",
    "    private val dense: Linear\n",
    "    private val vocabSize: Int\n",
    "\n",
    "    init {\n",
    "        this.addChildBlock(\"rnn\", rnnLayer)\n",
    "        this.vocabSize = vocabSize\n",
    "        dense = Linear.builder().setUnits(vocabSize.toLong()).build()\n",
    "        this.addChildBlock(\"linear\", dense)\n",
    "    }\n",
    "\n",
    "    override fun forwardInternal(\n",
    "        parameterStore: ParameterStore,\n",
    "        inputs: NDList,\n",
    "        training: Boolean,\n",
    "        params: PairList<String, Any>?\n",
    "    ): NDList {\n",
    "        val X = inputs[0].transpose().oneHot(vocabSize)\n",
    "        inputs[0] = X\n",
    "//        println(inputs)\n",
    "        val result = rnnLayer.forward(parameterStore, inputs, training)\n",
    "        val Y = result[0]\n",
    "        val state = result[1]\n",
    "        val shapeLength = Y.shape.dimension()\n",
    "        val output = dense.forward(\n",
    "            parameterStore,\n",
    "            NDList(Y.reshape(Shape(-1, Y.shape[shapeLength - 1]))),\n",
    "            training\n",
    "        )\n",
    "        return NDList(output[0], state)\n",
    "    }\n",
    "\n",
    "    override fun initializeChildBlocks(manager: NDManager, dataType: DataType, vararg inputShapes: Shape) {\n",
    "        val shape: Shape = rnnLayer.getOutputShapes(arrayOf(inputShapes[0]))[0]\n",
    "        dense.initialize(manager, dataType, Shape(vocabSize.toLong(), shape.get(shape.dimension() - 1)))\n",
    "    }\n",
    "\n",
    "    /* We won't implement this since we won't be using it but it's required as part of an AbstractBlock  */\n",
    "    override fun getOutputShapes(inputShapes: Array<Shape>): Array<Shape?> {\n",
    "        return arrayOfNulls<Shape>(0)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "## Training and Predicting\n",
    "\n",
    "Before training the model, let us make a prediction with the a model that has random weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time travellerjmmjjjjjjj\n"
     ]
    }
   ],
   "source": [
    "    val device = manager.device\n",
    "    val net = RNNModel(rnnLayer, vocab.length())\n",
    "    net.initialize(manager, DataType.FLOAT32, X.shape)\n",
    "    println(predictCh8(\"time traveller\", 10, net, vocab, device, manager))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "As is quite obvious, this model does not work at all. Next, we call `trainCh8` with the same hyperparameters defined in :numref:`sec_rnn_scratch` and train our model with high-level APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 : 8.245257727060578\n",
      "20 : 6.0875992419532245\n",
      "30 : 4.595497889528389\n",
      "40 : 3.822660152024894\n",
      "50 : 3.2301867387474688\n",
      "60 : 2.80533039619512\n",
      "70 : 2.4822068849381775\n",
      "80 : 2.287052411897664\n",
      "90 : 2.061359146862815\n",
      "100 : 1.9161627962336654\n",
      "110 : 1.832307384287594\n",
      "120 : 1.745760329774405\n",
      "130 : 1.6944842883948232\n",
      "140 : 1.6028999438217844\n",
      "150 : 1.5537197947336647\n",
      "160 : 1.5361287295435533\n",
      "170 : 1.4799384901306276\n",
      "180 : 1.4519396279346952\n",
      "190 : 1.425256105226484\n",
      "200 : 1.385519922477976\n",
      "210 : 1.3807876103069892\n",
      "220 : 1.3635620176237664\n",
      "230 : 1.3409062478980134\n",
      "240 : 1.308174846239329\n",
      "250 : 1.3085381352780492\n",
      "260 : 1.297337448067251\n",
      "270 : 1.3003313775132774\n",
      "280 : 1.2786158917626498\n",
      "290 : 1.2667826697630045\n",
      "300 : 1.240752235497341\n",
      "310 : 1.2783358644199587\n",
      "320 : 1.258791214111969\n",
      "330 : 1.2429950474258593\n",
      "340 : 1.2417753937279017\n",
      "350 : 1.2404491141516438\n",
      "360 : 1.221693000372478\n",
      "370 : 1.2273231406785485\n",
      "380 : 1.2258791047534625\n",
      "390 : 1.2214231827293505\n",
      "400 : 1.2357831085253768\n",
      "410 : 1.2150065401582133\n",
      "420 : 1.2369674216004263\n",
      "430 : 1.213300943074838\n",
      "440 : 1.199674360374777\n",
      "450 : 1.2037430446591726\n",
      "460 : 1.192696780250656\n",
      "470 : 1.1916622259898833\n",
      "480 : 1.185772116490552\n",
      "490 : 1.1858539815606262\n",
      "500 : 1.1973099109731464\n",
      "perplexity: 1.2, 29594.0 tokens/sec on cpu()\n",
      "\n",
      "time traveller came what they have to say and so specbly this is\n",
      "traveller came what they have to say and so i filby is that\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "time traveller came what"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val numEpochs: Int = Integer.getInteger(\"MAX_EPOCH\", 500)\n",
    "    val lr = 1.0f\n",
    "    trainCh8(net as Any, dataset, vocab, lr, numEpochs, device, false, manager)\n",
    "    predictCh8(\"time traveller\", 10, net, vocab, device, manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "Compared with the last section, this model achieves comparable perplexity,\n",
    "albeit within a shorter period of time, due to the code being more optimized by\n",
    "high-level APIs of the deep learning framework.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "* High-level APIs of the deep learning framework provides an implementation of the RNN layer.\n",
    "* The RNN layer of high-level APIs returns an output and an updated hidden state, where the output does not involve output layer computation.\n",
    "* Using high-level APIs leads to faster RNN training than using its implementation from scratch.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Can you make the RNN model overfit using the high-level APIs?\n",
    "1. What happens if you increase the number of hidden layers in the RNN model? Can you make the model work?\n",
    "1. Implement the autoregressive model of :numref:`sec_sequence` using an RNN.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
