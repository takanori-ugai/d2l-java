{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Densely Connected Networks (DenseNet)\n",
    "\n",
    "ResNet significantly changed the view of how to parametrize the functions in deep networks. DenseNet is to some extent the logical extension of this. To understand how to arrive at it, let us take a small detour to theory. Recall the Taylor expansion for functions. For scalars it can be written as\n",
    "\n",
    "$$f(x) = f(0) + f'(x) x + \\frac{1}{2} f''(x) x^2 + \\frac{1}{6} f'''(x) x^3 + o(x^3).$$\n",
    "\n",
    "## Function Decomposition\n",
    "\n",
    "The key point is that it decomposes the function into increasingly higher order terms. In a similar vein, ResNet decomposes functions into\n",
    "\n",
    "$$f(\\mathbf{x}) = \\mathbf{x} + g(\\mathbf{x}).$$\n",
    "\n",
    "That is, ResNet decomposes $f$ into a simple linear term and a more complex\n",
    "nonlinear one. What if we want to go beyond two terms? A solution was proposed\n",
    "by :cite:`Huang.Liu.Van-Der-Maaten.ea.2017` in the form of\n",
    "DenseNet, an architecture that reported record performance on the ImageNet\n",
    "dataset.\n",
    "\n",
    "![The main difference between ResNet (left) and DenseNet (right) in cross-layer connections: use of addition and use of concatenation. ](https://raw.githubusercontent.com/d2l-ai/d2l-en/master/img/densenet-block.svg)\n",
    "\n",
    ":label:`fig_densenet_block`\n",
    "\n",
    "\n",
    "As shown in :numref:`fig_densenet_block`, the key difference between ResNet and DenseNet is that in the latter case outputs are *concatenated* rather than added. As a result we perform a mapping from $\\mathbf{x}$ to its values after applying an increasingly complex sequence of functions.\n",
    "\n",
    "$$\\mathbf{x} \\to \\left[\\mathbf{x}, f_1(\\mathbf{x}), f_2(\\mathbf{x}, f_1(\\mathbf{x})), f_3(\\mathbf{x}, f_1(\\mathbf{x}), f_2(\\mathbf{x}, f_1(\\mathbf{x})), \\ldots\\right].$$\n",
    "\n",
    "In the end, all these functions are combined in an MLP to reduce the number of features again. In terms of implementation this is quite simple---rather than adding terms, we concatenate them. The name DenseNet arises from the fact that the dependency graph between variables becomes quite dense. The last layer of such a chain is densely connected to all previous layers. The main components that compose a DenseNet are dense blocks and transition layers. The former defines how the inputs and outputs are concatenated, while the latter controls the number of channels so that it is not too large. The dense connections are shown in :numref:`fig_densenet`.\n",
    "\n",
    "![Dense connections in DenseNet](https://raw.githubusercontent.com/d2l-ai/d2l-en/master/img/densenet.svg)\n",
    "\n",
    ":label:`fig_densenet`\n",
    "\n",
    "\n",
    "\n",
    "## Dense Blocks\n",
    "\n",
    "DenseNet uses the modified \"batch normalization, activation, and convolution\"\n",
    "architecture of ResNet (see the exercise in :numref:`sec_resnet`).\n",
    "First, we implement this architecture in the\n",
    "`conv_block` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"WIp5JD\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "       if(!window.letsPlotCallQueue) {\n",
       "           window.letsPlotCallQueue = [];\n",
       "       }; \n",
       "       window.letsPlotCall = function(f) {\n",
       "           window.letsPlotCallQueue.push(f);\n",
       "       };\n",
       "       (function() {\n",
       "           var script = document.createElement(\"script\");\n",
       "           script.type = \"text/javascript\";\n",
       "           script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.4.0/js-package/distr/lets-plot.min.js\";\n",
       "           script.onload = function() {\n",
       "               window.letsPlotCall = function(f) {f();};\n",
       "               window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "               window.letsPlotCallQueue = [];\n",
       "               \n",
       "               \n",
       "           };\n",
       "           script.onerror = function(event) {\n",
       "               window.letsPlotCall = function(f) {};\n",
       "               window.letsPlotCallQueue = [];\n",
       "               var div = document.createElement(\"div\");\n",
       "               div.style.color = 'darkred';\n",
       "               div.textContent = 'Error loading Lets-Plot JS';\n",
       "               document.getElementById(\"WIp5JD\").appendChild(div);\n",
       "           };\n",
       "           var e = document.getElementById(\"WIp5JD\");\n",
       "           e.appendChild(script);\n",
       "       })();\n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%use @file[../djl.json]\n",
    "%use lets-plot\n",
    "@file:DependsOn(\"../D2J-1.0-SNAPSHOT.jar\")\n",
    "//import jp.live.ugai.d2j.attention.Chap10Utils\n",
    "import jp.live.ugai.d2j.util.Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ai.djl.basicdataset.cv.classification.*;\n",
    "import ai.djl.metric.*;\n",
    "// import org.apache.commons.lang3.ArrayUtils;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "fun convBlock(numChannels: Int): SequentialBlock {\n",
    "    return SequentialBlock()\n",
    "        .add(BatchNorm.builder().build())\n",
    "        .add(Activation::relu)\n",
    "        .add(\n",
    "            Conv2d.builder()\n",
    "                .setFilters(numChannels)\n",
    "                .setKernelShape(Shape(3, 3))\n",
    "                .optPadding(Shape(1, 1))\n",
    "                .optStride(Shape(1, 1))\n",
    "                .build()\n",
    "        )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dense block consists of multiple `convBlock` units, each using the same number of output channels. In the forward computation, however, we concatenate the input and output of each block on the channel dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "class DenseBlock(numConvs: Int, numChannels: Int) : AbstractBlock(VERSION) {\n",
    "    var net = SequentialBlock()\n",
    "\n",
    "    init {\n",
    "        for (i in 0 until numConvs) {\n",
    "            net.add(addChildBlock(\"denseBlock$i\", convBlock(numChannels)))\n",
    "        }\n",
    "    }\n",
    "\n",
    "    override fun toString(): String {\n",
    "        return \"DenseBlock()\"\n",
    "    }\n",
    "\n",
    "    override fun forwardInternal(\n",
    "        parameterStore: ParameterStore,\n",
    "        X: NDList,\n",
    "        training: Boolean,\n",
    "        params: PairList<String, Any>?\n",
    "    ): NDList {\n",
    "        var X = X\n",
    "        var Y: NDArray\n",
    "        for (block in net.children.values()) {\n",
    "            Y = block.forward(parameterStore, X, training).singletonOrThrow()\n",
    "            X = NDList(NDArrays.concat(NDList(X.singletonOrThrow(), Y), 1))\n",
    "        }\n",
    "        return X\n",
    "    }\n",
    "\n",
    "    override fun getOutputShapes(inputs: Array<Shape>): Array<Shape> {\n",
    "        val shapesX: Array<Shape> = inputs\n",
    "        for (block in net.children.values()) {\n",
    "            val shapesY: Array<Shape> = block.getOutputShapes(shapesX)\n",
    "            shapesX[0] = Shape(\n",
    "                shapesX[0].get(0),\n",
    "                shapesY[0].get(1) + shapesX[0].get(1),\n",
    "                shapesX[0].get(2),\n",
    "                shapesX[0].get(3)\n",
    "            )\n",
    "        }\n",
    "        return shapesX\n",
    "    }\n",
    "\n",
    "    override fun initializeChildBlocks(manager: NDManager?, dataType: DataType?, vararg inputShapes: Shape) {\n",
    "        var shapesX: Shape = inputShapes[0]\n",
    "        for (block in net.children.values()) {\n",
    "            block.initialize(manager, DataType.FLOAT32, shapesX)\n",
    "            val shapesY: Array<Shape> = block.getOutputShapes(arrayOf(shapesX))\n",
    "            shapesX = Shape(\n",
    "                shapesX.get(0),\n",
    "                shapesY[0].get(1) + shapesX.get(1),\n",
    "                shapesX.get(2),\n",
    "                shapesX.get(3)\n",
    "            )\n",
    "        }\n",
    "    }\n",
    "\n",
    "    companion object {\n",
    "        private const val VERSION: Byte = 1\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we define a convolution block (`DenseBlock`) with two blocks of 10 output channels. When using an input with 3 channels, we will get an output with the $3+2\\times 10=23$ channels. The number of convolution block channels controls the increase in the number of output channels relative to the number of input channels. This is also referred to as the growth rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 23, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "    val manager = NDManager.newBaseManager()\n",
    "    var block = SequentialBlock().add(DenseBlock(2, 10))\n",
    "\n",
    "    val X = manager.randomUniform(0.0f, 1.0f, Shape(4, 3, 8, 8))\n",
    "\n",
    "    block.initialize(manager, DataType.FLOAT32, X.shape)\n",
    "\n",
    "    val parameterStore = ParameterStore(manager, true)\n",
    "\n",
    "    var currentShape = arrayOf<Shape>(X.shape)\n",
    "    for (child in block.children.values()) {\n",
    "        currentShape = child.getOutputShapes(currentShape)\n",
    "    }\n",
    "\n",
    "    println(currentShape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Layers\n",
    "\n",
    "Since each dense block will increase the number of channels, adding too many of them will lead to an excessively complex model. A transition layer is used to control the complexity of the model. It reduces the number of channels by using the $1\\times 1$ convolutional layer and halves the height and width of the average pooling layer with a stride of 2, further reducing the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "fun transitionBlock(numChannels: Int): SequentialBlock {\n",
    "    return SequentialBlock()\n",
    "        .add(BatchNorm.builder().build())\n",
    "        .add { arrays: NDList? -> Activation.relu(arrays) }\n",
    "        .add(\n",
    "            Conv2d.builder()\n",
    "                .setFilters(numChannels)\n",
    "                .setKernelShape(Shape(1, 1))\n",
    "                .optStride(Shape(1, 1))\n",
    "                .build()\n",
    "        )\n",
    "        .add(Pool.avgPool2dBlock(Shape(2, 2), Shape(2, 2)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a transition layer with 10 channels to the output of the dense block in the previous example.  This reduces the number of output channels to 10, and halves the height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "    block = transitionBlock(10)\n",
    "\n",
    "    block.initialize(manager, DataType.FLOAT32, currentShape[0])\n",
    "\n",
    "    for (pair in block.children) {\n",
    "        currentShape = pair.value.getOutputShapes(currentShape)\n",
    "    }\n",
    "\n",
    "    println(currentShape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet Model\n",
    "\n",
    "Next, we will construct a DenseNet model. DenseNet first uses the same single convolutional layer and maximum pooling layer as ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    val net = SequentialBlock()\n",
    "        .add(\n",
    "            Conv2d.builder()\n",
    "                .setFilters(64)\n",
    "                .setKernelShape(Shape(7, 7))\n",
    "                .optStride(Shape(2, 2))\n",
    "                .optPadding(Shape(3, 3))\n",
    "                .build()\n",
    "        )\n",
    "        .add(BatchNorm.builder().build())\n",
    "        .add { arrays: NDList? -> Activation.relu(arrays) }\n",
    "        .add(Pool.maxPool2dBlock(Shape(3, 3), Shape(2, 2), Shape(1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, similar to the four residual blocks that ResNet uses, DenseNet uses four dense blocks. Similar to ResNet, we can set the number of convolutional layers used in each dense block. Here, we set it to 4, consistent with the ResNet-18 in the previous section. Furthermore, we set the number of channels (i.e., growth rate) for the convolutional layers in the dense block to 32, so 128 channels will be added to each dense block.\n",
    "\n",
    "In ResNet, the height and width are reduced between each module by a residual block with a stride of 2. Here, we use the transition layer to halve the height and width and halve the number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "    var numChannels: Int = 64\n",
    "    val growthRate = 32\n",
    "\n",
    "    val numConvsInDenseBlocks = intArrayOf(4, 4, 4, 4)\n",
    "\n",
    "    for (index in numConvsInDenseBlocks.indices) {\n",
    "        val numConvs = numConvsInDenseBlocks[index]\n",
    "        net.add(DenseBlock(numConvs, growthRate))\n",
    "        numChannels += numConvs * growthRate\n",
    "        if (index != numConvsInDenseBlocks.size - 1) {\n",
    "            numChannels = numChannels / 2\n",
    "            net.add(transitionBlock(numChannels))\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to ResNet, a global pooling layer and fully connected layer are connected at the end to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequentialBlock {\n",
      "\tConv2d\n",
      "\tBatchNorm\n",
      "\tLambdaBlock\n",
      "\tmaxPool2d\n",
      "\tDenseBlock {\n",
      "\t\tdenseBlock0 {\n",
      "\t\t\tBatchNorm\n",
      "\t\t\tLambdaBlock\n",
      "\t\t\tConv2d\n",
      "\t\t}\n",
      "\t\tdenseBlock1 {\n",
      "\t\t\tBatchNorm\n",
      "\t\t\tLambdaBlock\n",
      "\t\t\tConv2d\n",
      "\t\t}\n",
      "\t\tdenseBlock2 {\n",
      "\t\t\tBatchNorm\n",
      "\t\t\tLambdaBlock\n",
      "\t\t\tConv2d\n",
      "\t\t}\n",
      "\t\tdenseBlock3 {\n",
      "\t\t\tBatchNorm\n",
      "\t\t\tLambdaBlock\n",
      "\t\t\tConv2d\n",
      "\t\t}\n",
      "\t}\n",
      "\tSequentialBlock {\n",
      "\t\tBatchNorm\n",
      "\t\tLambdaBlock\n",
      "\t\tConv2d\n",
      "\t\tavgPool2d\n",
      "\t}\n",
      "\tDenseBlock {\n",
      "\t\tdenseBlock0 {\n",
      "\t\t\tBatchNorm\n",
      "\t\t\tLambdaBlock\n",
      "\t\t\tConv2d\n",
      "\t\t}\n",
      "\t\tdenseBlock1 {\n",
      "\t\t\tBatchNorm\n",
      "\t\t\tLambdaBlock\n",
      "\t\t\tConv2d\n",
      "\t\t}\n",
      "\t\tdenseBlock2 {\n",
      "\t\t\tBatchNorm\n",
      "\t\t\tLambdaBlock\n",
      "\t\t\tConv2d\n",
      "\t\t}\n",
      "\t\tdenseBlock3 {\n",
      "\t\t\tBatchNorm\n",
      "\t\t\tLambdaBlock\n",
      "\t\t\tConv2d\n",
      "\t\t}\n",
      "\t}\n",
      "\tSequentialBlock {\n",
      "\t\tBatchNorm\n",
      "\t\tLambdaBlock\n",
      "\t\tConv2d\n",
      "\t\tavgPool2d\n",
      "\t}\n",
      "\tDenseBlock {\n",
      "\t\tdenseBlock0 {\n",
      "\t\t\tBatchNorm\n",
      "\t\t\tLambdaBlock\n",
      "\t\t\tConv2d\n",
      "\t\t}\n",
      "\t\tdenseBlock1 {\n",
      "\t\t\tBatchNorm\n",
      "\t\t\tLambdaBlock\n",
      "\t\t\tConv2d\n",
      "\t\t}\n",
      "\t\tdenseBlock2 {\n",
      "\t\t\tBatchNorm\n",
      "\t\t\tLambdaBlock\n",
      "\t\t\tConv2d\n",
      "\t\t}\n",
      "\t\tdenseBlock3 {\n",
      "\t\t\tBatchNorm\n",
      "\t\t\tLambdaBlock\n",
      "\t\t\tConv2d\n",
      "\t\t}\n",
      "\t}\n",
      "\tSequentialBlock {\n",
      "\t\tBatchNorm\n",
      "\t\tLambdaBlock\n",
      "\t\tConv2d\n",
      "\t\tavgPool2d\n",
      "\t}\n",
      "\tDenseBlock {\n",
      "\t\tdenseBlock0 {\n",
      "\t\t\tBatchNorm\n",
      "\t\t\tLambdaBlock\n",
      "\t\t\tConv2d\n",
      "\t\t}\n",
      "\t\tdenseBlock1 {\n",
      "\t\t\tBatchNorm\n",
      "\t\t\tLambdaBlock\n",
      "\t\t\tConv2d\n",
      "\t\t}\n",
      "\t\tdenseBlock2 {\n",
      "\t\t\tBatchNorm\n",
      "\t\t\tLambdaBlock\n",
      "\t\t\tConv2d\n",
      "\t\t}\n",
      "\t\tdenseBlock3 {\n",
      "\t\t\tBatchNorm\n",
      "\t\t\tLambdaBlock\n",
      "\t\t\tConv2d\n",
      "\t\t}\n",
      "\t}\n",
      "\tBatchNorm\n",
      "\tLambdaBlock\n",
      "\tglobalAvgPool2d\n",
      "\tLinear\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "    net\n",
    "        .add(BatchNorm.builder().build())\n",
    "        .add(Activation::relu)\n",
    "        .add(Pool.globalAvgPool2dBlock())\n",
    "        .add(Linear.builder().setUnits(10).build())\n",
    "\n",
    "    println(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition and Training\n",
    "\n",
    "Since we are using a deeper network here, in this section, we will reduce the input height and width from 224 to 96 to simplify the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.83, SoftmaxCrossEntropyLoss: 0.51\n",
      "Validating:  100% |████████████████████████████████████████|\n",
      "Training:     44% |██████████████████                      | Accuracy: 0.89, SoftmaxCrossEntropyLoss: 0.31"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The execution was interrupted"
     ]
    }
   ],
   "source": [
    "    val batchSize = 256\n",
    "    val lr = 0.1f\n",
    "    val numEpochs = Integer.getInteger(\"MAX_EPOCH\", 10)\n",
    "\n",
    "//    var trainLoss: DoubleArray\n",
    "//    var testAccuracy: DoubleArray\n",
    "//    var trainAccuracy: DoubleArray\n",
    "\n",
    "    val epochCount = IntArray(numEpochs) { it + 1 }\n",
    "\n",
    "    val trainIter = FashionMnist.builder()\n",
    "        .addTransform(Resize(96))\n",
    "        .addTransform(ToTensor())\n",
    "        .optUsage(Dataset.Usage.TRAIN)\n",
    "        .setSampling(batchSize, true)\n",
    "        .optLimit(java.lang.Long.getLong(\"DATASET_LIMIT\", Long.MAX_VALUE))\n",
    "        .build()\n",
    "\n",
    "    val testIter = FashionMnist.builder()\n",
    "        .addTransform(Resize(96))\n",
    "        .addTransform(ToTensor())\n",
    "        .optUsage(Dataset.Usage.TEST)\n",
    "        .setSampling(batchSize, true)\n",
    "        .optLimit(java.lang.Long.getLong(\"DATASET_LIMIT\", Long.MAX_VALUE))\n",
    "        .build()\n",
    "\n",
    "    trainIter.prepare()\n",
    "    testIter.prepare()\n",
    "\n",
    "    val model = Model.newInstance(\"cnn\")\n",
    "    model.block = net\n",
    "\n",
    "    val loss: Loss = Loss.softmaxCrossEntropyLoss()\n",
    "\n",
    "    val lrt: Tracker = Tracker.fixed(lr)\n",
    "    val sgd: Optimizer = Optimizer.sgd().setLearningRateTracker(lrt).build()\n",
    "\n",
    "    val config = DefaultTrainingConfig(loss).optOptimizer(sgd) // Optimizer (loss function)\n",
    "        .addEvaluator(Accuracy()) // Model Accuracy\n",
    "        .addTrainingListeners(*TrainingListener.Defaults.logging()) // Logging\n",
    "\n",
    "    val trainer: Trainer = model.newTrainer(config)\n",
    "    trainer.initialize(Shape(1, 1, 96, 96))\n",
    "\n",
    "    val evaluatorMetrics: MutableMap<String, DoubleArray> = mutableMapOf()\n",
    "    val avgTrainTimePerEpoch = Training.trainingChapter6(trainIter, testIter, numEpochs, trainer, evaluatorMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "null\n",
      "java.lang.NullPointerException\n",
      "\tat Line_56.<init>(Line_56.jupyter-kts:1)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.evalWithConfigAndOtherScriptsResults(BasicJvmScriptEvaluator.kt:105)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.invoke$suspendImpl(BasicJvmScriptEvaluator.kt:47)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.invoke(BasicJvmScriptEvaluator.kt)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmReplEvaluator.eval(BasicJvmReplEvaluator.kt:49)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.InternalEvaluatorImpl$eval$resultWithDiagnostics$1.invokeSuspend(InternalEvaluatorImpl.kt:103)\n",
      "\tat kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\n",
      "\tat kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:106)\n",
      "\tat kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:284)\n",
      "\tat kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:85)\n",
      "\tat kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:59)\n",
      "\tat kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source)\n",
      "\tat kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:38)\n",
      "\tat kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.InternalEvaluatorImpl.eval(InternalEvaluatorImpl.kt:103)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl$execute$1$result$1.invoke(CellExecutorImpl.kt:71)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl$execute$1$result$1.invoke(CellExecutorImpl.kt:69)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.withHost(repl.kt:635)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl.execute(CellExecutorImpl.kt:69)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.CellExecutor$DefaultImpls.execute$default(CellExecutor.kt:15)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl$evalEx$1.invoke(repl.kt:444)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl$evalEx$1.invoke(repl.kt:433)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.withEvalContext(repl.kt:397)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.evalEx(repl.kt:433)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.eval(repl.kt:485)\n",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.ProtocolKt$shellMessagesHandler$2$res$1.invoke(protocol.kt:321)\n",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.ProtocolKt$shellMessagesHandler$2$res$1.invoke(protocol.kt:320)\n",
      "\tat org.jetbrains.kotlinx.jupyter.JupyterExecutorImpl$runExecution$execThread$1.invoke(execution.kt:33)\n",
      "\tat org.jetbrains.kotlinx.jupyter.JupyterExecutorImpl$runExecution$execThread$1.invoke(execution.kt:31)\n",
      "\tat kotlin.concurrent.ThreadsKt$thread$thread$1.run(Thread.kt:30)\n"
     ]
    }
   ],
   "source": [
    "    val trainLoss = evaluatorMetrics.get(\"train_epoch_SoftmaxCrossEntropyLoss\")\n",
    "    val trainAccuracy = evaluatorMetrics.get(\"train_epoch_Accuracy\")\n",
    "    val testAccuracy = evaluatorMetrics.get(\"validate_epoch_Accuracy\")\n",
    "\n",
    "    print(\"loss %.3f,\".format(trainLoss!![numEpochs - 1]))\n",
    "    print(\" train acc %.3f,\".format(trainAccuracy!![numEpochs - 1]))\n",
    "    print(\" test acc %.3f\\n\".format(testAccuracy!![numEpochs - 1]))\n",
    "    println(\"%.1f examples/sec\".format(trainIter.size() / (avgTrainTimePerEpoch / Math.pow(10.0, 9.0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Contour Gradient Descent.](https://d2l-java-resources.s3.amazonaws.com/img/chapter_convolution-modern-cnn-denseNet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "null\n",
      "java.lang.NullPointerException\n",
      "\tat Line_58.<init>(Line_58.jupyter-kts:1)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.evalWithConfigAndOtherScriptsResults(BasicJvmScriptEvaluator.kt:105)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.invoke$suspendImpl(BasicJvmScriptEvaluator.kt:47)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.invoke(BasicJvmScriptEvaluator.kt)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmReplEvaluator.eval(BasicJvmReplEvaluator.kt:49)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.InternalEvaluatorImpl$eval$resultWithDiagnostics$1.invokeSuspend(InternalEvaluatorImpl.kt:103)\n",
      "\tat kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\n",
      "\tat kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:106)\n",
      "\tat kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:284)\n",
      "\tat kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:85)\n",
      "\tat kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:59)\n",
      "\tat kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source)\n",
      "\tat kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:38)\n",
      "\tat kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.InternalEvaluatorImpl.eval(InternalEvaluatorImpl.kt:103)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl$execute$1$result$1.invoke(CellExecutorImpl.kt:71)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl$execute$1$result$1.invoke(CellExecutorImpl.kt:69)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.withHost(repl.kt:635)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl.execute(CellExecutorImpl.kt:69)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.CellExecutor$DefaultImpls.execute$default(CellExecutor.kt:15)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl$evalEx$1.invoke(repl.kt:444)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl$evalEx$1.invoke(repl.kt:433)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.withEvalContext(repl.kt:397)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.evalEx(repl.kt:433)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.eval(repl.kt:485)\n",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.ProtocolKt$shellMessagesHandler$2$res$1.invoke(protocol.kt:321)\n",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.ProtocolKt$shellMessagesHandler$2$res$1.invoke(protocol.kt:320)\n",
      "\tat org.jetbrains.kotlinx.jupyter.JupyterExecutorImpl$runExecution$execThread$1.invoke(execution.kt:33)\n",
      "\tat org.jetbrains.kotlinx.jupyter.JupyterExecutorImpl$runExecution$execThread$1.invoke(execution.kt:31)\n",
      "\tat kotlin.concurrent.ThreadsKt$thread$thread$1.run(Thread.kt:30)\n"
     ]
    }
   ],
   "source": [
    "val trainLossLabel =  Array<String>(trainLoss!!.size) { \"train loss\" }\n",
    "val trainAccLabel = Array<String>(trainLoss!!.size) { \"train acc\" }\n",
    "val testAccLabel = Array<String>(trainLoss!!.size) { \"test acc\" }\n",
    "val data = mapOf<String, Any>(\n",
    "      \"label\" to trainLossLabel + trainAccLabel + testAccLabel,\n",
    "      \"epoch\" to epochCount + epochCount + epochCount,\n",
    "      \"metrics\" to trainLoss!! + trainAccuracy!! + testAccuracy!!\n",
    ")\n",
    "\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine { x = \"epoch\" ; y = \"metrics\" ; color = \"label\"}\n",
    "plot + ggsize(700, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* In terms of cross-layer connections, unlike ResNet, where inputs and outputs are added together, DenseNet concatenates inputs and outputs on the channel dimension.\n",
    "* The main units that compose DenseNet are dense blocks and transition layers.\n",
    "* We need to keep the dimensionality under control when composing the network by adding transition layers that shrink the number of channels again.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Why do we use average pooling rather than max-pooling in the transition layer?\n",
    "1. One of the advantages mentioned in the DenseNet paper is that its model parameters are smaller than those of ResNet. Why is this the case?\n",
    "1. One problem for which DenseNet has been criticized is its high memory consumption.\n",
    "    * Is this really the case? Try to change the input shape to $224\\times 224$ to see the actual (GPU) memory consumption.\n",
    "    * Can you think of an alternative means of reducing the memory consumption? How would you need to change the framework?\n",
    "1. Implement the various DenseNet versions presented in Table 1 of :cite:`Huang.Liu.Van-Der-Maaten.ea.2017`.\n",
    "1. Why do we not need to concatenate terms if we are just interested in $\\mathbf{x}$ and $f(\\mathbf{x})$ for ResNet? Why do we need this for more than two layers in DenseNet?\n",
    "1. Design a DenseNet for fully connected networks and apply it to the Housing Price prediction task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
