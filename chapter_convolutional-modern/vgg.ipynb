{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks Using Blocks (VGG)\n",
    "\n",
    ":label:`sec_vgg`\n",
    "\n",
    "\n",
    "While AlexNet proved that deep convolutional neural networks\n",
    "can achieve good results, it did not offer a general template\n",
    "to guide subsequent researchers in designing new networks.\n",
    "In the following sections, we will introduce several heuristic concepts\n",
    "commonly used to design deep networks.\n",
    "\n",
    "Progress in this field mirrors that in chip design\n",
    "where engineers went from placing transistors\n",
    "to logical elements to logic blocks.\n",
    "Similarly, the design of neural network architectures\n",
    "had grown progressively more abstract,\n",
    "with researchers moving from thinking in terms of\n",
    "individual neurons to whole layers,\n",
    "and now to blocks, repeating patterns of layers.\n",
    "\n",
    "The idea of using blocks first emerged from the\n",
    "[Visual Geometry Group](http://www.robots.ox.ac.uk/~vgg/) (VGG)\n",
    "at Oxford University,\n",
    "in their eponymously-named VGG network.\n",
    "It is easy to implement these repeated structures in code\n",
    "with any modern deep learning framework by using loops and subroutines.\n",
    "\n",
    "\n",
    "## VGG Blocks\n",
    "\n",
    "The basic building block of classic convolutional networks\n",
    "is a sequence of the following layers:\n",
    "(i) a convolutional layer\n",
    "(with padding to maintain the resolution),\n",
    "(ii) a nonlinearity such as a ReLU, (iii) a pooling layer such \n",
    "as a max pooling layer. \n",
    "One VGG block consists of a sequence of convolutional layers,\n",
    "followed by a max pooling layer for spatial downsampling.\n",
    "In the original VGG paper :cite:`Simonyan.Zisserman.2014`,\n",
    "the authors \n",
    "employed convolutions with $3\\times3$ kernels\n",
    "and $2 \\times 2$ max pooling with stride of $2$\n",
    "(halving the resolution after each block).\n",
    "In the code below, we define a function called `vggBlock`\n",
    "to implement one VGG block.\n",
    "The function takes two arguments\n",
    "corresponding to the number of convolutional layers `numConvs`\n",
    "and the number of output channels `numChannels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"JhwiLM\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "       if(!window.letsPlotCallQueue) {\n",
       "           window.letsPlotCallQueue = [];\n",
       "       }; \n",
       "       window.letsPlotCall = function(f) {\n",
       "           window.letsPlotCallQueue.push(f);\n",
       "       };\n",
       "       (function() {\n",
       "           var script = document.createElement(\"script\");\n",
       "           script.type = \"text/javascript\";\n",
       "           script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.4.0/js-package/distr/lets-plot.min.js\";\n",
       "           script.onload = function() {\n",
       "               window.letsPlotCall = function(f) {f();};\n",
       "               window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "               window.letsPlotCallQueue = [];\n",
       "               \n",
       "               \n",
       "           };\n",
       "           script.onerror = function(event) {\n",
       "               window.letsPlotCall = function(f) {};\n",
       "               window.letsPlotCallQueue = [];\n",
       "               var div = document.createElement(\"div\");\n",
       "               div.style.color = 'darkred';\n",
       "               div.textContent = 'Error loading Lets-Plot JS';\n",
       "               document.getElementById(\"JhwiLM\").appendChild(div);\n",
       "           };\n",
       "           var e = document.getElementById(\"JhwiLM\");\n",
       "           e.appendChild(script);\n",
       "       })();\n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%use @file[../djl.json]\n",
    "%use lets-plot\n",
    "@file:DependsOn(\"../D2J-1.0-SNAPSHOT.jar\")\n",
    "//import jp.live.ugai.d2j.attention.Chap10Utils\n",
    "import jp.live.ugai.d2j.util.Training\n",
    "import ai.djl.metric.Metrics\n",
    "\n",
    "fun getLong(nm: String, n: Long): Long {\n",
    "    val name = System.getProperty(nm)\n",
    "    return if (null == name) n.toLong() else name.toLong()\n",
    "}\n",
    "\n",
    "// %load ../utils/djl-imports\n",
    "// %load ../utils/plot-utils\n",
    "// %load ../utils/DataPoints.java\n",
    "// %load ../utils/Training.java\n",
    "// %load ../utils/Accumulator.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ai.djl.basicdataset.cv.classification.*;\n",
    "// import org.apache.commons.lang3.ArrayUtils;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "fun vggBlock(numConvs: Int,numChannels: Int) : SequentialBlock{\n",
    "\n",
    "    val tempBlock = SequentialBlock();\n",
    "    for (i in 0 until numConvs) {\n",
    "        // DJL has default stride of 1x1, so don't need to set it explicitly.\n",
    "        tempBlock\n",
    "                .add(Conv2d.builder()\n",
    "                        .setFilters(numChannels)\n",
    "                        .setKernelShape(Shape(3, 3))\n",
    "                        .optPadding(Shape(1, 1))\n",
    "                        .build()\n",
    "                )\n",
    "                .add(Activation::relu);\n",
    "    }\n",
    "    tempBlock.add(Pool.maxPool2dBlock(Shape(2, 2), Shape(2, 2)));\n",
    "    return tempBlock;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG Network\n",
    "\n",
    "Like AlexNet and LeNet,\n",
    "the VGG Network can be partitioned into two parts:\n",
    "the first consisting mostly of convolutional and pooling layers\n",
    "and a second consisting of fully-connected layers.\n",
    "The convolutional portion of the net connects several `vggBlock` modules\n",
    "in succession.\n",
    "In :numref:`fig_vgg`, the variable `convArch` consists of a list of tuples (one per block),\n",
    "where each contains two values: the number of convolutional layers\n",
    "and the number of output channels,\n",
    "which are precisely the arguments requires to call\n",
    "the `vggBlock` function.\n",
    "The fully-connected module is identical to that covered in AlexNet.\n",
    "\n",
    "![Designing a network from building blocks](https://raw.githubusercontent.com/d2l-ai/d2l-en/master/img/vgg.svg)\n",
    "\n",
    ":width:`400px`\n",
    "\n",
    "\n",
    ":label:`fig_vgg`\n",
    "\n",
    "\n",
    "The original VGG network had 5 convolutional blocks,\n",
    "among which the first two have one convolutional layer each\n",
    "and the latter three contain two convolutional layers each.\n",
    "The first block has 64 output channels\n",
    "and each subsequent block doubles the number of output channels,\n",
    "until that number reaches $512$.\n",
    "Since this network uses $8$ convolutional layers\n",
    "and $3$ fully-connected layers, it is often called VGG-11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "val convArch = arrayOf(intArrayOf(1, 64), intArrayOf(1, 128), intArrayOf(2, 256), intArrayOf(2, 512), intArrayOf(2, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements VGG-11. This is a simple matter of executing a for loop over `convArch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "fun  VGG(convArch : Array<IntArray>) : SequentialBlock {\n",
    "\n",
    "    val block = SequentialBlock();\n",
    "    // The convolutional layer part\n",
    "    for (i in 0 until  convArch.size) {\n",
    "        block.add(vggBlock(convArch[i][0], convArch[i][1]));\n",
    "    }\n",
    "\n",
    "    // The fully connected layer part\n",
    "    block\n",
    "        .add(Blocks.batchFlattenBlock())\n",
    "        .add(Linear\n",
    "                .builder()\n",
    "                .setUnits(4096)\n",
    "                .build())\n",
    "        .add(Activation::relu)\n",
    "        .add(Dropout\n",
    "                .builder()\n",
    "                .optRate(0.5f)\n",
    "                .build())\n",
    "        .add(Linear\n",
    "                .builder()\n",
    "                .setUnits(4096)\n",
    "                .build())\n",
    "        .add(Activation::relu)\n",
    "        .add(Dropout\n",
    "                .builder()\n",
    "                .optRate(0.5f)\n",
    "                .build())\n",
    "        .add(Linear.builder().setUnits(10).build());\n",
    "    \n",
    "    return block;\n",
    "}\n",
    "\n",
    "val block = VGG(convArch);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will construct a single-channel data example\n",
    "with a height and width of 224 to observe the output shape of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01SequentialBlock layer output : (1, 64, 112, 112)\n",
      "02SequentialBlock layer output : (1, 128, 56, 56)\n",
      "03SequentialBlock layer output : (1, 256, 28, 28)\n",
      "04SequentialBlock layer output : (1, 512, 14, 14)\n",
      "05SequentialBlock layer output : (1, 512, 7, 7)\n",
      "06LambdaBlock layer output : (1, 25088)\n",
      "07Linear layer output : (1, 4096)\n",
      "08LambdaBlock layer output : (1, 4096)\n",
      "09Dropout layer output : (1, 4096)\n",
      "10Linear layer output : (1, 4096)\n",
      "11LambdaBlock layer output : (1, 4096)\n",
      "12Dropout layer output : (1, 4096)\n",
      "13Linear layer output : (1, 10)\n"
     ]
    }
   ],
   "source": [
    "val lr = 0.05f;\n",
    "val model = Model.newInstance(\"vgg-display\");\n",
    "model.setBlock(block);\n",
    "\n",
    "val loss = Loss.softmaxCrossEntropyLoss();\n",
    "\n",
    "val lrt = Tracker.fixed(lr);\n",
    "val sgd = Optimizer.sgd().setLearningRateTracker(lrt).build();\n",
    "\n",
    "val config = DefaultTrainingConfig(loss).optOptimizer(sgd) // Optimizer (loss function)\n",
    "                .optDevices(Engine.getInstance().getDevices(1)) // single GPU\n",
    "                .addEvaluator(Accuracy()) // Model Accuracy\n",
    "                .addTrainingListeners(*TrainingListener.Defaults.logging()); // Logging\n",
    "\n",
    "val trainer = model.newTrainer(config);\n",
    "\n",
    "val inputShape = Shape(1, 1, 224, 224);\n",
    "\n",
    "val manager = NDManager.newBaseManager()\n",
    "    val X = manager.randomUniform(0f, 1.0f, inputShape);\n",
    "    trainer.initialize(inputShape);\n",
    "\n",
    "    var currentShape = X.getShape();\n",
    "\n",
    "    for (i in 0 until block.getChildren().size()) {\n",
    "        val newShape = block.getChildren().get(i).getValue().getOutputShapes(arrayOf<Shape>(currentShape))\n",
    "        currentShape = newShape[0];\n",
    "        println(block.getChildren().get(i).getKey() + \" layer output : \" + currentShape);\n",
    "    }\n",
    "manager.close()\n",
    "// save memory on VGG params\n",
    "model.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we halve height and width at each block,\n",
    "finally reaching a height and width of 7\n",
    "before flattening the representations\n",
    "for processing by the fully-connected layer.\n",
    "\n",
    "## Model Training\n",
    "\n",
    "Since VGG-11 is more computationally-heavy than AlexNet\n",
    "we construct a network with a smaller number of channels.\n",
    "This is more than sufficient for training on Fashion-MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "val ratio = 4;\n",
    "\n",
    "for(i in 0 until convArch.size){\n",
    "    convArch[i][1] = convArch[i][1] / ratio;\n",
    "}\n",
    "\n",
    "val inputShape = Shape(1, 1, 96, 96); // resize the input shape to save memory\n",
    "\n",
    "val model = Model.newInstance(\"vgg-tiny\");\n",
    "val newBlock = VGG(convArch);\n",
    "model.setBlock(newBlock);\n",
    "val loss = Loss.softmaxCrossEntropyLoss();\n",
    "\n",
    "val lrt = Tracker.fixed(lr);\n",
    "val sgd = Optimizer.sgd().setLearningRateTracker(lrt).build();\n",
    "\n",
    "val config = DefaultTrainingConfig(loss).optOptimizer(sgd) // Optimizer (loss function)\n",
    "                .optDevices(Engine.getInstance().getDevices(1)) // single GPU\n",
    "                .addEvaluator(Accuracy()) // Model Accuracy\n",
    "                .addTrainingListeners(*TrainingListener.Defaults.logging()); // Logging\n",
    "\n",
    "val trainer = model.newTrainer(config);\n",
    "trainer.initialize(inputShape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val batchSize = 128;\n",
    "val numEpochs = Integer.getInteger(\"MAX_EPOCH\", 10);\n",
    "\n",
    "//double[] trainLoss;\n",
    "//double[] testAccuracy;\n",
    "//double[] epochCount;\n",
    "//double[] trainAccuracy;\n",
    "\n",
    "val epochCount = IntArray(numEpochs) { it + 1 }\n",
    "\n",
    "val trainIter = FashionMnist.builder()\n",
    "        .addTransform(Resize(96))\n",
    "        .addTransform(ToTensor())\n",
    "        .optUsage(Dataset.Usage.TRAIN)\n",
    "        .setSampling(batchSize, true)\n",
    "        .optLimit(getLong(\"DATASET_LIMIT\", Long.MAX_VALUE))\n",
    "        .build();\n",
    "\n",
    "val testIter = FashionMnist.builder()\n",
    "        .addTransform(Resize(96))\n",
    "        .addTransform(ToTensor())\n",
    "        .optUsage(Dataset.Usage.TEST)\n",
    "        .setSampling(batchSize, true)\n",
    "        .optLimit(getLong(\"DATASET_LIMIT\", Long.MAX_VALUE))\n",
    "        .build();\n",
    "\n",
    "trainIter.prepare();\n",
    "testIter.prepare();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from using a slightly larger learning rate,\n",
    "the model training process is similar to that of AlexNet in the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.72, SoftmaxCrossEntropyLoss: 0.77████                                  | Accuracy: 0.32, SoftmaxCrossEntropyLoss: 1.90��███████                             | Accuracy: 0.48, SoftmaxCrossEntropyLoss: 1.43��█████████████████                      | Accuracy: 0.60, SoftmaxCrossEntropyLoss: 1.11% |██████████████████████████████          | Accuracy: 0.68, SoftmaxCrossEntropyLoss: 0.87███      | Accuracy: 0.70, SoftmaxCrossEntropyLoss: 0.82███████████████████████████   | Accuracy: 0.71, SoftmaxCrossEntropyLoss: 0.79\n",
      "Validating:  100% |████████████████████████████████████████|                            |    |            |██████████████                   |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.86, SoftmaxCrossEntropyLoss: 0.38               | Accuracy: 0.85, SoftmaxCrossEntropyLoss: 0.40% |██████████████████████████████████████  | Accuracy: 0.86, SoftmaxCrossEntropyLoss: 0.38\n",
      "Validating:  100% |████████████████████████████████████████|                         |��█████████████████████     |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.88, SoftmaxCrossEntropyLoss: 0.32 Accuracy: 0.88, SoftmaxCrossEntropyLoss: 0.33\n",
      "Validating:  100% |████████████████████████████████████████|��██████████████             |�███████████████████████      |███████████ |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.90, SoftmaxCrossEntropyLoss: 0.28  | Accuracy: 0.88, SoftmaxCrossEntropyLoss: 0.32:     27% |███████████                             | Accuracy: 0.90, SoftmaxCrossEntropyLoss: 0.29cy: 0.89, SoftmaxCrossEntropyLoss: 0.29uracy: 0.89, SoftmaxCrossEntropyLoss: 0.29███  | Accuracy: 0.90, SoftmaxCrossEntropyLoss: 0.28\n",
      "Validating:  100% |████████████████████████████████████████|�███████████████           |███████████████████████████████████████ |\n",
      "Training:     47% |███████████████████                     | Accuracy: 0.90, SoftmaxCrossEntropyLoss: 0.26"
     ]
    }
   ],
   "source": [
    "val evaluatorMetrics = mutableMapOf<String, DoubleArray>()\n",
    "val avgTrainTimePerEpoch = Training.trainingChapter6(trainIter, testIter, numEpochs, trainer, evaluatorMetrics);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "null\n",
      "java.lang.NullPointerException\n",
      "\tat Line_920.<init>(Line_920.jupyter-kts:6)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.evalWithConfigAndOtherScriptsResults(BasicJvmScriptEvaluator.kt:105)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.invoke$suspendImpl(BasicJvmScriptEvaluator.kt:47)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.invoke(BasicJvmScriptEvaluator.kt)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmReplEvaluator.eval(BasicJvmReplEvaluator.kt:49)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.InternalEvaluatorImpl$eval$resultWithDiagnostics$1.invokeSuspend(InternalEvaluatorImpl.kt:103)\n",
      "\tat kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\n",
      "\tat kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:106)\n",
      "\tat kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:284)\n",
      "\tat kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:85)\n",
      "\tat kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:59)\n",
      "\tat kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source)\n",
      "\tat kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:38)\n",
      "\tat kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.InternalEvaluatorImpl.eval(InternalEvaluatorImpl.kt:103)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl$execute$1$result$1.invoke(CellExecutorImpl.kt:71)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl$execute$1$result$1.invoke(CellExecutorImpl.kt:69)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.withHost(repl.kt:635)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl.execute(CellExecutorImpl.kt:69)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.CellExecutor$DefaultImpls.execute$default(CellExecutor.kt:15)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl$evalEx$1.invoke(repl.kt:444)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl$evalEx$1.invoke(repl.kt:433)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.withEvalContext(repl.kt:397)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.evalEx(repl.kt:433)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.eval(repl.kt:485)\n",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.ProtocolKt$shellMessagesHandler$2$res$1.invoke(protocol.kt:321)\n",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.ProtocolKt$shellMessagesHandler$2$res$1.invoke(protocol.kt:320)\n",
      "\tat org.jetbrains.kotlinx.jupyter.JupyterExecutorImpl$runExecution$execThread$1.invoke(execution.kt:33)\n",
      "\tat org.jetbrains.kotlinx.jupyter.JupyterExecutorImpl$runExecution$execThread$1.invoke(execution.kt:31)\n",
      "\tat kotlin.concurrent.ThreadsKt$thread$thread$1.run(Thread.kt:30)\n"
     ]
    }
   ],
   "source": [
    "val trainLoss = evaluatorMetrics.get(\"train_epoch_SoftmaxCrossEntropyLoss\");\n",
    "val trainAccuracy = evaluatorMetrics.get(\"train_epoch_Accuracy\");\n",
    "val testAccuracy = evaluatorMetrics.get(\"validate_epoch_Accuracy\");\n",
    "\n",
    "print(\"loss %.3f,\".format(trainLoss!![numEpochs - 1]))\n",
    "print(\" train acc %.3f,\".format(trainAccuracy!![numEpochs - 1]))\n",
    "print(\" test acc %.3f\\n\".format(testAccuracy!![numEpochs - 1]))\n",
    "print(\"%.1f examples/sec\".format(trainIter.size() / (avgTrainTimePerEpoch / Math.pow(10.0, 9.0))))\n",
    "println();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Contour Gradient Descent.](https://d2l-java-resources.s3.amazonaws.com/img/chapter_convolution-modern-cnn-VGG.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// String[] lossLabel = new String[trainLoss.length + testAccuracy.length + trainAccuracy.length];\n",
    "\n",
    "val trainLossLabel =  Array<String>(trainLoss!!.size) { \"train loss\" }\n",
    "val trainAccLabel = Array<String>(trainLoss!!.size) { \"train acc\" }\n",
    "val testAccLabel = Array<String>(trainLoss!!.size) { \"test acc\" }\n",
    "val data = mapOf<String, Any>(\n",
    "      \"label\" to trainLossLabel + trainAccLabel + testAccLabel,\n",
    "      \"epoch\" to epochCount + epochCount + epochCount,\n",
    "      \"metrics\" to trainLoss!! + trainAccuracy!! + testAccuracy!!\n",
    ")\n",
    "\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine { x = \"epoch\" ; y = \"metrics\" ; color = \"label\"}\n",
    "plot + ggsize(700, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* VGG-11 constructs a network using reusable convolutional blocks. Different VGG models can be defined by the differences in the number of convolutional layers and output channels in each block.\n",
    "* The use of blocks leads to very compact representations of the network definition. It allows for efficient design of complex networks.\n",
    "* In their work Simonyan and Ziserman experimented with various architectures. In particular, they found that several layers of deep and narrow convolutions (i.e., $3 \\times 3$) were more effective than fewer layers of wider convolutions.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. When printing out the dimensions of the layers we only saw 8 results rather than 11. Where did the remaining 3 layer informations go?\n",
    "1. Compared with AlexNet, VGG is much slower in terms of computation, and it also needs more GPU memory. Try to analyze the reasons for this.\n",
    "1. Try to change the height and width of the images in Fashion-MNIST from 224 to 96. What influence does this have on the experiments?\n",
    "1. Refer to Table 1 in :cite:`Simonyan.Zisserman.2014` to construct other common models, such as VGG-16 or VGG-19."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
