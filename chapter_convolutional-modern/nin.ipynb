{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network in Network (NiN)\n",
    "\n",
    ":label:`sec_nin`\n",
    "\n",
    "\n",
    "LeNet, AlexNet, and VGG all share a common design pattern:\n",
    "extract features exploiting *spatial* structure\n",
    "via a sequence of convolutions and pooling layers\n",
    "and then post-process the representations via fully-connected layers.\n",
    "The improvements upon LeNet by AlexNet and VGG mainly lie\n",
    "in how these later networks widen and deepen these two modules.\n",
    "Alternatively, one could imagine using fully-connected layers\n",
    "earlier in the process.\n",
    "However, a careless use of dense layers might give up the\n",
    "spatial structure of the representation entirely,\n",
    "Network in Network (NiN) blocks offer an alternative.\n",
    "They were proposed in :cite:`Lin.Chen.Yan.2013` based on a very simple insight---to\n",
    "use an MLP on the channels for each pixel separately.\n",
    "\n",
    "## NiN Blocks\n",
    "\n",
    "Recall that the inputs and outputs of convolutional layers\n",
    "consist of four-dimensional arrays with axes\n",
    "corresponding to the batch, channel, height, and width.\n",
    "Also recall that the inputs and outputs of fully-connected layers\n",
    "are typically two-dimensional arrays corresponding to the batch, and features.\n",
    "The idea behind NiN is to apply a fully-connected layer\n",
    "at each pixel location (for each height and  width).\n",
    "If we tie the weights across each spatial location,\n",
    "we could think of this as a $1\\times 1$ convolutional layer\n",
    "(as described in :numref:`sec_channels`)\n",
    "or as a fully-connected layer acting independently on each pixel location.\n",
    "Another way to view this is to think of each element in the spatial dimension\n",
    "(height and width) as equivalent to an example\n",
    "and the channel as equivalent to a feature.\n",
    ":numref:`fig_nin` illustrates the main structural differences\n",
    "between NiN and AlexNet, VGG, and other networks.\n",
    "\n",
    "![The figure on the left shows the network structure of AlexNet and VGG, and the figure on the right shows the network structure of NiN. ](https://raw.githubusercontent.com/d2l-ai/d2l-en/master/img/nin.svg)\n",
    "\n",
    ":width:`600px`\n",
    "\n",
    "\n",
    ":label:`fig_nin`\n",
    "\n",
    "\n",
    "\n",
    "The NiN block consists of one convolutional layer\n",
    "followed by two $1\\times 1$ convolutional layers that act as\n",
    "per-pixel fully-connected layers with ReLU activations.\n",
    "The convolution width of the first layer is typically set by the user.\n",
    "The subsequent widths are fixed to $1 \\times 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"HWJp0M\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "       if(!window.letsPlotCallQueue) {\n",
       "           window.letsPlotCallQueue = [];\n",
       "       }; \n",
       "       window.letsPlotCall = function(f) {\n",
       "           window.letsPlotCallQueue.push(f);\n",
       "       };\n",
       "       (function() {\n",
       "           var script = document.createElement(\"script\");\n",
       "           script.type = \"text/javascript\";\n",
       "           script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.4.0/js-package/distr/lets-plot.min.js\";\n",
       "           script.onload = function() {\n",
       "               window.letsPlotCall = function(f) {f();};\n",
       "               window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "               window.letsPlotCallQueue = [];\n",
       "               \n",
       "               \n",
       "           };\n",
       "           script.onerror = function(event) {\n",
       "               window.letsPlotCall = function(f) {};\n",
       "               window.letsPlotCallQueue = [];\n",
       "               var div = document.createElement(\"div\");\n",
       "               div.style.color = 'darkred';\n",
       "               div.textContent = 'Error loading Lets-Plot JS';\n",
       "               document.getElementById(\"HWJp0M\").appendChild(div);\n",
       "           };\n",
       "           var e = document.getElementById(\"HWJp0M\");\n",
       "           e.appendChild(script);\n",
       "       })();\n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%use @file[../djl-pytorch.json]\n",
    "%use lets-plot\n",
    "@file:DependsOn(\"../D2J-1.0-SNAPSHOT.jar\")\n",
    "//import jp.live.ugai.d2j.attention.Chap10Utils\n",
    "import jp.live.ugai.d2j.util.Training\n",
    "import ai.djl.metric.Metrics\n",
    "\n",
    "fun getLong(nm: String, n: Long): Long {\n",
    "    val name = System.getProperty(nm)\n",
    "    return if (null == name) n.toLong() else name.toLong()\n",
    "}\n",
    "\n",
    "// %load ../utils/djl-imports\n",
    "// %load ../utils/plot-utils\n",
    "// %load ../utils/DataPoints.java\n",
    "// %load ../utils/Training.java\n",
    "// %load ../utils/Accumulator.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ai.djl.basicdataset.cv.classification.*;\n",
    "import ai.djl.metric.*;\n",
    "// import org.apache.commons.lang3.ArrayUtils;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "// setting the seed for demonstration purpose. You can remove it when you run the notebook\n",
    "Engine.getInstance().setRandomSeed(5555);\n",
    "\n",
    "fun  niNBlock(numChannels: Int, kernelShape: Shape, \n",
    "                     strideShape:Shape , paddingShape: Shape ): SequentialBlock{\n",
    "    \n",
    "    val tempBlock = SequentialBlock();\n",
    "    \n",
    "    tempBlock.add(Conv2d.builder()\n",
    "              .setKernelShape(kernelShape)\n",
    "              .optStride(strideShape)\n",
    "              .optPadding(paddingShape)\n",
    "              .setFilters(numChannels)\n",
    "              .build())\n",
    "        .add(Activation::relu)\n",
    "        .add(Conv2d.builder()\n",
    "              .setKernelShape(Shape(1, 1))\n",
    "              .setFilters(numChannels)\n",
    "              .build())\n",
    "        .add(Activation::relu)\n",
    "        .add(Conv2d.builder()\n",
    "              .setKernelShape(Shape(1, 1))\n",
    "              .setFilters(numChannels)\n",
    "              .build())\n",
    "        .add(Activation::relu); \n",
    "    \n",
    "    return tempBlock;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NiN Model\n",
    "\n",
    "The original NiN network was proposed shortly after AlexNet\n",
    "and clearly draws some inspiration.\n",
    "NiN uses convolutional layers with window shapes\n",
    "of $11\\times 11$, $5\\times 5$, and $3\\times 3$,\n",
    "and the corresponding numbers of output channels are the same as in AlexNet. Each NiN block is followed by a maximum pooling layer\n",
    "with a stride of 2 and a window shape of $3\\times 3$.\n",
    "\n",
    "Once significant difference between NiN and AlexNet\n",
    "is that NiN avoids dense connections altogether.\n",
    "Instead, NiN uses an NiN block with a number of output channels equal to the number of label classes, followed by a *global* average pooling layer,\n",
    "yielding a vector of [logits](https://en.wikipedia.org/wiki/Logit).\n",
    "One advantage of NiN's design is that it significantly\n",
    "reduces the number of required model parameters.\n",
    "However, in practice, this design sometimes requires\n",
    "increased model training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialBlock {\n",
       "\tSequentialBlock {\n",
       "\t\tConv2d\n",
       "\t\tLambdaBlock\n",
       "\t\tConv2d\n",
       "\t\tLambdaBlock\n",
       "\t\tConv2d\n",
       "\t\tLambdaBlock\n",
       "\t}\n",
       "\tmaxPool2d\n",
       "\tSequentialBlock {\n",
       "\t\tConv2d\n",
       "\t\tLambdaBlock\n",
       "\t\tConv2d\n",
       "\t\tLambdaBlock\n",
       "\t\tConv2d\n",
       "\t\tLambdaBlock\n",
       "\t}\n",
       "\tmaxPool2d\n",
       "\tSequentialBlock {\n",
       "\t\tConv2d\n",
       "\t\tLambdaBlock\n",
       "\t\tConv2d\n",
       "\t\tLambdaBlock\n",
       "\t\tConv2d\n",
       "\t\tLambdaBlock\n",
       "\t}\n",
       "\tmaxPool2d\n",
       "\tDropout\n",
       "\tSequentialBlock {\n",
       "\t\tConv2d\n",
       "\t\tLambdaBlock\n",
       "\t\tConv2d\n",
       "\t\tLambdaBlock\n",
       "\t\tConv2d\n",
       "\t\tLambdaBlock\n",
       "\t}\n",
       "\tglobalAvgPool2d\n",
       "\tbatchFlatten\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val block = SequentialBlock();\n",
    "\n",
    "block.add(niNBlock(96, Shape(11, 11), Shape(4, 4), Shape(0, 0)))\n",
    "     .add(Pool.maxPool2dBlock(Shape(3, 3), Shape(2, 2)))\n",
    "     .add(niNBlock(256, Shape(5, 5), Shape(1, 1), Shape(2, 2)))\n",
    "     .add(Pool.maxPool2dBlock(Shape(3, 3), Shape(2, 2)))\n",
    "     .add(niNBlock(384, Shape(3, 3), Shape(1, 1), Shape(1, 1)))\n",
    "     .add(Pool.maxPool2dBlock(Shape(3, 3), Shape(2, 2)))\n",
    "     .add(Dropout.builder().optRate(0.5f).build())\n",
    "     // There are 10 label classes\n",
    "     .add(niNBlock(10, Shape(3, 3), Shape(1, 1), Shape(1, 1)))\n",
    "     // The global average pooling layer automatically sets the window shape\n",
    "     // to the height and width of the input\n",
    "     .add(Pool.globalAvgPool2dBlock())\n",
    "     // Transform the four-dimensional output into two-dimensional output\n",
    "     // with a shape of (batch size, 10)\n",
    "     .add(Blocks.batchFlattenBlock());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a data example to see the output shape of each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01SequentialBlock layer output : (1, 96, 54, 54)\n",
      "02LambdaBlock layer output : (1, 96, 26, 26)\n",
      "03SequentialBlock layer output : (1, 256, 26, 26)\n",
      "04LambdaBlock layer output : (1, 256, 12, 12)\n",
      "05SequentialBlock layer output : (1, 384, 12, 12)\n",
      "06LambdaBlock layer output : (1, 384, 5, 5)\n",
      "07Dropout layer output : (1, 384, 5, 5)\n",
      "08SequentialBlock layer output : (1, 10, 5, 5)\n",
      "09LambdaBlock layer output : (1, 10)\n",
      "10LambdaBlock layer output : (1, 10)\n"
     ]
    }
   ],
   "source": [
    "val lr = 0.1f;\n",
    "val model = Model.newInstance(\"cnn\");\n",
    "model.setBlock(block);\n",
    "\n",
    "val loss = Loss.softmaxCrossEntropyLoss();\n",
    "\n",
    "val lrt = Tracker.fixed(lr);\n",
    "val sgd = Optimizer.sgd().setLearningRateTracker(lrt).build();\n",
    "\n",
    "val config = DefaultTrainingConfig(loss).optOptimizer(sgd) // Optimizer (loss function)\n",
    "                .optDevices(Engine.getInstance().getDevices(1)) // single GPU\n",
    "                .addEvaluator(Accuracy()) // Model Accuracy\n",
    "                .addTrainingListeners(*TrainingListener.Defaults.logging()); // Logging\n",
    "\n",
    "val trainer = model.newTrainer(config);\n",
    "\n",
    "val manager = NDManager.newBaseManager();\n",
    "val X = manager.randomUniform(0f, 1.0f, Shape(1, 1, 224, 224));\n",
    "trainer.initialize(X.getShape());\n",
    "\n",
    "var currentShape = X.getShape();\n",
    "\n",
    "for (i in 0 until block.getChildren().size()) {\n",
    "\n",
    "    val newShape = block.getChildren().get(i).getValue().getOutputShapes(arrayOf<Shape>(currentShape))\n",
    "    currentShape = newShape[0];\n",
    "    println(block.getChildren().get(i).getKey() + \" layer output : \" + currentShape);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition and Training\n",
    "\n",
    "As before we use Fashion-MNIST to train the model.\n",
    "NiN's training is similar to that for AlexNet and VGG,\n",
    "but it often uses a larger learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val batchSize = 128;\n",
    "val numEpochs = Integer.getInteger(\"MAX_EPOCH\", 10);\n",
    "\n",
    "//double[] trainLoss;\n",
    "//double[] testAccuracy;\n",
    "//double[] epochCount;\n",
    "//double[] trainAccuracy;\n",
    "\n",
    "val epochCount = IntArray(numEpochs) { it + 1 }\n",
    "\n",
    "val trainIter = FashionMnist.builder()\n",
    "        .addTransform(Resize(224))\n",
    "        .addTransform(ToTensor())\n",
    "        .optUsage(Dataset.Usage.TRAIN)\n",
    "        .setSampling(batchSize, true)\n",
    "        .optLimit(getLong(\"DATASET_LIMIT\", Long.MAX_VALUE))\n",
    "        .build();\n",
    "\n",
    "val testIter = FashionMnist.builder()\n",
    "        .addTransform(Resize(224))\n",
    "        .addTransform(ToTensor())\n",
    "        .optUsage(Dataset.Usage.TEST)\n",
    "        .setSampling(batchSize, true)\n",
    "        .optLimit(getLong(\"DATASET_LIMIT\", Long.MAX_VALUE))\n",
    "        .build();\n",
    "\n",
    "trainIter.prepare();\n",
    "testIter.prepare();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.13, SoftmaxCrossEntropyLoss: 2.26\n",
      "Validating:  100% |████████████████████████████████████████|████              ||██████████████████████████████████████|\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.36, SoftmaxCrossEntropyLoss: 1.72██                        | Accuracy: 0.24, SoftmaxCrossEntropyLoss: 1.99   | Accuracy: 0.26, SoftmaxCrossEntropyLoss: 1.93\n",
      "Validating:  100% |████████████████████████████████████████|                         |��███ |\n",
      "Training:     72% |█████████████████████████████           | Accuracy: 0.51, SoftmaxCrossEntropyLoss: 1.41                                     | Accuracy: 0.36, SoftmaxCrossEntropyLoss: 1.72"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Loss became NaN, try reduce learning rate,add clipGradient option to your optimizer, check input data and loss calculation.\n",
      "ai.djl.TrainingDivergedException: The Loss became NaN, try reduce learning rate,add clipGradient option to your optimizer, check input data and loss calculation.\n",
      "\tat ai.djl.training.listener.DivergenceCheckTrainingListener.onTrainingBatch(DivergenceCheckTrainingListener.java:27)\n",
      "\tat ai.djl.training.EasyTrain.lambda$trainBatch$3(EasyTrain.java:115)\n",
      "\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1541)\n",
      "\tat ai.djl.training.Trainer.notifyListeners(Trainer.java:271)\n",
      "\tat ai.djl.training.EasyTrain.trainBatch(EasyTrain.java:115)\n",
      "\tat ai.djl.training.EasyTrain.fit(EasyTrain.java:58)\n",
      "\tat Line_9$Training.trainingChapter6(Line_9.jupyter-kts:128)\n",
      "\tat Line_588.<init>(Line_588.jupyter-kts:2)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.evalWithConfigAndOtherScriptsResults(BasicJvmScriptEvaluator.kt:105)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.invoke$suspendImpl(BasicJvmScriptEvaluator.kt:47)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.invoke(BasicJvmScriptEvaluator.kt)\n",
      "\tat kotlin.script.experimental.jvm.BasicJvmReplEvaluator.eval(BasicJvmReplEvaluator.kt:49)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.InternalEvaluatorImpl$eval$resultWithDiagnostics$1.invokeSuspend(InternalEvaluatorImpl.kt:103)\n",
      "\tat kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\n",
      "\tat kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:106)\n",
      "\tat kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:284)\n",
      "\tat kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:85)\n",
      "\tat kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:59)\n",
      "\tat kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source)\n",
      "\tat kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:38)\n",
      "\tat kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.InternalEvaluatorImpl.eval(InternalEvaluatorImpl.kt:103)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl$execute$1$result$1.invoke(CellExecutorImpl.kt:71)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl$execute$1$result$1.invoke(CellExecutorImpl.kt:69)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.withHost(repl.kt:635)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl.execute(CellExecutorImpl.kt:69)\n",
      "\tat org.jetbrains.kotlinx.jupyter.repl.CellExecutor$DefaultImpls.execute$default(CellExecutor.kt:15)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl$evalEx$1.invoke(repl.kt:444)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl$evalEx$1.invoke(repl.kt:433)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.withEvalContext(repl.kt:397)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.evalEx(repl.kt:433)\n",
      "\tat org.jetbrains.kotlinx.jupyter.ReplForJupyterImpl.eval(repl.kt:485)\n",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.ProtocolKt$shellMessagesHandler$2$res$1.invoke(protocol.kt:321)\n",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.ProtocolKt$shellMessagesHandler$2$res$1.invoke(protocol.kt:320)\n",
      "\tat org.jetbrains.kotlinx.jupyter.JupyterExecutorImpl$runExecution$execThread$1.invoke(execution.kt:33)\n",
      "\tat org.jetbrains.kotlinx.jupyter.JupyterExecutorImpl$runExecution$execThread$1.invoke(execution.kt:31)\n",
      "\tat kotlin.concurrent.ThreadsKt$thread$thread$1.run(Thread.kt:30)\n"
     ]
    }
   ],
   "source": [
    "val evaluatorMetrics = mutableMapOf<String, DoubleArray>();\n",
    "val avgTrainTimePerEpoch = Training.trainingChapter6(trainIter, testIter, numEpochs, trainer, evaluatorMetrics);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Line_590.jupyter-kts (1:1 - 10) Unresolved reference: trainLoss\n",
      "Line_590.jupyter-kts (2:1 - 14) Unresolved reference: trainAccuracy\n",
      "Line_590.jupyter-kts (3:1 - 13) Unresolved reference: testAccuracy\n",
      "Line_590.jupyter-kts (5:27 - 36) Unresolved reference: trainLoss\n",
      "Line_590.jupyter-kts (6:33 - 46) Unresolved reference: trainAccuracy\n",
      "Line_590.jupyter-kts (7:33 - 45) Unresolved reference: testAccuracy\n",
      "Line_590.jupyter-kts (8:86 - 88) The integer literal does not conform to the expected type Double\n",
      "Line_590.jupyter-kts (8:90 - 91) The integer literal does not conform to the expected type Double"
     ]
    }
   ],
   "source": [
    "trainLoss = evaluatorMetrics.get(\"train_epoch_SoftmaxCrossEntropyLoss\");\n",
    "trainAccuracy = evaluatorMetrics.get(\"train_epoch_Accuracy\");\n",
    "testAccuracy = evaluatorMetrics.get(\"validate_epoch_Accuracy\");\n",
    "\n",
    "print(\"loss %.3f,\".format(trainLoss[numEpochs - 1]))\n",
    "print(\" train acc %.3f,\".format(trainAccuracy[numEpochs - 1]))\n",
    "print(\" test acc %.3f\\n\".format(testAccuracy[numEpochs - 1]))\n",
    "print(\"%.1f examples/sec\".format(trainIter.size() / (avgTrainTimePerEpoch / Math.pow(10, 9))))\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Contour Gradient Descent.](https://d2l-java-resources.s3.amazonaws.com/img/nin-plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String[] lossLabel = new String[trainLoss.length + testAccuracy.length + trainAccuracy.length];\n",
    "\n",
    "Arrays.fill(lossLabel, 0, trainLoss.length, \"train loss\");\n",
    "Arrays.fill(lossLabel, trainAccuracy.length, trainLoss.length + trainAccuracy.length, \"train acc\");\n",
    "Arrays.fill(lossLabel, trainLoss.length + trainAccuracy.length,\n",
    "                trainLoss.length + testAccuracy.length + trainAccuracy.length, \"test acc\");\n",
    "\n",
    "Table data = Table.create(\"Data\").addColumns(\n",
    "            DoubleColumn.create(\"epoch\", ArrayUtils.addAll(epochCount, ArrayUtils.addAll(epochCount, epochCount))),\n",
    "            DoubleColumn.create(\"metrics\", ArrayUtils.addAll(trainLoss, ArrayUtils.addAll(trainAccuracy, testAccuracy))),\n",
    "            StringColumn.create(\"lossLabel\", lossLabel)\n",
    ");\n",
    "\n",
    "render(LinePlot.create(\"\", data, \"epoch\", \"metrics\", \"lossLabel\"),\"text/html\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Line_592.jupyter-kts (3:37 - 46) Unresolved reference: trainLoss\n",
      "Line_592.jupyter-kts (4:35 - 44) Unresolved reference: trainLoss\n",
      "Line_592.jupyter-kts (5:34 - 43) Unresolved reference: trainLoss\n",
      "Line_592.jupyter-kts (9:20 - 29) Unresolved reference: trainLoss\n",
      "Line_592.jupyter-kts (9:34 - 47) Unresolved reference: trainAccuracy\n",
      "Line_592.jupyter-kts (9:52 - 64) Unresolved reference: testAccuracy"
     ]
    }
   ],
   "source": [
    "// String[] lossLabel = new String[trainLoss.length + testAccuracy.length + trainAccuracy.length];\n",
    "\n",
    "val trainLossLabel =  Array<String>(trainLoss!!.size) { \"train loss\" }\n",
    "val trainAccLabel = Array<String>(trainLoss!!.size) { \"train acc\" }\n",
    "val testAccLabel = Array<String>(trainLoss!!.size) { \"test acc\" }\n",
    "val data = mapOf<String, Any>(\n",
    "      \"label\" to trainLossLabel + trainAccLabel + testAccLabel,\n",
    "      \"epoch\" to epochCount + epochCount + epochCount,\n",
    "      \"metrics\" to trainLoss!! + trainAccuracy!! + testAccuracy!!\n",
    ")\n",
    "\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine { x = \"epoch\" ; y = \"metrics\" ; color = \"label\"}\n",
    "plot + ggsize(700, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* NiN uses blocks consisting of a convolutional layer and multiple $1\\times 1$ convolutional layer. This can be used within the convolutional stack to allow for more per-pixel nonlinearity.\n",
    "* NiN removes the fully connected layers and replaces them with global average pooling (i.e., summing over all locations) after reducing the number of channels to the desired number of outputs (e.g., 10 for Fashion-MNIST).\n",
    "* Removing the dense layers reduces overfitting. NiN has dramatically fewer parameters.\n",
    "* The NiN design influenced many subsequent convolutional neural networks designs.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Tune the hyper-parameters to improve the classification accuracy.\n",
    "1. Why are there two $1\\times 1$ convolutional layers in the NiN block? Remove one of them, and then observe and analyze the experimental phenomena.\n",
    "1. Calculate the resource usage for NiN\n",
    "    * What is the number of parameters?\n",
    "    * What is the amount of computation?\n",
    "    * What is the amount of memory needed during training?\n",
    "    * What is the amount of memory needed during inference?\n",
    "1. What are possible problems with reducing the $384 \\times 5 \\times 5$ representation to a $10 \\times 5 \\times 5$ representation in one step?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
