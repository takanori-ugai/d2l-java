{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks with Parallel Concatenations (GoogLeNet)\n",
    "\n",
    ":label:`sec_googlenet`\n",
    "\n",
    "\n",
    "In 2014, :cite:`Szegedy.Liu.Jia.ea.2015`\n",
    "won the ImageNet Challenge, proposing a structure\n",
    "that combined the strengths of the NiN and repeated blocks paradigms.\n",
    "One focus of the paper was to address the question\n",
    "of which sized convolutional kernels are best.\n",
    "After all, previous popular networks employed choices\n",
    "as small as $1 \\times 1$ and as large as $11 \\times 11$.\n",
    "One insight in this paper was that sometimes\n",
    "it can be advantageous to employ a combination of variously-sized kernels.\n",
    "In this section, we will introduce GoogLeNet,\n",
    "presenting a slightly simplified version of the original model---we\n",
    "omit a few ad hoc features that were added to stabilize training\n",
    "but are unnecessary now with better training algorithms available.\n",
    "\n",
    "## Inception Blocks\n",
    "\n",
    "The basic convolutional block in GoogLeNet is called an Inception block,\n",
    "likely named due to a quote from the movie Inception (\"We Need To Go Deeper\"),\n",
    "which launched a viral meme.\n",
    "\n",
    "![Structure of the Inception block. ](https://raw.githubusercontent.com/d2l-ai/d2l-en/master/img/inception.svg)\n",
    "\n",
    "As depicted in the figure above,\n",
    "the inception block consists of four parallel paths.\n",
    "The first three paths use convolutional layers\n",
    "with window sizes of $1\\times 1$, $3\\times 3$, and $5\\times 5$\n",
    "to extract information from different spatial sizes.\n",
    "The middle two paths perform a $1\\times 1$ convolution on the input\n",
    "to reduce the number of input channels, reducing the model's complexity.\n",
    "The fourth path uses a $3\\times 3$ maximum pooling layer,\n",
    "followed by a $1\\times 1$ convolutional layer\n",
    "to change the number of channels.\n",
    "The four paths all use appropriate padding to give the input and output the same height and width.\n",
    "Finally, the outputs along each path are concatenated\n",
    "along the channel dimension and comprise the block's output.\n",
    "The commonly-tuned parameters of the Inception block\n",
    "are the number of output channels per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"gRFYuT\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "       if(!window.letsPlotCallQueue) {\n",
       "           window.letsPlotCallQueue = [];\n",
       "       }; \n",
       "       window.letsPlotCall = function(f) {\n",
       "           window.letsPlotCallQueue.push(f);\n",
       "       };\n",
       "       (function() {\n",
       "           var script = document.createElement(\"script\");\n",
       "           script.type = \"text/javascript\";\n",
       "           script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.4.0/js-package/distr/lets-plot.min.js\";\n",
       "           script.onload = function() {\n",
       "               window.letsPlotCall = function(f) {f();};\n",
       "               window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "               window.letsPlotCallQueue = [];\n",
       "               \n",
       "               \n",
       "           };\n",
       "           script.onerror = function(event) {\n",
       "               window.letsPlotCall = function(f) {};\n",
       "               window.letsPlotCallQueue = [];\n",
       "               var div = document.createElement(\"div\");\n",
       "               div.style.color = 'darkred';\n",
       "               div.textContent = 'Error loading Lets-Plot JS';\n",
       "               document.getElementById(\"gRFYuT\").appendChild(div);\n",
       "           };\n",
       "           var e = document.getElementById(\"gRFYuT\");\n",
       "           e.appendChild(script);\n",
       "       })();\n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%use @file[../djl-pytorch.json]\n",
    "%use lets-plot\n",
    "@file:DependsOn(\"org.apache.commons:commons-lang3:3.12.0\")\n",
    "import ai.djl.metric.Metrics\n",
    "\n",
    "fun getLong(nm: String, n: Long): Long {\n",
    "    val name = System.getProperty(nm)\n",
    "    return if (null == name) n.toLong() else name.toLong()\n",
    "}\n",
    "\n",
    "class Accumulator(n: Int) {\n",
    "    val data = FloatArray(n) { 0f }\n",
    "\n",
    "\n",
    "    /* Adds a set of numbers to the array */\n",
    "    fun add(args: FloatArray) {\n",
    "        for (i in 0..args.size - 1) {\n",
    "            data[i] += args[i]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /* Resets the array */\n",
    "    fun reset() {\n",
    "        data.fill(0f)\n",
    "    }\n",
    "\n",
    "    /* Returns the data point at the given index */\n",
    "    fun get(index: Int): Float {\n",
    "        return data[index]\n",
    "    }\n",
    "}\n",
    "\n",
    "class DataPoints(X:NDArray , y:NDArray ) {\n",
    "    private val X = X\n",
    "    private val y = y\n",
    "\n",
    "    fun  getX() : NDArray{\n",
    "        return X\n",
    "    }\n",
    "    \n",
    "    fun getY() :NDArray {\n",
    "        return y\n",
    "    }\n",
    "}\n",
    "\n",
    "fun syntheticData(manager:NDManager , w: NDArray , b : Float, numExamples: Int) : DataPoints {\n",
    "    val X = manager.randomNormal(Shape(numExamples.toLong(), w.size()))\n",
    "    var y = X.matMul(w).add(b)\n",
    "    // Add noise\n",
    "    y = y.add(manager.randomNormal(0f, 0.01f, y.getShape(), DataType.FLOAT32))\n",
    "    return DataPoints(X, y);\n",
    "}\n",
    "\n",
    "object Training {\n",
    "\n",
    "    fun linreg(X: NDArray, w: NDArray, b: NDArray): NDArray {\n",
    "        return X.dot(w).add(b);\n",
    "    }\n",
    "\n",
    "    fun squaredLoss(yHat: NDArray, y: NDArray): NDArray {\n",
    "        return (yHat.sub(y.reshape(yHat.getShape())))\n",
    "            .mul((yHat.sub(y.reshape(yHat.getShape()))))\n",
    "            .div(2);\n",
    "    }\n",
    "\n",
    "    fun sgd(params: NDList, lr: Float, batchSize: Int) {\n",
    "    val lrt = Tracker.fixed(lr);\n",
    "    val opt = Optimizer.sgd().setLearningRateTracker(lrt).build();\n",
    "        for (param in params) {\n",
    "            // Update param in place.\n",
    "            // param = param - param.gradient * lr / batchSize\n",
    "            // val ind = params.indexOf(param)\n",
    "            // params.rep\n",
    "            // params.set(ind, param.sub(param.getGradient().mul(lr).div(batchSize)))\n",
    "            opt.update(param.toString(), param, param.getGradient().div(batchSize))\n",
    "//            param.subi(param.getGradient().mul(lr).div(batchSize));\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * Allows to do gradient calculations on a subManager. This is very useful when you are training\n",
    "     * on a lot of epochs. This subManager could later be closed and all NDArrays generated from the\n",
    "     * calculations in this function will be cleared from memory when subManager is closed. This is\n",
    "     * always a great practice but the impact is most notable when there is lot of data on various\n",
    "     * epochs.\n",
    "     */\n",
    "    fun sgd(params: NDList, lr: Float, batchSize: Int, subManager: NDManager) {\n",
    "        for (param in params) {\n",
    "            // Update param in place.\n",
    "            // param = param - param.gradient * lr / batchSize\n",
    "            val gradient = param.getGradient()\n",
    "            gradient.attach(subManager);\n",
    "            param.subi(gradient.mul(lr).div(batchSize))\n",
    "        }\n",
    "    }\n",
    "\n",
    "    fun accuracy(yHat: NDArray, y: NDArray): Float {\n",
    "        // Check size of 1st dimension greater than 1\n",
    "        // to see if we have multiple samples\n",
    "        if (yHat.getShape().size(1) > 1) {\n",
    "            // Argmax gets index of maximum args for given axis 1\n",
    "            // Convert yHat to same dataType as y (int32)\n",
    "            // Sum up number of true entries\n",
    "            return yHat.argMax(1)\n",
    "                .toType(DataType.INT32, false)\n",
    "                .eq(y.toType(DataType.INT32, false))\n",
    "                .sum()\n",
    "                .toType(DataType.FLOAT32, false)\n",
    "                .getFloat();\n",
    "        }\n",
    "        return yHat.toType(DataType.INT32, false)\n",
    "            .eq(y.toType(DataType.INT32, false))\n",
    "            .sum()\n",
    "            .toType(DataType.FLOAT32, false)\n",
    "            .getFloat();\n",
    "    }\n",
    "\n",
    "    fun trainingChapter6(\n",
    "        trainIter: ArrayDataset,\n",
    "        testIter: ArrayDataset,\n",
    "        numEpochs: Int,\n",
    "        trainer: Trainer,\n",
    "        evaluatorMetrics: MutableMap<String, DoubleArray>\n",
    "    ): Double {\n",
    "\n",
    "        trainer.setMetrics(Metrics())\n",
    "\n",
    "        EasyTrain.fit(trainer, numEpochs, trainIter, testIter)\n",
    "\n",
    "        val metrics = trainer.getMetrics()\n",
    "\n",
    "        trainer.getEvaluators()\n",
    "            .forEach { evaluator ->\n",
    "                {\n",
    "                    evaluatorMetrics.put(\n",
    "                        \"train_epoch_\" + evaluator.getName(),\n",
    "                        metrics.getMetric(\"train_epoch_\" + evaluator.getName()).stream()\n",
    "                            .mapToDouble { x -> x.getValue() }\n",
    "                            .toArray())\n",
    "                    evaluatorMetrics.put(\n",
    "                        \"validate_epoch_\" + evaluator.getName(),\n",
    "                        metrics\n",
    "                            .getMetric(\"validate_epoch_\" + evaluator.getName())\n",
    "                            .stream()\n",
    "                            .mapToDouble { x -> x.getValue() }\n",
    "                            .toArray())\n",
    "                }\n",
    "            }\n",
    "\n",
    "        return metrics.mean(\"epoch\")\n",
    "    }\n",
    "\n",
    "    /* Softmax-regression-scratch */\n",
    "    fun evaluateAccuracy(net: UnaryOperator<NDArray>, dataIterator: Iterable<Batch>): Float {\n",
    "        val metric = Accumulator(2) // numCorrectedExamples, numExamples\n",
    "        for (batch in dataIterator) {\n",
    "            val X = batch.getData().head()\n",
    "            val y = batch.getLabels().head()\n",
    "            metric.add(floatArrayOf(accuracy(net.apply(X), y), y.size().toFloat()))\n",
    "            batch.close()\n",
    "        }\n",
    "        return metric.get(0) / metric.get(1)\n",
    "    }\n",
    "    /* End Softmax-regression-scratch */\n",
    "\n",
    "    /* MLP */\n",
    "    /* Evaluate the loss of a model on the given dataset */\n",
    "    fun evaluateLoss(\n",
    "        net: UnaryOperator<NDArray>,\n",
    "        dataIterator: Iterable<Batch>,\n",
    "        loss: BinaryOperator<NDArray>\n",
    "    ): Float {\n",
    "        val metric = Accumulator(2) // sumLoss, numExamples\n",
    "\n",
    "        for (batch in dataIterator) {\n",
    "            val X = batch . getData ().head();\n",
    "            val y = batch . getLabels ().head();\n",
    "            metric.add(\n",
    "                floatArrayOf(loss.apply(net.apply(X), y).sum().getFloat(), y.size().toFloat()) )\n",
    "            batch.close()\n",
    "        }\n",
    "        return metric.get(0) / metric.get(1)\n",
    "    }\n",
    "    /* End MLP */\n",
    "}\n",
    "\n",
    "// %load ../utils/djl-imports\n",
    "// %load ../utils/plot-utils\n",
    "// %load ../utils/DataPoints.java\n",
    "// %load ../utils/Training.java\n",
    "// %load ../utils/Accumulator.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ai.djl.basicdataset.cv.classification.*;\n",
    "import org.apache.commons.lang3.ArrayUtils;\n",
    "import java.util.stream.*;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "// c1 - c4 are the number of output channels for each layer in the path\n",
    "fun inceptionBlock(c1: Int, c2: IntArray, c3: IntArray, c4: Int) : ParallelBlock {\n",
    "\n",
    "    // Path 1 is a single 1 x 1 convolutional layer\n",
    "    val p1 = SequentialBlock().add(\n",
    "            Conv2d.builder()\n",
    "                    .setFilters(c1)\n",
    "                    .setKernelShape(Shape(1, 1))\n",
    "                    .build())\n",
    "            .add(Activation::relu);\n",
    "\n",
    "    // Path 2 is a 1 x 1 convolutional layer followed by a 3 x 3\n",
    "    // convolutional layer\n",
    "    val p2 = SequentialBlock().add(\n",
    "            Conv2d.builder()\n",
    "                    .setFilters(c2[0])\n",
    "                    .setKernelShape(Shape(1, 1))\n",
    "                    .build())\n",
    "            .add(Activation::relu)\n",
    "            .add(\n",
    "                    Conv2d.builder()\n",
    "                            .setFilters(c2[1])\n",
    "                            .setKernelShape(Shape(3, 3))\n",
    "                            .optPadding(Shape(1, 1))\n",
    "                            .build())\n",
    "            .add(Activation::relu);\n",
    "\n",
    "    // Path 3 is a 1 x 1 convolutional layer followed by a 5 x 5\n",
    "    // convolutional layer\n",
    "    val p3 = SequentialBlock().add(\n",
    "            Conv2d.builder()\n",
    "                    .setFilters(c3[0])\n",
    "                    .setKernelShape(Shape(1, 1))\n",
    "                    .build())\n",
    "            .add(Activation::relu)\n",
    "            .add(\n",
    "                    Conv2d.builder()\n",
    "                            .setFilters(c3[1])\n",
    "                            .setKernelShape(Shape(5, 5))\n",
    "                            .optPadding(Shape(2, 2))\n",
    "                            .build())\n",
    "            .add(Activation::relu);\n",
    "\n",
    "    // Path 4 is a 3 x 3 maximum pooling layer followed by a 1 x 1\n",
    "    // convolutional layer\n",
    "    val p4 : Block = SequentialBlock()\n",
    "            .add(Pool.maxPool2dBlock(Shape(3, 3), Shape(1, 1), Shape(1, 1)))\n",
    "            .add(Conv2d.builder()\n",
    "                    .setFilters(c4)\n",
    "                    .setKernelShape(Shape(1, 1))\n",
    "                    .build())\n",
    "            .add(Activation::relu);\n",
    "\n",
    "    // Concatenate the outputs on the channel dimension\n",
    "    return ParallelBlock(\n",
    "            { list: List<NDList> ->\n",
    "                val concatenatedList = list\n",
    "                    .stream()\n",
    "                    .map { obj: NDList -> obj.head() }\n",
    "                    .collect(Collectors.toList())\n",
    "                NDList(NDArrays.concat(NDList(concatenatedList), 1))\n",
    "            }, Arrays.asList(p1, p2, p3, p4)\n",
    "        )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain some intuition for why this network works so well,\n",
    "consider the combination of the filters.\n",
    "They explore the image in varying ranges.\n",
    "This means that details at different extents\n",
    "can be recognized efficiently by different filters.\n",
    "At the same time, we can allocate different amounts of parameters\n",
    "for different ranges (e.g., more for short range\n",
    "but not ignore the long range entirely).\n",
    "\n",
    "## GoogLeNet Model\n",
    "\n",
    "As shown in :numref:`fig_inception_full`, GoogLeNet uses a stack of a total of 9 inception blocks\n",
    "and global average pooling to generate its estimates.\n",
    "Maximum pooling between inception blocks reduced the dimensionality.\n",
    "The first part is identical to AlexNet and LeNet,\n",
    "the stack of blocks is inherited from VGG\n",
    "and the global average pooling avoids\n",
    "a stack of fully-connected layers at the end.\n",
    "The architecture is depicted below.\n",
    "\n",
    "![Full GoogLeNet Model](https://raw.githubusercontent.com/d2l-ai/d2l-en/master/img/inception-full.svg)\n",
    "\n",
    ":label:`fig_inception_full`\n",
    "\n",
    "\n",
    "We can now implement GoogLeNet piece by piece.\n",
    "The first component uses a 64-channel $7\\times 7$ convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialBlock {\n",
       "\tConv2d\n",
       "\tLambdaBlock\n",
       "\tmaxPool2d\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val block1 = SequentialBlock();\n",
    "block1\n",
    "    .add(Conv2d.builder()\n",
    "                .setKernelShape(Shape(7, 7))\n",
    "                .optPadding(Shape(3, 3))\n",
    "                .optStride(Shape(2, 2))\n",
    "                .setFilters(64)\n",
    "                .build())\n",
    "    .add(Activation::relu)\n",
    "    .add(Pool.maxPool2dBlock(Shape(3, 3), Shape(2, 2), Shape(1, 1)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second component uses two convolutional layers:\n",
    "first, a 64-channel $1\\times 1$ convolutional layer,\n",
    "then a $3\\times 3$ convolutional layer that triples the number of channels. This corresponds to the second path in the Inception block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialBlock {\n",
       "\tConv2d\n",
       "\tLambdaBlock\n",
       "\tConv2d\n",
       "\tLambdaBlock\n",
       "\tmaxPool2d\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val block2 = SequentialBlock();\n",
    "block2\n",
    "    .add(Conv2d.builder()\n",
    "            .setFilters(64)\n",
    "            .setKernelShape(Shape(1, 1))\n",
    "            .build())\n",
    "    .add(Activation::relu)\n",
    "    .add(Conv2d.builder()\n",
    "            .setFilters(192)\n",
    "            .setKernelShape(Shape(3, 3))\n",
    "            .optPadding(Shape(1, 1))\n",
    "            .build())\n",
    "    .add(Activation::relu)\n",
    "    .add(Pool.maxPool2dBlock(Shape(3, 3), Shape(2, 2), Shape(1, 1)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third component connects two complete Inception blocks in series.\n",
    "The number of output channels of the first Inception block is\n",
    "$64+128+32+32=256$, and the ratio to the output channels\n",
    "of the four paths is $64:128:32:32=2:4:1:1$.\n",
    "The second and third paths first reduce the number of input channels\n",
    "to $96/192=1/2$ and $16/192=1/12$, respectively,\n",
    "and then connect the second convolutional layer.\n",
    "The number of output channels of the second Inception block\n",
    "is increased to $128+192+96+64=480$, and the ratio to the number of output channels per path is $128:192:96:64 = 4:6:3:2$.\n",
    "The second and third paths first reduce the number of input channels\n",
    "to $128/256=1/2$ and $32/256=1/8$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialBlock {\n",
       "\tParallelBlock {\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tmaxPool2d\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t}\n",
       "\tParallelBlock {\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tmaxPool2d\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t}\n",
       "\tmaxPool2d\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val block3 = SequentialBlock();\n",
    "block3\n",
    "        .add(inceptionBlock(64, intArrayOf(96, 128), intArrayOf(16, 32), 32))\n",
    "        .add(inceptionBlock(128, intArrayOf(128, 192), intArrayOf(32, 96), 64))\n",
    "        .add(Pool.maxPool2dBlock(Shape(3, 3), Shape(2, 2), Shape(1, 1)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth block is more complicated.\n",
    "It connects five Inception blocks in series,\n",
    "and they have $192+208+48+64=512$, $160+224+64+64=512$,\n",
    "$128+256+64+64=512$, $112+288+64+64=528$,\n",
    "and $256+320+128+128=832$ output channels, respectively.\n",
    "The number of channels assigned to these paths is similar\n",
    "to that in the third module:\n",
    "the second path with the $3\\times 3$ convolutional layer\n",
    "outputs the largest number of channels,\n",
    "followed by the first path with only the $1\\times 1$ convolutional layer,\n",
    "the third path with the $5\\times 5$ convolutional layer,\n",
    "and the fourth path with the $3\\times 3$ maximum pooling layer.\n",
    "The second and third paths will first reduce\n",
    "the number of channels according the ratio.\n",
    "These ratios are slightly different in different Inception blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialBlock {\n",
       "\tParallelBlock {\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tmaxPool2d\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t}\n",
       "\tParallelBlock {\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tmaxPool2d\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t}\n",
       "\tParallelBlock {\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tmaxPool2d\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t}\n",
       "\tParallelBlock {\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tmaxPool2d\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t}\n",
       "\tParallelBlock {\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t\tSequentialBlock {\n",
       "\t\t\tmaxPool2d\n",
       "\t\t\tConv2d\n",
       "\t\t\tLambdaBlock\n",
       "\t\t}\n",
       "\t}\n",
       "\tmaxPool2d\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val block4 = SequentialBlock();\n",
    "block4\n",
    "        .add(inceptionBlock(192, intArrayOf(96, 208), intArrayOf(16, 48), 64))\n",
    "        .add(inceptionBlock(160, intArrayOf(112, 224),intArrayOf(24, 64), 64))\n",
    "        .add(inceptionBlock(128, intArrayOf(128, 256), intArrayOf(24, 64), 64))\n",
    "        .add(inceptionBlock(112, intArrayOf(144, 288), intArrayOf(32, 64), 64))\n",
    "        .add(inceptionBlock(256, intArrayOf(160, 320), intArrayOf(32, 128), 128))\n",
    "        .add(Pool.maxPool2dBlock(Shape(3, 3), Shape(2, 2), Shape(1, 1)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fifth block has two Inception blocks with $256+320+128+128=832$\n",
    "and $384+384+128+128=1024$ output channels.\n",
    "The number of channels assigned to each path\n",
    "is the same as that in the third and fourth modules,\n",
    "but differs in specific values.\n",
    "It should be noted that the fifth block is followed by the output layer.\n",
    "This block uses the global average pooling layer\n",
    "to change the height and width of each channel to 1, just as in NiN.\n",
    "Finally, we turn the output into a two-dimensional array\n",
    "followed by a fully-connected layer\n",
    "whose number of outputs is the number of label classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "val block5 = SequentialBlock();\n",
    "block5\n",
    "        .add(inceptionBlock(256, intArrayOf(160, 320), intArrayOf(32, 128), 128))\n",
    "        .add(inceptionBlock(384, intArrayOf(192, 384), intArrayOf(48, 128), 128))\n",
    "        .add(Pool.globalAvgPool2dBlock());\n",
    "\n",
    "var block = SequentialBlock();\n",
    "block = block.addAll(block1, block2, block3, block4, block5, Linear.builder().setUnits(10).build());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GoogLeNet model is computationally complex,\n",
    "so it is not as easy to modify the number of channels as in VGG.\n",
    "To have a reasonable training time on Fashion-MNIST,\n",
    "we reduce the input height and width from 224 to 96.\n",
    "This simplifies the computation.\n",
    "The changes in the shape of the output\n",
    "between the various modules is demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01SequentialBlock0 layer output : (1, 64, 24, 24)\n",
      "02SequentialBlock1 layer output : (1, 192, 12, 12)\n",
      "03SequentialBlock2 layer output : (1, 480, 6, 6)\n",
      "04SequentialBlock3 layer output : (1, 832, 3, 3)\n",
      "05SequentialBlock4 layer output : (1, 1024)\n",
      "06Linear5 layer output : (1, 10)\n"
     ]
    }
   ],
   "source": [
    "val manager = NDManager.newBaseManager();\n",
    "val lr = 0.1f;\n",
    "val model = Model.newInstance(\"cnn\");\n",
    "model.setBlock(block);\n",
    "\n",
    "val loss = Loss.softmaxCrossEntropyLoss();\n",
    "\n",
    "val lrt = Tracker.fixed(lr);\n",
    "val sgd = Optimizer.sgd().setLearningRateTracker(lrt).build();\n",
    "\n",
    "val config = DefaultTrainingConfig(loss).optOptimizer(sgd) // Optimizer (loss function)\n",
    "        .optDevices(Engine.getInstance().getDevices(1)) // single GPU\n",
    "        .addEvaluator(Accuracy()) // Model Accuracy\n",
    "        .addTrainingListeners(*TrainingListener.Defaults.logging()); // Logging\n",
    "\n",
    "val trainer = model.newTrainer(config);\n",
    "\n",
    "val X = manager.randomUniform(0f, 1.0f, Shape(1, 1, 96, 96));\n",
    "trainer.initialize(X.getShape());\n",
    "var currentShape = X.getShape();\n",
    "\n",
    "for (i in 0 until block.getChildren().size()) {\n",
    "    val newShape = block.getChildren().get(i).getValue().getOutputShapes(arrayOf<Shape>(currentShape));\n",
    "    currentShape = newShape[0];\n",
    "    println(block.getChildren().get(i).getKey()+ i + \" layer output : \" + currentShape);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition and Training\n",
    "\n",
    "As before, we train our model using the Fashion-MNIST dataset.\n",
    " We transform it to $96 \\times 96$ pixel resolution\n",
    " before invoking the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val batchSize = 128;\n",
    "val numEpochs = Integer.getInteger(\"MAX_EPOCH\", 10);\n",
    "\n",
    "//double[] trainLoss;\n",
    "//double[] testAccuracy;\n",
    "//double[] epochCount;\n",
    "//double[] trainAccuracy;\n",
    "\n",
    "val epochCount = IntArray(numEpochs) { it + 1 }\n",
    "//new double[numEpochs];\n",
    "//\n",
    "//for (int i = 0; i < epochCount.length; i++) {\n",
    "//    epochCount[i] = (i + 1);\n",
    "//}\n",
    "\n",
    "val trainIter = FashionMnist.builder()\n",
    "        .addTransform(Resize(96))\n",
    "        .addTransform(ToTensor())\n",
    "        .optUsage(Dataset.Usage.TRAIN)\n",
    "        .setSampling(batchSize, true)\n",
    "        .optLimit(getLong(\"DATASET_LIMIT\", Long.MAX_VALUE))\n",
    "        .build();\n",
    "\n",
    "val testIter = FashionMnist.builder()\n",
    "        .addTransform(Resize(96))\n",
    "        .addTransform(ToTensor())\n",
    "        .optUsage(Dataset.Usage.TEST)\n",
    "        .setSampling(batchSize, true)\n",
    "        .optLimit(getLong(\"DATASET_LIMIT\", Long.MAX_VALUE))\n",
    "        .build();\n",
    "\n",
    "trainIter.prepare();\n",
    "testIter.prepare();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.38, SoftmaxCrossEntropyLoss: 1.73: 2.55██████████████                 | Accuracy: 0.21, SoftmaxCrossEntropyLoss: 2.27, SoftmaxCrossEntropyLoss: 2.21 2.11\n",
      "Validating:  100% |████████████████████████████████████████|         |ng:   80% |█████████████████████████████████       |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.73, SoftmaxCrossEntropyLoss: 0.73��█                                 | Accuracy: 0.58, SoftmaxCrossEntropyLoss: 1.17��████████████                        | Accuracy: 0.64, SoftmaxCrossEntropyLoss: 0.96       | Accuracy: 0.71, SoftmaxCrossEntropyLoss: 0.79xCrossEntropyLoss: 0.73\n",
      "Validating:  100% |████████████████████████████████████████|�██                               |         |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.84, SoftmaxCrossEntropyLoss: 0.44��███████████                        | Accuracy: 0.82, SoftmaxCrossEntropyLoss: 0.48\n",
      "Validating:  100% |████████████████████████████████████████|                     |�████████████████               |\n",
      "Training:    100% |████████████████████████████████████████| Accuracy: 0.87, SoftmaxCrossEntropyLoss: 0.36: 0.36\n",
      "Validating:  100% |████████████████████████████████████████|\n",
      "Training:     47% |███████████████████                     | Accuracy: 0.88, SoftmaxCrossEntropyLoss: 0.32aining:      8% |████                                    | Accuracy: 0.88, SoftmaxCrossEntropyLoss: 0.32"
     ]
    }
   ],
   "source": [
    "val evaluatorMetrics = mutableMapOf<String, DoubleArray>()\n",
    "val avgTrainTimePerEpoch = Training.trainingChapter6(trainIter, testIter, numEpochs, trainer, evaluatorMetrics);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Line_960.jupyter-kts (1:1 - 10) Unresolved reference: trainLoss\n",
      "Line_960.jupyter-kts (2:1 - 14) Unresolved reference: trainAccuracy\n",
      "Line_960.jupyter-kts (3:1 - 13) Unresolved reference: testAccuracy\n",
      "Line_960.jupyter-kts (5:27 - 36) Unresolved reference: trainLoss\n",
      "Line_960.jupyter-kts (6:33 - 46) Unresolved reference: trainAccuracy\n",
      "Line_960.jupyter-kts (7:33 - 45) Unresolved reference: testAccuracy\n",
      "Line_960.jupyter-kts (8:86 - 88) The integer literal does not conform to the expected type Double\n",
      "Line_960.jupyter-kts (8:90 - 91) The integer literal does not conform to the expected type Double"
     ]
    }
   ],
   "source": [
    "trainLoss = evaluatorMetrics.get(\"train_epoch_SoftmaxCrossEntropyLoss\");\n",
    "trainAccuracy = evaluatorMetrics.get(\"train_epoch_Accuracy\");\n",
    "testAccuracy = evaluatorMetrics.get(\"validate_epoch_Accuracy\");\n",
    "\n",
    "print(\"loss %.3f,\".format(trainLoss[numEpochs - 1]))\n",
    "print(\" train acc %.3f,\".format(trainAccuracy[numEpochs - 1]))\n",
    "print(\" test acc %.3f\\n\".format(testAccuracy[numEpochs - 1]))\n",
    "print(\"%.1f examples/sec\".format(trainIter.size() / (avgTrainTimePerEpoch / Math.pow(10, 9))))\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Contour Gradient Descent.](https://d2l-java-resources.s3.amazonaws.com/img/chapter_convolution-modern-cnn-googleNet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Line_962.jupyter-kts (3:37 - 46) Unresolved reference: trainLoss\n",
      "Line_962.jupyter-kts (4:35 - 44) Unresolved reference: trainLoss\n",
      "Line_962.jupyter-kts (5:34 - 43) Unresolved reference: trainLoss\n",
      "Line_962.jupyter-kts (9:20 - 29) Unresolved reference: trainLoss\n",
      "Line_962.jupyter-kts (9:34 - 47) Unresolved reference: trainAccuracy\n",
      "Line_962.jupyter-kts (9:52 - 64) Unresolved reference: testAccuracy"
     ]
    }
   ],
   "source": [
    "// String[] lossLabel = new String[trainLoss.length + testAccuracy.length + trainAccuracy.length];\n",
    "\n",
    "val trainLossLabel =  Array<String>(trainLoss!!.size) { \"train loss\" }\n",
    "val trainAccLabel = Array<String>(trainLoss!!.size) { \"train acc\" }\n",
    "val testAccLabel = Array<String>(trainLoss!!.size) { \"test acc\" }\n",
    "val data = mapOf<String, Any>(\n",
    "      \"label\" to trainLossLabel + trainAccLabel + testAccLabel,\n",
    "      \"epoch\" to epochCount + epochCount + epochCount,\n",
    "      \"metrics\" to trainLoss!! + trainAccuracy!! + testAccuracy!!\n",
    ")\n",
    "\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine { x = \"epoch\" ; y = \"metrics\" ; color = \"label\"}\n",
    "plot + ggsize(700, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* The Inception block is equivalent to a subnetwork with four paths. It extracts information in parallel through convolutional layers of different window shapes and maximum pooling layers. $1 \\times 1$ convolutions reduce channel dimensionality on a per-pixel level. Max-pooling reduces the resolution.\n",
    "* GoogLeNet connects multiple well-designed Inception blocks with other layers in series. The ratio of the number of channels assigned in the Inception block is obtained through a large number of experiments on the ImageNet dataset.\n",
    "* GoogLeNet, as well as its succeeding versions, was one of the most efficient models on ImageNet, providing similar test accuracy with lower computational complexity.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. There are several iterations of GoogLeNet. Try to implement and run them. Some of them include the following:\n",
    "    * Add a batch normalization layer :cite:`Ioffe.Szegedy.2015`, as described\n",
    "      later in :numref:`sec_batch_norm`.\n",
    "    * Make adjustments to the Inception block\n",
    "      :cite:`Szegedy.Vanhoucke.Ioffe.ea.2016`.\n",
    "    * Use \"label smoothing\" for model regularization\n",
    "      :cite:`Szegedy.Vanhoucke.Ioffe.ea.2016`.\n",
    "    * Include it in the residual connection\n",
    "      :cite:`Szegedy.Ioffe.Vanhoucke.ea.2017`, as described later in\n",
    "      :numref:`sec_resnet`.\n",
    "1. What is the minimum image size for GoogLeNet to work?\n",
    "1. Compare the model parameter sizes of AlexNet, VGG, and NiN with GoogLeNet. How do the latter two network architectures significantly reduce the model parameter size?\n",
    "1. Why do we need a large range convolution initially?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
