{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Layers\n",
    "\n",
    "One factor behind deep learning's success\n",
    "is the availability of a wide range of layers\n",
    "that can be composed in creative ways\n",
    "to design architectures suitable\n",
    "for a wide variety of tasks.\n",
    "For instance, researchers have invented layers\n",
    "specifically for handling images, text,\n",
    "looping over sequential data,\n",
    "performing dynamic programming, etc.\n",
    "Sooner or later you will encounter (or invent)\n",
    "a layer that does not exist yet in DJL.\n",
    "In these cases, you must build a custom layer.\n",
    "In this section, we show you how.\n",
    "\n",
    "## Layers without Parameters\n",
    "\n",
    "To start, we construct a custom layer (a Block) \n",
    "that does not have any parameters of its own. \n",
    "This should look familiar if you recall our \n",
    "introduction to DJL's `Block` in :numref:`sec_model_construction`. \n",
    "The following `CenteredLayer` class simply\n",
    "subtracts the mean from its input. \n",
    "To build it, we simply need to inherit \n",
    "from the `AbstractBlock` class and implement the `forward()` and `getOutputShapes()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%use @file[../djl.json]\n",
    "// %load ../utils/djl-imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenteredLayer : AbstractBlock() {\n",
    "\n",
    "    @Override\n",
    "    override protected fun forwardInternal(\n",
    "            parameterStore: ParameterStore ,\n",
    "            inputs: NDList ,\n",
    "            training: Boolean,\n",
    "            params: PairList<String, Any>? ) : NDList {\n",
    "        val current = inputs;\n",
    "        // Subtract the mean from the input\n",
    "        return NDList(current.head().sub(current.head().mean()));\n",
    "    }\n",
    "    \n",
    "    @Override\n",
    "    override fun getOutputShapes(inputs: Array<Shape>): Array<Shape> {\n",
    "        // Output shape should be the same as input\n",
    "        return inputs;\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify that our layer works as intended by feeding some data through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ND: (5) cpu() float32\n",
       "[-2., -1.,  0.,  1.,  2.]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val manager = NDManager.newBaseManager();\n",
    "\n",
    "val layer = CenteredLayer()\n",
    "\n",
    "val model = Model.newInstance(\"centered-layer\");\n",
    "model.setBlock(layer);\n",
    "\n",
    "val predictor = model.newPredictor(NoopTranslator());\n",
    "val input = manager.create(floatArrayOf(1f, 2f, 3f, 4f, 5f))\n",
    "predictor.predict(NDList(input)).singletonOrThrow();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now incorporate our layer as a component\n",
    "in constructing more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val net = SequentialBlock();\n",
    "net.add(Linear.builder().setUnits(128).build());\n",
    "net.add(CenteredLayer());\n",
    "net.setInitializer(NormalInitializer(), Parameter.Type.WEIGHT);\n",
    "net.initialize(manager, DataType.FLOAT32, input.getShape());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an extra sanity check, we can send random data \n",
    "through the network and check that the mean is in fact 0.\n",
    "Because we are dealing with floating point numbers, \n",
    "we may still see a *very* small nonzero number\n",
    "due to quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ND: () cpu() float32\n",
       " 6.40284270e-10\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input = manager.randomUniform(-0.07f, 0.07f, Shape(4, 8));\n",
    "val y = predictor.predict(NDList(input)).singletonOrThrow();\n",
    "y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers with Parameters\n",
    "\n",
    "Now that we know how to define simple layers,\n",
    "let us move on to defining layers with parameters\n",
    "that can be adjusted through training. \n",
    "This lets us tell DJL what we need to calculate gradients for.\n",
    "To automate some of the routine work,\n",
    "the `Parameter` class and the `ParameterList` \n",
    "provide some basic housekeeping functionality.\n",
    "In particular, they govern access, initialization, \n",
    "sharing, saving, and loading model parameters. \n",
    "This way, among other benefits, we will not need to write\n",
    "custom serialization routines for every custom layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the basic ingredients that we need\n",
    "to implement our own version of DJL's `Linear` layer. \n",
    "Recall that this layer requires two parameters:\n",
    "one for weight and one for bias. \n",
    "In this implementation, we bake in the ReLU activation as a default.\n",
    "In the constructor, `inUnits` and `outUnits`\n",
    "denote the number of inputs and outputs, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a new `Parameter` by calling its constructor and passing in\n",
    "a name, a reference to the block it is to be associated with, and its type which\n",
    "we can set from `ParameterType`.\n",
    "Then we call `addParameter()` in our `Linear`'s constructor \n",
    "with the newly instantiated `Parameter` and its respective `Shape`.\n",
    "We do this for both weight and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [],
   "source": [
    "class MyLinear(val outUnits: Int, val inUnits: Int): AbstractBlock() {\n",
    "\n",
    "    private val weight:  Parameter \n",
    "    private val  bias: Parameter\n",
    "    \n",
    "\n",
    "    // outUnits: the number of outputs in this layer \n",
    "    // inUnits: the number of inputs in this layer\n",
    "    init {\n",
    "        weight = addParameter(\n",
    "            Parameter.builder()\n",
    "                .setName(\"weight\")\n",
    "                .setType(Parameter.Type.WEIGHT)\n",
    "                .optShape(Shape(inUnits.toLong(), outUnits.toLong()))\n",
    "                .build())\n",
    "        bias = addParameter(\n",
    "            Parameter.builder()\n",
    "                .setName(\"bias\")\n",
    "                .setType(Parameter.Type.BIAS)\n",
    "                .optShape(Shape(outUnits.toLong()))\n",
    "                .build())\n",
    "    }\n",
    "    \n",
    "    @Override\n",
    "    override protected fun forwardInternal(\n",
    "            parameterStore: ParameterStore ,\n",
    "            inputs: NDList ,\n",
    "            training: Boolean,\n",
    "            params: PairList<String, Any>?): NDList {\n",
    "        val input = inputs.singletonOrThrow();\n",
    "        val device = input.getDevice();\n",
    "        // Since we added the parameter, we can now access it from the parameter store\n",
    "        val weightArr = parameterStore.getValue(weight, device, false);\n",
    "        val biasArr = parameterStore.getValue(bias, device, false);\n",
    "        return relu(linear(input, weightArr, biasArr));\n",
    "    }\n",
    "    \n",
    "    // Applies linear transformation\n",
    "    fun linear(input: NDArray , weight: NDArray , bias: NDArray) : NDArray {\n",
    "        return input.dot(weight).add(bias);\n",
    "    }\n",
    "    \n",
    "    // Applies relu transformation\n",
    "    fun relu(input: NDArray) : NDList {\n",
    "        return NDList(Activation.relu(input));\n",
    "    }\n",
    "    \n",
    "    @Override\n",
    "    override fun getOutputShapes(inputs: Array<Shape>) : Array<Shape> {\n",
    "        return arrayOf<Shape>(Shape(outUnits.toLong(), inUnits.toLong()))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the `MyLinear` class \n",
    "and access its model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight\n",
      "bias\n"
     ]
    }
   ],
   "source": [
    "// 5 units in -> 3 units out\n",
    "val linear = MyLinear(3, 5); \n",
    "var params = linear.getParameters();\n",
    "for (param in params) {\n",
    "    println(param.getKey());\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us initialize and test our `Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ND: (2, 3) cpu() float32\n",
       "[[1.2648, 0.    , 0.    ],\n",
       " [0.1136, 0.302 , 0.    ],\n",
       "]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input = manager.randomUniform(0f, 1f, Shape(2, 5));\n",
    "\n",
    "linear.initialize(manager, DataType.FLOAT32, input.getShape());\n",
    "\n",
    "val model = Model.newInstance(\"my-linear\");\n",
    "model.setBlock(linear);\n",
    "\n",
    "val predictor = model.newPredictor(NoopTranslator());\n",
    "predictor.predict(NDList(input)).singletonOrThrow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also construct models using custom layers.\n",
    "Once we have that we can use it just like the built-in dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ND: (2, 1) cpu() float32\n",
       "[[11.008 ],\n",
       " [ 3.3061],\n",
       "]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input = manager.randomUniform(0f, 1f, Shape(2, 64));\n",
    "\n",
    "val net = SequentialBlock();\n",
    "net.add(MyLinear(8, 64)); // 64 units in -> 8 units out\n",
    "net.add(MyLinear(1, 8)); // 8 units in -> 1 unit out\n",
    "net.initialize(manager, DataType.FLOAT32, input.getShape());\n",
    "\n",
    "val model = Model.newInstance(\"lin-reg-custom\");\n",
    "model.setBlock(net);\n",
    "\n",
    "val predictor = model.newPredictor(NoopTranslator());\n",
    "predictor.predict(NDList(input)).singletonOrThrow();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* We can design custom layers via the Block class. This allows us to define flexible new layers that behave differently from any existing layers in the library.\n",
    "* Once defined, custom layers can be invoked in arbitrary contexts and architectures.\n",
    "* Blocks can have local parameters, which are stored in a `LinkedHashMap<String, Parameter>` object in each `parameters` attribute.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Design a layer that learns an affine transform of the data.\n",
    "1. Design a layer that takes an input and computes a tensor reduction, \n",
    "   i.e., it returns $y_k = \\sum_{i, j} W_{ijk} x_i x_j$.\n",
    "1. Design a layer that returns the leading half of the Fourier coefficients of the data. Hint: look up `Fast Fourier Transform`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
