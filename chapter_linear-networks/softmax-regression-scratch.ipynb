{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Softmax Regression from Scratch\n",
    ":label:`sec_softmax_scratch`\n",
    "\n",
    "Just as we implemented linear regression from scratch,\n",
    "we believe that multiclass logistic (softmax) regression\n",
    "is similarly fundamental and you ought to know\n",
    "the gory details of how to implement it yourself.\n",
    "As with linear regression, after doing things by hand\n",
    "we will breeze through an implementation in DJL for comparison.\n",
    "To begin, let us import the familiar packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"VKZfUI\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "       if(!window.letsPlotCallQueue) {\n",
       "           window.letsPlotCallQueue = [];\n",
       "       }; \n",
       "       window.letsPlotCall = function(f) {\n",
       "           window.letsPlotCallQueue.push(f);\n",
       "       };\n",
       "       (function() {\n",
       "           var script = document.createElement(\"script\");\n",
       "           script.type = \"text/javascript\";\n",
       "           script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.4.0/js-package/distr/lets-plot.min.js\";\n",
       "           script.onload = function() {\n",
       "               window.letsPlotCall = function(f) {f();};\n",
       "               window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "               window.letsPlotCallQueue = [];\n",
       "               \n",
       "               \n",
       "           };\n",
       "           script.onerror = function(event) {\n",
       "               window.letsPlotCall = function(f) {};\n",
       "               window.letsPlotCallQueue = [];\n",
       "               var div = document.createElement(\"div\");\n",
       "               div.style.color = 'darkred';\n",
       "               div.textContent = 'Error loading Lets-Plot JS';\n",
       "               document.getElementById(\"VKZfUI\").appendChild(div);\n",
       "           };\n",
       "           var e = document.getElementById(\"VKZfUI\");\n",
       "           e.appendChild(script);\n",
       "       })();\n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%use @file[../djl.json]\n",
    "%use lets-plot\n",
    "@file:DependsOn(\"../D2J-1.0-SNAPSHOT.jar\")\n",
    "//import jp.live.ugai.d2j.attention.Chap10Utils\n",
    "import jp.live.ugai.d2j.util.Training\n",
    "import jp.live.ugai.d2j.util.ImageUtils\n",
    "import jp.live.ugai.d2j.util.Accumulator\n",
    "// %load ../utils/plot-utils.ipynb\n",
    "// %load ../utils/Training.java\n",
    "// %load ../utils/FashionMnistUtils.java\n",
    "// %load ../utils/ImageUtils.java\n",
    "import ai.djl.basicdataset.cv.classification.FashionMnist\n",
    "import ai.djl.metric.Metrics\n",
    "\n",
    "fun getLong(nm: String, n: Long): Long {\n",
    "    val name = System.getProperty(nm)\n",
    "    return if (null == name) n.toLong() else name.toLong()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ai.djl.ndarray.NDArray;\n",
    "import ai.djl.ndarray.NDManager;\n",
    "import ai.djl.training.dataset.ArrayDataset;\n",
    "import ai.djl.training.dataset.Record;\n",
    "\n",
    "import java.awt.Color;\n",
    "import java.awt.Graphics2D;\n",
    "import java.awt.image.BufferedImage;\n",
    "\n",
    "object FashionMnistUtils {\n",
    "\n",
    "    fun getFashionMnistLabels(labelIndices: IntArray) : Array<String>{\n",
    "        val textLabels = arrayOf(\n",
    "            \"t-shirt\",\n",
    "            \"trouser\",\n",
    "            \"pullover\",\n",
    "            \"dress\",\n",
    "            \"coat\",\n",
    "            \"sandal\",\n",
    "            \"shirt\",\n",
    "            \"sneaker\",\n",
    "            \"bag\",\n",
    "            \"ankle boot\"\n",
    "        )\n",
    "        val convertedLabels = arrayOf<String>()\n",
    "        for (i in 0.. labelIndices.size-1) {\n",
    "            convertedLabels[i] = textLabels[labelIndices[i]];\n",
    "        }\n",
    "        return convertedLabels;\n",
    "    }\n",
    "\n",
    "    fun getFashionMnistLabel(labelIndice: Int) : String{\n",
    "        val textLabels = arrayOf(\n",
    "            \"t-shirt\",\n",
    "            \"trouser\",\n",
    "            \"pullover\",\n",
    "            \"dress\",\n",
    "            \"coat\",\n",
    "            \"sandal\",\n",
    "            \"shirt\",\n",
    "            \"sneaker\",\n",
    "            \"bag\",\n",
    "            \"ankle boot\"\n",
    "        )\n",
    "        return textLabels[labelIndice]\n",
    "    }\n",
    "\n",
    "    fun showImages(\n",
    "            dataset: ArrayDataset , number: Int, width: Int, height: Int,scale: Int,manager: NDManager): BufferedImage {\n",
    "        val images = mutableListOf<BufferedImage>()\n",
    "        val labels = mutableListOf<String>()\n",
    "        for (i in 0 until number) {\n",
    "            val record = dataset.get(manager, i.toLong());\n",
    "            val array = record.getData().get(0).squeeze(-1);\n",
    "            val y = record.getLabels().get(0).getFloat().toInt()\n",
    "            images.add(toImage(array, width, height))\n",
    "            labels.add(getFashionMnistLabel(y))\n",
    "        }\n",
    "        val w = images.first().getWidth() * scale;\n",
    "        val h = images.first().getHeight() * scale;\n",
    "\n",
    "        return ImageUtils.showImages(images, labels, w, h);\n",
    "    }\n",
    "\n",
    "    fun showImages(\n",
    "            dataset: ArrayDataset ,\n",
    "            predLabels: IntArray,\n",
    "            width: Int,\n",
    "            height: Int,\n",
    "            scale: Int,\n",
    "            manager: NDManager ) : BufferedImage{\n",
    "        val number = predLabels.size\n",
    "        val images = mutableListOf<BufferedImage>()\n",
    "        val labels = mutableListOf<String>()\n",
    "        for (i in 0.. number-1) {\n",
    "            val record = dataset.get(manager, i.toLong());\n",
    "            val array = record.getData().get(0).squeeze(-1);\n",
    "            images.add(toImage(array, width, height))\n",
    "            labels.add(getFashionMnistLabel(predLabels[i]))\n",
    "        }\n",
    "        val w = images.first().getWidth() * scale;\n",
    "        val h = images.first().getHeight() * scale;\n",
    "\n",
    "        return ImageUtils.showImages(images, labels, w, h);\n",
    "    }\n",
    "\n",
    "    fun toImage(array: NDArray,width: Int,height: Int) : BufferedImage{\n",
    "        System.setProperty(\"apple.awt.UIElement\", \"true\");\n",
    "        val img = BufferedImage(width, height, BufferedImage.TYPE_BYTE_GRAY);\n",
    "        val g = img.getGraphics();\n",
    "        for (i in 0 .. width-1) {\n",
    "            for (j in 0 .. height-1) {\n",
    "                val c = array.getFloat(j.toLong(), i.toLong()) / 255; // scale down to between 0 and 1\n",
    "                g.setColor(Color(c, c, c)); // set as a gray color\n",
    "                g.fillRect(i, j, 1, 1);\n",
    "            }\n",
    "        }\n",
    "        g.dispose();\n",
    "        return img;\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with the Fashion-MNIST dataset, just introduced in :numref:`sec_fashion_mnist`,\n",
    "setting up an iterator with batch size $256$. We also set it to randomly shuffled elements for each batch for the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "val batchSize = 256\n",
    "val randomShuffle = true\n",
    "\n",
    "// get training and validation dataset\n",
    "val trainingSet = FashionMnist.builder()\n",
    "        .optUsage(Dataset.Usage.TRAIN)\n",
    "        .setSampling(batchSize, randomShuffle)\n",
    "        .optLimit(getLong(\"DATASET_LIMIT\", Long.MAX_VALUE))\n",
    "        .build()\n",
    "\n",
    "val validationSet = FashionMnist.builder()\n",
    "        .optUsage(Dataset.Usage.TEST)\n",
    "        .setSampling(batchSize, false)\n",
    "        .optLimit(getLong(\"DATASET_LIMIT\", Long.MAX_VALUE))\n",
    "        .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Model Parameters\n",
    "\n",
    "As in our linear regression example,\n",
    "each example here will be represented by a fixed-length vector.\n",
    "Each example in the raw data is a $28 \\times 28$ image.\n",
    "In this section, we will flatten each image,\n",
    "treating them as $784$-long 1D vectors.\n",
    "In the future, we will talk about more sophisticated strategies\n",
    "for exploiting the spatial structure in images,\n",
    "but for now we treat each pixel location as just another feature.\n",
    "\n",
    "Recall that in softmax regression,\n",
    "we have as many outputs as there are categories.\n",
    "Because our dataset has $10$ categories,\n",
    "our network will have an output dimension of $10$.\n",
    "Consequently, our weights will constitute a $784 \\times 10$ matrix\n",
    "and the biases will constitute a $1 \\times 10$ vector.\n",
    "As with linear regression, we will initialize our weights $W$\n",
    "with Gaussian noise and our biases to take the initial value $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "val numInputs = 784L\n",
    "val numOutputs = 10L\n",
    "\n",
    "val manager = NDManager.newBaseManager()\n",
    "val W = manager.randomNormal(0f, 0.01f, Shape(numInputs, numOutputs), DataType.FLOAT32);\n",
    "val b = manager.zeros(Shape(numOutputs), DataType.FLOAT32);\n",
    "val params = NDList(W, b)\n",
    "    // Attach Gradients\n",
    "for (param in params) {\n",
    "    param.setRequiresGradient(true)\n",
    "}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Softmax\n",
    "\n",
    "Before implementing the softmax regression model,\n",
    "let us briefly review how operators such as `sum()` work\n",
    "along specific dimensions in an `NDArray`.\n",
    "Given a matrix `X` we can sum over all elements (default) or only\n",
    "over elements in the same axis, *i.e.*, the column (`new int[]{0}`) or the same row (`new int[]{1}`).\n",
    "We wrap the axis in an int array since we can specify multiple axes as well.\n",
    "For example if we call `sum()` with `new int[]{0, 1}`, it sums up the elements over both the rows and columns.\n",
    "In this 2D array, this means the total sum of the elements within! \n",
    "Note that if `X` is an array with shape `($2$, $3$)`\n",
    "and we sum over the columns (`X.sum(new int[]{0})`),\n",
    "the result will be a (1D) vector with shape `($3$,)`.\n",
    "If we want to keep the number of axes in the original array\n",
    "(resulting in a 2D array with shape `($1$, $3$)`),\n",
    "rather than collapsing out the dimension that we summed over\n",
    "we can specify `true` when invoking `sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ND: (2, 3) cpu() int32\n",
      "[[ 1,  2,  3],\n",
      " [ 4,  5,  6],\n",
      "]\n",
      "\n",
      "ND: (1, 3) cpu() int32\n",
      "[[ 5,  7,  9],\n",
      "]\n",
      "\n",
      "ND: (2, 1) cpu() int32\n",
      "[[ 6],\n",
      " [15],\n",
      "]\n",
      "\n",
      "ND: (1, 1) cpu() int32\n",
      "[[21],\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val X = manager.create(arrayOf(intArrayOf(1, 2, 3), intArrayOf(4, 5, 6)))\n",
    "println(X)\n",
    "println(X.sum(intArrayOf(0), true))\n",
    "println(X.sum(intArrayOf(1), true))\n",
    "println(X.sum(intArrayOf(0, 1), true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to implement the softmax function.\n",
    "Recall that softmax consists of two steps:\n",
    "First, we exponentiate each term (using `exp()`).\n",
    "Then, we sum over each row (we have one row per example in the batch)\n",
    "to get the normalization constants for each example.\n",
    "Finally, we divide each row by its normalization constant,\n",
    "ensuring that the result sums to $1$.\n",
    "Before looking at the code, let us recall\n",
    "how this looks expressed as an equation:\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(\\mathbf{X})_{ij} = \\frac{\\exp(X_{ij})}{\\sum_k \\exp(X_{ik})}.\n",
    "$$\n",
    "\n",
    "The denominator, or normalization constant,\n",
    "is also sometimes called the partition function\n",
    "(and its logarithm is called the log-partition function).\n",
    "The origins of that name are in [statistical physics](https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics))\n",
    "where a related equation models the distribution\n",
    "over an ensemble of particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "fun softmax(X: NDArray): NDArray  {\n",
    "    val Xexp = X.exp()\n",
    "    val partition = Xexp.sum(intArrayOf(1), true)\n",
    "    return Xexp.div(partition) // The broadcast mechanism is applied here\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, for any random input,\n",
    "we turn each element into a non-negative number.\n",
    "Moreover, each row sums up to 1,\n",
    "as is required for a probability.\n",
    "Note that while this looks correct mathematically,\n",
    "we were a bit sloppy in our implementation\n",
    "because we failed to take precautions against numerical overflow or underflow\n",
    "due to large (or very small) elements of the matrix,\n",
    "as we did in :numref:`sec_naive_bayes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ND: (2, 5) cpu() float32\n",
      "[[0.4817, 0.1687, 0.1143, 0.0855, 0.1497],\n",
      " [0.106 , 0.2568, 0.0916, 0.5211, 0.0244],\n",
      "]\n",
      "\n",
      "ND: (2) cpu() float32\n",
      "[1., 1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val X = manager.randomNormal(Shape(2, 5))\n",
    "val Xprob = softmax(X)\n",
    "println(Xprob)\n",
    "println(Xprob.sum(intArrayOf(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "Now that we have defined the softmax operation,\n",
    "we can implement the softmax regression model.\n",
    "The below code defines the forward pass through the network.\n",
    "Note that we flatten each original image in the batch\n",
    "into a vector with length `numInputs` with the `reshape()` function\n",
    "before passing the data through our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "fun net(X: NDArray) : NDArray {\n",
    "        val currentW = params.get(0);\n",
    "        val currentB = params.get(1);\n",
    "        return softmax(X.reshape(Shape(-1, numInputs)).dot(currentW).add(currentB))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Loss Function\n",
    "\n",
    "Next, we need to implement the cross-entropy loss function,\n",
    "introduced in :numref:`sec_softmax`.\n",
    "This may be the most common loss function\n",
    "in all of deep learning because, at the moment,\n",
    "classification problems far outnumber regression problems.\n",
    "\n",
    "Recall that cross-entropy takes the negative log likelihood\n",
    "of the predicted probability assigned to the true label $-\\log P(y \\mid x)$.\n",
    "Rather than iterating over the predictions with a Java `for` loop\n",
    "(which tends to be inefficient),\n",
    "we can use the NDArray `get()` function in conjunction with `NDIndex`\n",
    "to let us easily select the appropriate terms\n",
    "from the matrix of softmax entries. This is typically known as the `pick()`\n",
    "operator in other frameworks such as `PyTorch`.\n",
    "Below, we illustrate the usage on a toy example,\n",
    "with $3$ categories and $2$ examples.\n",
    "\n",
    "The `\":, {}\"` section of the `NDIndex` selects all arrays\n",
    "and the `manager.create(new int[]{0, 2})` creates an\n",
    "`NDArray` with the values 0 and 2 to pick the 0th and 2nd elements\n",
    "for each respective `NDArray`.\n",
    "\n",
    "Note: when using `NDIndex` in this way, the passed in `NDArray` used for picking\n",
    "indices must be of type `int` or `long`. You can use the `toType()` function\n",
    "to change the type of the `NDArray` which will be shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "ND: (1, 2, 3) cpu() float32\n",
      "[[[0.1, 0.3, 0.6],\n",
      "  [0.3, 0.2, 0.5],\n",
      " ],\n",
      "]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ND: (2, 1) cpu() float32\n",
       "[[0.1],\n",
       " [0.5],\n",
       "]\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val yHat = manager.create(arrayOf(floatArrayOf(0.1f, 0.3f, 0.6f), floatArrayOf(0.3f, 0.2f, 0.5f))).reshape(1,2,3)\n",
    "//yHat.get(new NDIndex(\":, {}\", manager.create(new int[]{0, 2})));\n",
    "//yHat.get(NDIndex(\":, {}\", manager.create(intArrayOf(0,2))))\n",
    "//val x = NDIndex(\":,  {}\", manager.create(intArrayOf(0,2)))\n",
    "//val x = NDIndex().addAllDim(Math.floorMod(-1, yHat.getShape().dimension())).addPickDim(y)\n",
    "val x = NDIndex().addAllDim().addPickDim(manager.create(intArrayOf(0,2)))\n",
    "//println(yHat.getShape())\n",
    "println(yHat.getShape().dimension())\n",
    "//println(Math.floorMod(-1, 4))\n",
    "println(yHat.reshape(1,2,3))\n",
    "//println(y)\n",
    "//val y = NDIndex(\":, 0,2\")\n",
    "//println(yHat.get(\":, 0:2\"))\n",
    "yHat.get(0).get(x)\n",
    "//manager.create(intArrayOf(0,2))\n",
    "//yHat.get(NDIndex(\":, {}\", longArrayOf(0,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement the cross-entropy loss function efficiently with just one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "// Cross Entropy only cares about the target class's probability\n",
    "// Get the column index for each row\n",
    "fun crossEntropy(yHat: NDArray, y: NDArray) : NDArray {\n",
    "        // Here, y is not guranteed to be of datatype int or long\n",
    "        // and in our case we know its a float32.\n",
    "        // We must first convert it to int or long(here we choose int)\n",
    "        // before we can use it with NDIndex to \"pick\" indices. \n",
    "        // It also takes in a boolean for returning a copy of the existing NDArray\n",
    "        // but we don't want that so we pass in `false`.\n",
    "//        return yHat.get(NDIndex(\":, {}\", y.toType(DataType.INT32, false))).log().neg()\n",
    "        val pickIndex = NDIndex()\n",
    "            .addAllDim(Math.floorMod(-1, yHat.getShape().dimension()))\n",
    "//            .addPickDim(y.toType(DataType.INT32, false))\n",
    "            .addPickDim(y)\n",
    "        return yHat.get(pickIndex).log().neg()\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Accuracy\n",
    "\n",
    "Given the predicted probability distribution `yHat`,\n",
    "we typically choose the class with highest predicted probability\n",
    "whenever we must output a *hard* prediction.\n",
    "Indeed, many applications require that we make a choice.\n",
    "Gmail must categorize an email into Primary, Social, Updates, or Forums.\n",
    "It might estimate probabilities internally,\n",
    "but at the end of the day it has to choose one among the categories.\n",
    "\n",
    "When predictions are consistent with the actual category `y`, they are correct.\n",
    "The classification accuracy is the fraction of all predictions that are correct.\n",
    "Although it can be difficult optimize accuracy directly (it is not differentiable),\n",
    "it is often the performance metric that we care most about,\n",
    "and we will nearly always report it when training classifiers.\n",
    "\n",
    "To compute accuracy we do the following:\n",
    "First, we execute `yHat.argMax(1)` where 1 is the axis\n",
    "to gather the predicted classes\n",
    "(given by the indices for the largest entries in each row).\n",
    "The result has the same shape as the variable `y`.\n",
    "Now we just need to check how frequently the two match.\n",
    "Since the equality function `eq()` is datatype-sensitive\n",
    "(e.g., a `float32` and a `float32` are never equal),\n",
    "we also need to convert both to the same type (we pick `int32`).\n",
    "The result is an `NDArray` containing entries of 0 (false) and 1 (true).\n",
    "We then sum the number of true entries and convert the result to a float.\n",
    "Finally, we get the mean by dividing by the number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "// Saved in the utils for later use\n",
    "fun accuracy(yHat: NDArray, y: NDArray) : Float {\n",
    "    // Check size of 1st dimension greater than 1\n",
    "    // to see if we have multiple samples\n",
    "    if (yHat.getShape().size(1) > 1) {\n",
    "        // Argmax gets index of maximum args for given axis 1\n",
    "        // Convert yHat to same dataType as y (int32)\n",
    "        // Sum up number of true entries\n",
    "        return yHat.argMax(1).toType(DataType.INT32, false).eq(y.toType(DataType.INT32, false))\n",
    "            .sum().toType(DataType.FLOAT32, false).getFloat()\n",
    "    }\n",
    "    return yHat.toType(DataType.INT32, false).eq(y.toType(DataType.INT32, false))\n",
    "        .sum().toType(DataType.FLOAT32, false).getFloat()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue to use the variables `yHat` and `y`\n",
    "defined in the `pick()` function,\n",
    "as the predicted probability distribution and label, respectively.\n",
    "We can see that the first example's prediction category is $2$\n",
    "(the largest element of the row is $0.6$ with an index of $2$),\n",
    "which is inconsistent with the actual label, $0$.\n",
    "The second example's prediction category is $2$\n",
    "(the largest element of the row is $0.5$ with an index of $2$),\n",
    "which is consistent with the actual label, $2$.\n",
    "Therefore, the classification accuracy rate for these two examples is $0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val y = manager.create(intArrayOf(0,2))\n",
    "accuracy(yHat, y) / y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can evaluate the accuracy for model `net` on the dataset\n",
    "(accessed via `dataIterator`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "// Saved in the utils for future use\n",
    "fun evaluateAccuracy( net: (NDArray) -> NDArray,  dataIterator: Iterable<Batch>) : Float{\n",
    "    val metric = Accumulator(2)  // numCorrectedExamples, numExamples\n",
    "    val batch = dataIterator.iterator().next();\n",
    "    val X = batch.getData().head();\n",
    "    val y = batch.getLabels().head();\n",
    "    metric.add(floatArrayOf(accuracy(net(X), y), y.size().toFloat()))\n",
    "    batch.close()\n",
    "\n",
    "    return metric.get(0) / metric.get(1);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we initialized the `net` model with random weights,\n",
    "the accuracy of this model should be close to random guessing,\n",
    "i.e., $0.1$ for $10$ classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11328125"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateAccuracy(::net, validationSet.getData(manager))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "The training loop for softmax regression should look strikingly familiar\n",
    "if you read through our implementation\n",
    "of linear regression in :numref:`sec_linear_scratch`.\n",
    "Here we refactor the implementation to make it reusable.\n",
    "First, we define a function to train for one data epoch.\n",
    "Note that `updater()` is a general function to update the model parameters,\n",
    "which accepts the batch size as an argument.\n",
    "Currently, it is a wrapper of `Training.sgd()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [],
   "source": [
    "val lr = 0.1f\n",
    "\n",
    "fun trainEpochCh3(net: (NDArray)->NDArray ,\n",
    "                  trainIter: Iterable<Batch> ,\n",
    "                  loss: (NDArray, NDArray)->NDArray , updater: (NDList, Float, Int)->Unit ) : FloatArray {\n",
    "    val metric = Accumulator(3); // trainLossSum, trainAccSum, numExamples\n",
    "    \n",
    "    for (batch in trainIter) {\n",
    "        var X = batch.getData().head();\n",
    "        val y = batch.getLabels().head();\n",
    "        X = X.reshape(Shape(-1, numInputs));\n",
    "            \n",
    "        val gc = Engine.getInstance().newGradientCollector()\n",
    "            // Minibatch loss in X and y\n",
    "           val yHat = net(X);\n",
    "            val l = loss(yHat, y)\n",
    "            gc.backward(l);  // Compute gradient on l with respect to w and b\n",
    "            metric.add(floatArrayOf(l.sum().toType(DataType.FLOAT32, false).getFloat(), \n",
    "                                   accuracy(yHat, y), \n",
    "                                   y.size().toFloat()))\n",
    "        gc.close()\n",
    "        updater(params, lr, batch.getSize());  // Update parameters using their gradient\n",
    "        batch.close();\n",
    "    }\n",
    "    // Return trainLoss, trainAccuracy\n",
    "    return floatArrayOf(metric.get(0) / metric.get(2), metric.get(1) / metric.get(2))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before showing the implementation of the training function, we define a utility class that draws data in animation. Again, it aims to simplify the code in later chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.jetbrains.letsPlot.*\n",
    "import org.jetbrains.letsPlot.intern.Plot\n",
    "\n",
    "// Saved in utils\n",
    "/* Animates a graph with real-time data. */\n",
    "class Animator {\n",
    "    val epochs = mutableListOf<Int>()\n",
    "    val values = mutableListOf<Float>()\n",
    "    val metrics = mutableListOf<String>()      \n",
    "    // Add a single metric to the table\n",
    "   fun add(epoch: Int, value: Float, metric: String) {\n",
    "       epochs.add(epoch)\n",
    "       values.add(value)\n",
    "       metrics.add(metric)\n",
    "       \n",
    "    }\n",
    "    \n",
    "    // Add accuracy, train accuracy, and train loss metrics for a given epoch\n",
    "    // Then plot it on the graph\n",
    "    fun add(epoch : Int, accuracy: Float, trainAcc: Float, trainLoss: Float) {\n",
    "        add(epoch, trainLoss, \"train loss\");\n",
    "        add(epoch, trainAcc, \"train accuracy\");\n",
    "        add(epoch, accuracy, \"test accuracy\");\n",
    "    }\n",
    "    \n",
    "    // Display the graph\n",
    "    fun show() : Plot {\n",
    "        val data = mapOf(\"epoch\" to epochs, \"value\" to values, \"metric\" to metrics)\n",
    "        // updateDisplay(id, LinePlot.create(\"\", data, \"epoch\", \"value\", \"metric\"));\n",
    "//        println(data)\n",
    "        var plot = letsPlot(data)\n",
    "        plot += geomLine { x = \"epoch\" ; y = \"value\" ; color = \"metric\"}\n",
    "        return plot + ggsize(500, 500)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function then runs multiple epochs and visualize the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [],
   "source": [
    "fun trainCh3(net: (NDArray)->NDArray , trainDataset: Dataset , testDataset:Dataset , \n",
    "                     loss: (NDArray, NDArray)->NDArray , numEpochs: Int, updater: (NDList, Float, Int)->Unit ) \n",
    "     : Plot {\n",
    "    val animator = Animator()\n",
    "    for (i in 1..numEpochs) {\n",
    "        val trainMetrics = trainEpochCh3(net, trainDataset.getData(manager), loss, updater)\n",
    "        val accuracy = evaluateAccuracy(net, testDataset.getData(manager));\n",
    "        val trainAccuracy = trainMetrics[1];\n",
    "        val trainLoss = trainMetrics[0];\n",
    "        \n",
    "        animator.add(i, accuracy, trainAccuracy, trainLoss)\n",
    "        println(\"Epoch %d: Test Accuracy: %f\".format(i, accuracy))\n",
    "        println(\"Train Accuracy: %f\".format(trainAccuracy))\n",
    "        println(\"Train Loss: %f\".format(trainLoss))\n",
    "    }\n",
    "    return animator.show()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we use the minibatch stochastic gradient descent\n",
    "to optimize the loss function of the model.\n",
    "Note that the number of epochs (`numEpochs`),\n",
    "and learning rate (`lr`) are both adjustable hyper-parameters.\n",
    "By changing their values, we may be able\n",
    "to increase the classification accuracy of the model.\n",
    "In practice we will want to split our data three ways\n",
    "into training, validation, and test data,\n",
    "using the validation data to choose\n",
    "the best values of our hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Test Accuracy: 0.750000\n",
      "Train Accuracy: 0.746950\n",
      "Train Loss: 0.786403\n",
      "Epoch 2: Test Accuracy: 0.812500\n",
      "Train Accuracy: 0.812350\n",
      "Train Loss: 0.571506\n",
      "Epoch 3: Test Accuracy: 0.828125\n",
      "Train Accuracy: 0.825617\n",
      "Train Loss: 0.525999\n",
      "Epoch 4: Test Accuracy: 0.847656\n",
      "Train Accuracy: 0.832233\n",
      "Train Loss: 0.501169\n",
      "Epoch 5: Test Accuracy: 0.835938\n",
      "Train Accuracy: 0.836683\n",
      "Train Loss: 0.485659\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "   <div id=\"UsaL3a\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"metric\":[\"train loss\",\"train accuracy\",\"test accuracy\",\"train loss\",\"train accuracy\",\"test accuracy\",\"train loss\",\"train accuracy\",\"test accuracy\",\"train loss\",\"train accuracy\",\"test accuracy\",\"train loss\",\"train accuracy\",\"test accuracy\"],\n",
       "\"epoch\":[1.0,1.0,1.0,2.0,2.0,2.0,3.0,3.0,3.0,4.0,4.0,4.0,5.0,5.0,5.0],\n",
       "\"value\":[0.7864027619361877,0.7469499707221985,0.75,0.571505606174469,0.8123499751091003,0.8125,0.525999128818512,0.8256166577339172,0.828125,0.5011689066886902,0.8322333097457886,0.84765625,0.48565855622291565,0.8366833329200745,0.8359375]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":500.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"epoch\",\n",
       "\"y\":\"value\",\n",
       "\"color\":\"metric\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"UsaL3a\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numEpochs = 5;\n",
    "val lr = 0.01f;\n",
    "\n",
    "fun updater(params:NDList , lr: Float, batchSize: Int) {\n",
    "        for (param in params) {\n",
    "            param.subi(param.getGradient().mul(lr).div(batchSize));\n",
    "        }\n",
    "}\n",
    "\n",
    "trainCh3(::net, trainingSet, validationSet, ::crossEntropy, numEpochs, ::updater);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Now that training is complete,\n",
    "our model is ready to classify some images.\n",
    "Given a series of images,\n",
    "we will compare their actual labels\n",
    "(first line of text output)\n",
    "and the model predictions\n",
    "(second line of text output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\"/>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.awt.image.BufferedImage\n",
    "// Saved in the FashionMnistUtils class for later use\n",
    "// Number should be < batchSize for this function to work properly\n",
    "fun  predictCh3(net: (NDArray)->NDArray , dataset: ArrayDataset , number: Int, manager: NDManager ) : BufferedImage {\n",
    "    val predLabels = IntArray(number)\n",
    "    \n",
    "    val batch = dataset.getData(manager).iterator().next()\n",
    "    val X = batch.getData().head()\n",
    "    val yHat = net(X).argMax(1).toType(DataType.INT32, false).toIntArray()\n",
    "    for (i in 0..number-1) {\n",
    "        predLabels[i] = yHat[i]\n",
    "    }\n",
    "    \n",
    "    return FashionMnistUtils.showImages(dataset, predLabels, 28, 28, 4, manager)\n",
    "}\n",
    "\n",
    "predictCh3(::net, validationSet, 6, manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "With softmax regression, we can train models for multi-category classification.\n",
    "The training loop is very similar to that in linear regression:\n",
    "retrieve and read data, define models and loss functions,\n",
    "then train models using optimization algorithms.\n",
    "As you will soon find out, most common deep learning models\n",
    "have similar training procedures.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. In this section, we directly implemented the softmax function based on the mathematical definition of the softmax operation. What problems might this cause (hint: try to calculate the size of $\\exp(50)$)?\n",
    "1. The function `crossEntropy()` in this section is implemented according to the definition of the cross-entropy loss function.  What could be the problem with this implementation (hint: consider the domain of the logarithm)?\n",
    "1. What solutions you can think of to fix the two problems above?\n",
    "1. Is it always a good idea to return the most likely label. E.g., would you do this for medical diagnosis?\n",
    "1. Assume that we want to use softmax regression to predict the next word based on some features. What are some problems that might arise from a large vocabulary?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
