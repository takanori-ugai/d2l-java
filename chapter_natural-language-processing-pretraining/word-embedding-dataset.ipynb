{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# The Dataset for Pretraining Word Embedding\n",
    ":label:`sec_word2vec_data`\n",
    "\n",
    "In this section, we will introduce how to preprocess a dataset with\n",
    "negative sampling :numref:`sec_approx_train` and load into minibatches for\n",
    "word2vec training. The dataset we use is [Penn Tree Bank (PTB)]( https://catalog.ldc.upenn.edu/LDC99T42), which is a small but commonly-used corpus. It takes samples from Wall Street Journal articles and includes training sets, validation sets, and test sets.\n",
    "\n",
    "First, import the packages and modules required for the experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"KkFTLE\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "       if(!window.letsPlotCallQueue) {\n",
       "           window.letsPlotCallQueue = [];\n",
       "       }; \n",
       "       window.letsPlotCall = function(f) {\n",
       "           window.letsPlotCallQueue.push(f);\n",
       "       };\n",
       "       (function() {\n",
       "           var script = document.createElement(\"script\");\n",
       "           script.type = \"text/javascript\";\n",
       "           script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.4.0/js-package/distr/lets-plot.min.js\";\n",
       "           script.onload = function() {\n",
       "               window.letsPlotCall = function(f) {f();};\n",
       "               window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "               window.letsPlotCallQueue = [];\n",
       "               \n",
       "               \n",
       "           };\n",
       "           script.onerror = function(event) {\n",
       "               window.letsPlotCall = function(f) {};\n",
       "               window.letsPlotCallQueue = [];\n",
       "               var div = document.createElement(\"div\");\n",
       "               div.style.color = 'darkred';\n",
       "               div.textContent = 'Error loading Lets-Plot JS';\n",
       "               document.getElementById(\"KkFTLE\").appendChild(div);\n",
       "           };\n",
       "           var e = document.getElementById(\"KkFTLE\");\n",
       "           e.appendChild(script);\n",
       "       })();\n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%use @file[../djl.json]\n",
    "%use lets-plot\n",
    "@file:DependsOn(\"org.apache.commons:commons-math3:3.6.1\")\n",
    "@file:DependsOn(\"../D2J-1.0-SNAPSHOT.jar\")\n",
    "//import jp.live.ugai.d2j.attention.Chap10Utils\n",
    "import org.apache.commons.math3.distribution.EnumeratedDistribution\n",
    "import jp.live.ugai.d2j.util.ImageUtils\n",
    "import jp.live.ugai.d2j.util.StopWatch\n",
    "import jp.live.ugai.d2j.util.Training\n",
    "import jp.live.ugai.d2j.timemachine.Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val manager = NDManager.newBaseManager();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 3
   },
   "source": [
    "## Reading and Preprocessing the Dataset\n",
    "\n",
    "This dataset has already been preprocessed. Each line of the dataset acts as a sentence. All the words in a sentence are separated by spaces. In the word embedding task, each word is a token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun readPTB() : List<List<String>> {\n",
    "    val ptbURL = \"http://d2l-data.s3-accelerate.amazonaws.com/ptb.zip\";\n",
    "    val input = URL(ptbURL).openStream();\n",
    "    ZipUtils.unzip(input, Paths.get(\"./\"));\n",
    "\n",
    "    val lines = mutableListOf<String>();\n",
    "   val file = File(\"./ptb/ptb.train.txt\");\n",
    "    val myReader = java.util.Scanner(file);\n",
    "    while (myReader.hasNextLine()) {\n",
    "        lines.add(myReader.nextLine());\n",
    "    }\n",
    "    val tokens = mutableListOf<List<String>>()\n",
    "    for (i in 0 until lines.size) {\n",
    "        tokens.add(lines.get(i).trim().split(\" \"))\n",
    "    }\n",
    "    return tokens;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences: 42068\n"
     ]
    }
   ],
   "source": [
    "val sentences = readPTB();\n",
    "println(\"# sentences: \" + sentences.size);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "Next we build a vocabulary with words appeared not greater than 10 times mapped into a \"&lt;unk&gt;\" token. Note that the preprocessed PTB data also contains \"&lt;unk&gt;\" tokens presenting rare words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6719\n"
     ]
    }
   ],
   "source": [
    "val vocab = Vocab(sentences, 10, listOf<String>())\n",
    "println(vocab.length());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 7
   },
   "source": [
    "## Subsampling\n",
    "\n",
    "In text data, there are generally some words that appear at high frequencies, such \"the\", \"a\", and \"in\" in English. Generally speaking, in a context window, it is better to train the word embedding model when a word (such as \"chip\") and a lower-frequency word (such as \"microprocessor\") appear at the same time, rather than when a word appears with a higher-frequency word (such as \"the\"). Therefore, when training the word embedding model, we can perform subsampling on the words :cite:`Mikolov.Sutskever.Chen.ea.2013`. Specifically, each indexed word $w_i$ in the dataset will drop out at a certain probability. The dropout probability is given as:\n",
    "\n",
    "$$ P(w_i) = \\max\\left(1 - \\sqrt{\\frac{t}{f(w_i)}}, 0\\right),$$\n",
    "\n",
    "Here, $f(w_i)$ is the ratio of the instances of word $w_i$ to the total number of words in the dataset, and the constant $t$ is a hyperparameter (set to $10^{-4}$ in this experiment). As we can see, it is only possible to drop out the word $w_i$ in subsampling when $f(w_i) > t$. The higher the word's frequency, the higher its dropout probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun keep(token: String, counter: Map<Any, Double>, numTokens: Int) : Boolean {\n",
    "    // Return True if to keep this token during subsampling\n",
    "    return kotlin.random.Random.nextFloat() < Math.sqrt(1e-4 / counter.get(token)!! * numTokens)\n",
    "}\n",
    "\n",
    "fun subSampling(sentences: List<List<String>>, vocab: Vocab) : List<List<String>> {\n",
    "    val tempSentences = mutableListOf<List<String>>()\n",
    "    for (i in 0 until sentences.size) {\n",
    "        val tmp = mutableListOf<String>()\n",
    "        for (j in 0 until sentences[i].size) {\n",
    "            tmp.add(vocab.idxToToken.get(vocab.getIdx(sentences[i][j])))\n",
    "        }\n",
    "        tempSentences.add(tmp)\n",
    "    }\n",
    "    // Count the frequency for each word\n",
    "    val counter = Vocab.countCorpus2D(sentences);\n",
    "    var numTokens : Int = 0\n",
    "    for (value in counter.values) {\n",
    "        numTokens += value\n",
    "    }\n",
    "\n",
    "    // Now do the subsampling\n",
    "    val output = mutableListOf<List<String>>()\n",
    "    for (i in 0 until tempSentences.size) {\n",
    "        val tks = mutableListOf<String>()\n",
    "        for (j in 0 until tempSentences[i].size) {\n",
    "            val tk = tempSentences[i][j];\n",
    "            if (keep(tempSentences[i][j], counter.map { it.key to it.value.toDouble() }.toMap() , numTokens)) {\n",
    "                tks.add(tk);\n",
    "            }\n",
    "        }\n",
    "        output.add(tks)\n",
    "    }\n",
    "\n",
    "    return output\n",
    "}\n",
    "\n",
    "val subsampled = subSampling(sentences, vocab);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "Compare the sequence lengths before and after sampling, we can see subsampling significantly reduced the sequence length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"cmd2Bs\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"fill\":\"cond\"\n",
       "},\n",
       "\"data\":{\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":250.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "},\n",
       "\"stat\":\"bin\",\n",
       "\"position\":\"stack\",\n",
       "\"binwidth\":5.0,\n",
       "\"geom\":\"histogram\",\n",
       "\"data\":{\n",
       "\"..count..\":[137.0,2159.0,4987.0,7674.0,8175.0,7342.0,5405.0,3076.0,1692.0,800.0,339.0,142.0,81.0,22.0,16.0,12.0,8.0,1.0,1356.0,14518.0,16523.0,7433.0,1868.0,296.0,57.0,14.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n",
       "\"x\":[-1.0,4.0,9.0,14.0,19.0,24.0,29.0,34.0,39.0,44.0,49.0,54.0,59.0,64.0,69.0,74.0,79.0,84.0,-1.0,4.0,9.0,14.0,19.0,24.0,29.0,34.0,39.0,44.0,49.0,54.0,59.0,64.0,69.0,74.0,79.0,84.0],\n",
       "\"cond\":[\"body\",\"body\",\"body\",\"body\",\"body\",\"body\",\"body\",\"body\",\"body\",\"body\",\"body\",\"body\",\"body\",\"body\",\"body\",\"body\",\"body\",\"body\",\"subsampled\",\"subsampled\",\"subsampled\",\"subsampled\",\"subsampled\",\"subsampled\",\"subsampled\",\"subsampled\",\"subsampled\",\"subsampled\",\"subsampled\",\"subsampled\",\"subsampled\",\"subsampled\",\"subsampled\",\"subsampled\",\"subsampled\",\"subsampled\"]\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"cmd2Bs\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val y1 = sentences.map { it.size }\n",
    "val y2 = subsampled.map { it.size }\n",
    "\n",
    "val label = List<String>(sentences.size){\"body\"}\n",
    "    val label2 = List<String>(subsampled.size){\"subsampled\"}\n",
    "val data = mapOf(\"x\" to y1 + y2, \"cond\" to label + label2)\n",
    "var plot = letsPlot(data){x = \"x\"; fill=\"cond\"} \n",
    "plot += geomHistogram(binWidth=5)\n",
    "plot + ggsize(500, 250)\n",
    "//println(y1)\n",
    "// HistogramTrace trace1 =\n",
    "//        HistogramTrace.builder(y1).opacity(.75).name(\"origin\").nBinsX(20).build();\n",
    "// HistogramTrace trace2 =\n",
    "//         HistogramTrace.builder(y2).opacity(.75).name(\"subsampled\").nBinsX(20).build();\n",
    "\n",
    "// Layout layout =\n",
    "//        Layout.builder()\n",
    "//                .barMode(Layout.BarMode.GROUP)\n",
    "//                .showLegend(true)\n",
    "//                .xAxis(Axis.builder().title(\"# tokens per sentence\").build())\n",
    "//                .yAxis(Axis.builder().title(\"count\").build())\n",
    "//                .build();\n",
    "//new Figure(layout, trace1, trace2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "For individual tokens, the sampling rate of the high-frequency word \"the\" is less than 1/20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of \"the\": before=50770, after=2128\n"
     ]
    }
   ],
   "source": [
    "fun compareCounts(token: String, sentences: List<List<String>>, subsampled: List<List<String>>): String {\n",
    "    var beforeCount = 0;\n",
    "    for (i in 0 until sentences.size) {\n",
    "            beforeCount += sentences[i].count{ it.equals(token) } \n",
    "    }\n",
    "\n",
    "    var afterCount = 0;\n",
    "    for (i in 0 until subsampled.size) {\n",
    "        afterCount += subsampled[i].count { it.equals(token) }\n",
    "    }\n",
    "\n",
    "    return \"# of \\\"the\\\": before=\" + beforeCount + \", after=\" + afterCount;\n",
    "}\n",
    "\n",
    "println(compareCounts(\"the\", sentences, subsampled));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "But the low-frequency word \"join\" is completely preserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of \"the\": before=45, after=45\n"
     ]
    }
   ],
   "source": [
    "println(compareCounts(\"join\", sentences, subsampled));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "Last, we map each token into an index to construct the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[2115, 145]\n",
      "[5277, 3054, 1580]\n"
     ]
    }
   ],
   "source": [
    "val corpus = mutableListOf<List<Int>>()\n",
    "for (i in 0 until subsampled.size) {\n",
    "    corpus.add(vocab.getIdxs(subsampled[i]))\n",
    "}\n",
    "for (i in 0 until 3) {\n",
    "    println(corpus[i])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "Next we read the corpus with token indicies into data batches for training.\n",
    "\n",
    "### Extracting Central Target Words and Context Words\n",
    "\n",
    "We use words with a distance from the central target word not exceeding the context window size as the context words of the given center target word. The following definition function extracts all the central target words and their context words. It uniformly and randomly samples an integer to be used as the context window size between integer 1 and the `maxWindowSize` (maximum context window).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun  getCentersAndContext(\n",
    "        corpus: List<List<Int>>, maxWindowSize: Int): kotlin.Pair<List<Int>, List<List<Int>>> {\n",
    "    var centers = mutableListOf<Int>()\n",
    "    val contexts = mutableListOf<List<Int>>()\n",
    "\n",
    "    for (line in corpus) {\n",
    "        // Each sentence needs at least 2 words to form a \"central target word\n",
    "        // - context word\" pair\n",
    "        if (line.size < 2) {\n",
    "            continue;\n",
    "        }\n",
    "        centers.addAll(line)\n",
    "        for (i in 0 until line.size) { // Context window centered at i\n",
    "            val windowSize = kotlin.random.Random.nextInt(maxWindowSize - 1) + 1;\n",
    "            val indices =\n",
    "                    (Math.max(0, i - windowSize) until Math.min(line.size, i + 1 + windowSize)).toMutableList()\n",
    "            // Exclude the central target word from the context words\n",
    "            indices.remove(indices.indexOf(i))\n",
    "            val context = indices.map { line[it] }\n",
    "            contexts.add(context);\n",
    "        }\n",
    "    }\n",
    "    return kotlin.Pair(centers, contexts);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "Next, we create an artificial dataset containing two sentences of 7 and 3 words, respectively. Assume the maximum context window is 2 and print all the central target words and their context words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [[1], [0, 2], [2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6], [8], [7, 9], [9]])\n",
      "Center 0 has contexts[1]\n",
      "Center 1 has contexts[0, 2]\n",
      "Center 2 has contexts[2, 3]\n",
      "Center 3 has contexts[2, 3, 4]\n",
      "Center 4 has contexts[3, 4, 5]\n",
      "Center 5 has contexts[4, 5, 6]\n",
      "Center 6 has contexts[5, 6]\n",
      "Center 7 has contexts[8]\n",
      "Center 8 has contexts[7, 9]\n",
      "Center 9 has contexts[9]\n"
     ]
    }
   ],
   "source": [
    "val tinyDataset =\n",
    "        listOf<List<Int>>(\n",
    "            (0 until 7).toList(),\n",
    "            (7 until 10).toList())\n",
    "\n",
    "println(\"dataset \" +tinyDataset)\n",
    "var centerContextPair =\n",
    "        getCentersAndContext(tinyDataset, 2);\n",
    "println(centerContextPair)\n",
    "for (i in 0 until centerContextPair.second.size) {\n",
    "    println(\n",
    "            \"Center \"\n",
    "                    + centerContextPair.first.get(i)\n",
    "                    + \" has contexts\"\n",
    "                    + centerContextPair.second.get(i))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "We set the maximum context window size to 5. The following extracts all the central target words and their context words in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# center-context pairs:353384\n"
     ]
    }
   ],
   "source": [
    "centerContextPair = getCentersAndContext(corpus, 5);\n",
    "val allCenters = centerContextPair.first\n",
    "val allContexts = centerContextPair.second\n",
    "println(\"# center-context pairs:\" + allCenters.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 23
   },
   "source": [
    "### Negative Sampling\n",
    "\n",
    "We use negative sampling for approximate training. For a central and context word pair, we randomly sample $K$ noise words ($K=5$ in the experiment). According to the suggestion in the Word2vec paper, the noise word sampling probability $P(w)$ is the ratio of the word frequency of $w$ to the total word frequency raised to the power of 0.75 :cite:`Mikolov.Sutskever.Chen.ea.2013`.\n",
    "\n",
    "We first define a class to draw a candidate according to the sampling weights. It caches a 10,000 size random number bank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 0, 2, 0, 1, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "class RandomGenerator(samplingWeights: List<Double> ) {\n",
    "    /* Draw a random int in [0, n] according to n sampling weights. */\n",
    "\n",
    "    private val population: kotlin.collections.List<Int>\n",
    "    private val samplingWeights: kotlin.collections.List<Double> \n",
    "    private var candidates: MutableList<Int> \n",
    "    private val pmf: MutableList<org.apache.commons.math3.util.Pair<Int, Double>>\n",
    "    private var i: Int\n",
    "\n",
    "    init {\n",
    "        this.population =\n",
    "                (0 until samplingWeights.size).toList()\n",
    "        this.samplingWeights = samplingWeights\n",
    "        this.candidates = mutableListOf()\n",
    "        this.i = 0;\n",
    "\n",
    "        this.pmf = mutableListOf()\n",
    "        for (i in 0 until samplingWeights.size) {\n",
    "            this.pmf.add(org.apache.commons.math3.util.Pair(this.population.get(i), this.samplingWeights.get(i)));\n",
    "        }\n",
    "    }\n",
    "\n",
    "    fun draw(): Int {\n",
    "        if (this.i == this.candidates.size) {\n",
    "            this.candidates =\n",
    "                    EnumeratedDistribution(this.pmf).sample(10000, arrayOf<Int>()).toMutableList()\n",
    "            this.i = 0;\n",
    "        }\n",
    "        this.i += 1;\n",
    "        return this.candidates.get(this.i - 1);\n",
    "    }\n",
    "}\n",
    "\n",
    "val generator =\n",
    "        RandomGenerator(listOf(2.0, 3.0, 4.0))\n",
    "val generatorOutput = List(10) { generator.draw() }\n",
    "println(generatorOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun getNegatives(allContexts: List<List<Int>>, corpus: List<List<Int>>, K: Int): List<List<Int>> {\n",
    "    val counter = Vocab.countCorpus2D(corpus)\n",
    "    val samplingWeights = mutableListOf<Double>()\n",
    "    for (entry in counter) {\n",
    "        samplingWeights.add(Math.pow(entry.value.toDouble(), 0.75));\n",
    "    }\n",
    "    val allNegatives = mutableListOf<List<Int>>()\n",
    "    val generator = RandomGenerator(samplingWeights)\n",
    "    for (contexts in allContexts) {\n",
    "        val negatives = mutableListOf<Int>()\n",
    "        while (negatives.size < contexts.size * K) {\n",
    "            val neg = generator.draw();\n",
    "            // Noise words cannot be context words\n",
    "            if (!contexts.contains(neg)) {\n",
    "                negatives.add(neg);\n",
    "            }\n",
    "        }\n",
    "        allNegatives.add(negatives);\n",
    "    }\n",
    "    return allNegatives;\n",
    "}\n",
    "\n",
    "val allNegatives = getNegatives(allContexts, corpus, 5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "### Reading into Batches\n",
    "\n",
    "We extract all central target words `allCenters`, and the context words `allContexts` and noise words `allNegatives` of each central target word from the dataset. We will read them in random minibatches.\n",
    "\n",
    "In a minibatch of data, the $i^\\mathrm{th}$ example includes a central word and its corresponding $n_i$ context words and $m_i$ noise words. Since the context window size of each example may be different, the sum of context words and noise words, $n_i+m_i$, will be different. When constructing a minibatch, we concatenate the context words and noise words of each example, and add 0s for padding until the length of the concatenations are the same, that is, the length of all concatenations is $\\max_i n_i+m_i$(`maxLen`). In order to avoid the effect of padding on the loss function calculation, we construct the mask variable `masks`, each element of which corresponds to an element in the concatenation of context and noise words, `contextsNegatives`. When an element in the variable `contextsNegatives` is a padding, the element in the mask variable `masks` at the same position will be 0. Otherwise, it takes the value 1. In order to distinguish between positive and negative examples, we also need to distinguish the context words from the noise words in the `contextsNegatives` variable. Based on the construction of the mask variable, we only need to create a label variable `labels` with the same shape as the `contextsNegatives` variable and set the elements corresponding to context words (positive examples) to 1, and the rest to 0.\n",
    "\n",
    "Next, we will implement the minibatch reading function `batchifyData`. Its minibatch input `data` is a list of `NDArrays`, each element of which contains central target words `center`, context words `context`, and noise words `negative`. The minibatch data returned by this function conforms to the format we need, for example, it includes the mask variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun batchifyData(data: List<NDList>) : NDList{\n",
    "    val centers = NDList();\n",
    "    val contextsNegatives = NDList();\n",
    "    val masks = NDList();\n",
    "    val labels = NDList();\n",
    "\n",
    "    var maxLen = 0L\n",
    "    for (ndList in data) { // center, context, negative = ndList\n",
    "        maxLen =\n",
    "                Math.max(\n",
    "                        maxLen,\n",
    "                        ndList.get(1).countNonzero().getLong()\n",
    "                                + ndList.get(2).countNonzero().getLong());\n",
    "    }\n",
    "    for (ndList in data) { // center, context, negative = ndList\n",
    "        val center = ndList.get(0);\n",
    "        val context = ndList.get(1);\n",
    "        val negative = ndList.get(2);\n",
    "\n",
    "        var count = 0L;\n",
    "        for (i in 0 until context.size()) {\n",
    "            // If a 0 is found, we want to stop adding these\n",
    "            // values to NDArray\n",
    "            if (context.get(i).getInt() == 0) {\n",
    "                break;\n",
    "            }\n",
    "            contextsNegatives.add(context.get(i).reshape(1));\n",
    "            masks.add(manager.create(1).reshape(1));\n",
    "            labels.add(manager.create(1).reshape(1));\n",
    "            count += 1;\n",
    "        }\n",
    "        for (i in 0 until negative.size()) {\n",
    "            // If a 0 is found, we want to stop adding these\n",
    "            // values to NDArray\n",
    "            if (negative.get(i).getInt() == 0) {\n",
    "                break;\n",
    "            }\n",
    "            contextsNegatives.add(negative.get(i).reshape(1));\n",
    "            masks.add(manager.create(1).reshape(1));\n",
    "            labels.add(manager.create(0).reshape(1));\n",
    "            count += 1;\n",
    "        }\n",
    "        // Fill with zeroes remaining array\n",
    "        while (count != maxLen) {\n",
    "            contextsNegatives.add(manager.create(0).reshape(1));\n",
    "            masks.add(manager.create(0).reshape(1));\n",
    "            labels.add(manager.create(0).reshape(1));\n",
    "            count += 1;\n",
    "        }\n",
    "\n",
    "        // Add this NDArrays to output NDArrays\n",
    "        centers.add(center.reshape(1));\n",
    "    }\n",
    "    return NDList(\n",
    "            NDArrays.concat(centers).reshape(data.size.toLong(), -1),\n",
    "            NDArrays.concat(contextsNegatives).reshape(data.size.toLong(), -1),\n",
    "            NDArrays.concat(masks).reshape(data.size.toLong(), -1),\n",
    "            NDArrays.concat(labels).reshape(data.size.toLong(), -1));\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "Construct two simple examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: ND: (2, 1) cpu() int32\n",
      "[[ 1],\n",
      " [ 1],\n",
      "]\n",
      "\n",
      "contexts_negatives shape: ND: (2, 6) cpu() int32\n",
      "[[ 2,  2,  3,  3,  3,  3],\n",
      " [ 2,  2,  2,  3,  3,  0],\n",
      "]\n",
      "\n",
      "masks shape: ND: (2, 6) cpu() int32\n",
      "[[ 1,  1,  1,  1,  1,  1],\n",
      " [ 1,  1,  1,  1,  1,  0],\n",
      "]\n",
      "\n",
      "labels shape: ND: (2, 6) cpu() int32\n",
      "[[ 1,  1,  0,  0,  0,  0],\n",
      " [ 1,  1,  1,  0,  0,  0],\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val x1 =\n",
    "        NDList(\n",
    "                manager.create(intArrayOf(1)),\n",
    "                manager.create(intArrayOf(2, 2)),\n",
    "                manager.create(intArrayOf(3, 3, 3, 3)))\n",
    "val x2 =\n",
    "        NDList(\n",
    "                manager.create(intArrayOf(1)),\n",
    "                manager.create(intArrayOf(2, 2, 2)),\n",
    "                manager.create(intArrayOf(3, 3)))\n",
    "\n",
    "val batchedData = batchifyData(listOf<NDList>(x1, x2));\n",
    "val names = listOf(\"centers\", \"contexts_negatives\", \"masks\", \"labels\")\n",
    "for (i in 0 until batchedData.size) {\n",
    "    println(names[i] + \" shape: \" + batchedData.get(i));\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "We use the `batchifyData` function just defined to specify the minibatch reading method for the `ArrayDataset` instance iterator.\n",
    "\n",
    "## Putting All Things Together\n",
    "\n",
    "Last, we define the `loadDataPTB` function that read the PTB dataset and return the dataset. In addition, we will create a function called `convertNDArray` that will convert the `centers`, `contexts`, and `negatives` lists, into `NDArrays` by putting 0s where there is no data in order for the rows to have the same lenghts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun convertNDArray(data: List<List<Any>>, manager: NDManager): NDList {\n",
    "    val centers: MutableList<Int> = (data[0] as List<Int>).toMutableList()\n",
    "    val contexts: List<MutableList<Int>> = (data[1] as List<List<Int>>).map { it.toMutableList() }\n",
    "    val negatives: List<MutableList<Int>> = (data[2] as List<List<Int>>).map { it.toMutableList() }\n",
    "\n",
    "    // Create centers NDArray\n",
    "    val centersNDArray = manager.create(centers.toIntArray());\n",
    "\n",
    "    // Create contexts NDArray\n",
    "    var maxLen = 0;\n",
    "    for (context in contexts) {\n",
    "        maxLen = Math.max(maxLen, context.size)\n",
    "    }\n",
    "    // Fill arrays with 0s to all have same lengths and be able to create NDArray\n",
    "    for (context in contexts) {\n",
    "        while (context.size != maxLen) {\n",
    "            context.add(0);\n",
    "        }\n",
    "    }\n",
    "    val contextsNDArray = manager.create(contexts.map { it.toIntArray() }.toTypedArray());\n",
    "\n",
    "    // Create negatives NDArray\n",
    "    maxLen = 0;\n",
    "    for (negative in negatives) {\n",
    "        maxLen = Math.max(maxLen, negative.size)\n",
    "    }\n",
    "    // Fill arrays with 0s to all have same lengths and be able to create NDArray\n",
    "    for (negative in negatives) {\n",
    "        while (negative.size != maxLen) {\n",
    "            negative.add(0);\n",
    "        }\n",
    "    }\n",
    "    val negativesNDArray =\n",
    "            manager.create(\n",
    "                    negatives.map { it.toIntArray() }.toTypedArray());\n",
    "\n",
    "    return NDList(centersNDArray, contextsNDArray, negativesNDArray);\n",
    "}\n",
    "\n",
    "    fun loadDataPTB(\n",
    "        batchSize: Int,\n",
    "        maxWindowSize: Int,\n",
    "        numNoiseWords: Int,\n",
    "        manager: NDManager\n",
    "    ): Pair<ArrayDataset, Vocab> {\n",
    "        val sentences = readPTB()\n",
    "        val vocab = Vocab(sentences, 10, listOf<String>())\n",
    "        val subSampled = subSampling(sentences, vocab)\n",
    "        val corpus = mutableListOf<List<Int>>()\n",
    "        for (i in 0 until subSampled.size) {\n",
    "            corpus.add(vocab.getIdxs(subSampled[i]))\n",
    "        }\n",
    "        val pair = getCentersAndContext(corpus, maxWindowSize)\n",
    "        val negatives = getNegatives(pair.second, corpus, numNoiseWords)\n",
    "\n",
    "        val ndArrays =\n",
    "            convertNDArray(listOf(pair.first, pair.second, negatives), manager)\n",
    "        val dataset =\n",
    "            ArrayDataset.Builder()\n",
    "                .setData(ndArrays.get(0), ndArrays.get(1), ndArrays.get(2))\n",
    "                .optDataBatchifier(\n",
    "                    object : Batchifier {\n",
    "                        override fun batchify(ndLists: Array<NDList>): NDList {\n",
    "                            return batchifyData(ndLists.toList())\n",
    "                        }\n",
    "\n",
    "                        override fun unbatchify(ndList: NDList): Array<NDList> {\n",
    "                            return arrayOf<NDList>()\n",
    "                        }\n",
    "                    })\n",
    "                .setSampling(batchSize, true)\n",
    "                .build()\n",
    "\n",
    "        return Pair(dataset, vocab)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 33
   },
   "source": [
    "Let us print the first minibatch of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: (512, 1)\n",
      "contexts_negatives shape: (512, 54)\n",
      "masks shape: (512, 54)\n",
      "labels shape: (512, 54)\n"
     ]
    }
   ],
   "source": [
    "val datasetVocab = loadDataPTB(512, 5, 5, manager);\n",
    "val dataset = datasetVocab.getKey();\n",
    "val vocab = datasetVocab.getValue();\n",
    "\n",
    "val batch = dataset.getData(manager).iterator().next();\n",
    "for (i in 0 until batch.getData().size) {\n",
    "    println(names[i] + \" shape: \" + batch.getData().get(i).getShape());\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 35
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Subsampling attempts to minimize the impact of high-frequency words on the training of a word embedding model.\n",
    "* We can pad examples of different lengths to create minibatches with examples of all the same length and use mask variables to distinguish between padding and non-padding elements, so that only non-padding elements participate in the calculation of the loss function.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. We use the `batchifyData` function to specify the minibatch reading method for the `ArrayDataset` instance iterator and print the shape of each variable in the first batch read. How should these shapes be calculated?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
