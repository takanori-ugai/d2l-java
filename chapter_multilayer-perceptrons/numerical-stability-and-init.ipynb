{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Stability and Initialization\n",
    "\n",
    ":label:`sec_numerical_stability`\n",
    "\n",
    "\n",
    "\n",
    "Thus far, every model that we have implemented\n",
    "required that we initialize its parameters \n",
    "according to some pre-specified distribution.\n",
    "Until now, we took the initialization scheme for granted,\n",
    "glossing over the details of how these these choices are made.\n",
    "You might have even gotten the impression that these choices\n",
    "are not especially important.\n",
    "To the contrary, the choice of initialization scheme\n",
    "plays a significant role in neural network learning,\n",
    "and it can be crucial for maintaining numerical stability.\n",
    "Moreover, these choices can be tied up in interesting ways\n",
    "with the choice of the nonlinear activation function.\n",
    "Which function we choose and how we initialize parameters\n",
    "can determine how quickly our optimization algorithm converges.\n",
    "Poor choices here can cause us to encounter\n",
    "exploding or vanishing gradients while training.\n",
    "In this section, we delve into these topics with greater detail\n",
    "and discuss some useful heuristics\n",
    "that you will find useful\n",
    "throughout your career in deep learning.\n",
    "\n",
    "\n",
    "## Vanishing and Exploding Gradients\n",
    "\n",
    "Consider a deep network with $L$ layers,\n",
    "input $\\mathbf{x}$ and output $\\mathbf{o}$.\n",
    "With each layer $l$ defined by a transformation $f_l$\n",
    "parameterized by weights $\\mathbf{W}_l$\n",
    "our network can be expressed as:\n",
    "\n",
    "$$\\mathbf{h}^{l+1} = f_l (\\mathbf{h}^l) \\text{ and thus } \\mathbf{o} = f_L \\circ \\ldots, \\circ f_1(\\mathbf{x}).$$\n",
    "\n",
    "If all activations and inputs are vectors,\n",
    "we can write the gradient of $\\mathbf{o}$ with respect to \n",
    "any set of parameters $\\mathbf{W}_l$ as follows:\n",
    "\n",
    "$$\\partial_{\\mathbf{W}_l} \\mathbf{o} = \\underbrace{\\partial_{\\mathbf{h}^{L-1}} \\mathbf{h}^L}_{:= \\mathbf{M}_L} \\cdot \\ldots, \\cdot \\underbrace{\\partial_{\\mathbf{h}^{l}} \\mathbf{h}^{l+1}}_{:= \\mathbf{M}_l} \\underbrace{\\partial_{\\mathbf{W}_l} \\mathbf{h}^l}_{:= \\mathbf{v}_l}.$$\n",
    "\n",
    "In other words, this gradient is\n",
    "the product of $L-l$ matrices\n",
    "$\\mathbf{M}_L \\cdot \\ldots, \\cdot \\mathbf{M}_l$\n",
    "and the gradient vector $\\mathbf{v}_l$.\n",
    "Thus we are susceptible to the same \n",
    "problems of numerical underflow that often crop up \n",
    "when multiplying together too many probabilities.\n",
    "When dealing with probabilities, a common trick is to\n",
    "switch into log-space, i.e., shifting \n",
    "pressure from the mantissa to the exponent \n",
    "of the numerical representation. \n",
    "Unfortunately, our problem above is more serious:\n",
    "initially the matrices $M_l$ may have a wide variety of eigenvalues.\n",
    "They might be small or large, and \n",
    "their product might be *very large* or *very small*.\n",
    "\n",
    "The risks posed by unstable gradients \n",
    "go beyond numerical representation.\n",
    "Gradients of unpredictable magnitude \n",
    "also threaten the stability of our optimization algorithms.\n",
    "We may be facing parameter updates that are either\n",
    "(i) excessively large, destroying our model\n",
    "(the *exploding* gradient problem);\n",
    "or (ii) excessively small \n",
    "(the *vanishing gradient problem*),\n",
    "rendering learning impossible as parameters \n",
    "hardly move on each update.\n",
    "\n",
    "\n",
    "### Vanishing Gradients\n",
    "\n",
    "One frequent culprit causing the vanishing gradient problem\n",
    "is the choice of the activation function $\\sigma$\n",
    "that is appended following each layer's linear operations.\n",
    "Historically, the sigmoid function \n",
    "$1/(1 + \\exp(-x))$ (introduced in :numref:`sec_mlp`)\n",
    "was popular because it resembles a thresholding function.\n",
    "Since early artificial neural networks were inspired\n",
    "by biological neural networks,\n",
    "the idea of neurons that fire either *fully* or *not at all*\n",
    "(like biological neurons) seemed appealing.\n",
    "Let us take a closer look at the sigmoid\n",
    "to see why it can cause vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"EF6Smh\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "       if(!window.letsPlotCallQueue) {\n",
       "           window.letsPlotCallQueue = [];\n",
       "       }; \n",
       "       window.letsPlotCall = function(f) {\n",
       "           window.letsPlotCallQueue.push(f);\n",
       "       };\n",
       "       (function() {\n",
       "           var script = document.createElement(\"script\");\n",
       "           script.type = \"text/javascript\";\n",
       "           script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.4.0/js-package/distr/lets-plot.min.js\";\n",
       "           script.onload = function() {\n",
       "               window.letsPlotCall = function(f) {f();};\n",
       "               window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "               window.letsPlotCallQueue = [];\n",
       "               \n",
       "               \n",
       "           };\n",
       "           script.onerror = function(event) {\n",
       "               window.letsPlotCall = function(f) {};\n",
       "               window.letsPlotCallQueue = [];\n",
       "               var div = document.createElement(\"div\");\n",
       "               div.style.color = 'darkred';\n",
       "               div.textContent = 'Error loading Lets-Plot JS';\n",
       "               document.getElementById(\"EF6Smh\").appendChild(div);\n",
       "           };\n",
       "           var e = document.getElementById(\"EF6Smh\");\n",
       "           e.appendChild(script);\n",
       "       })();\n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%use @file[../djl-pytorch.json]\n",
    "%use lets-plot\n",
    "@file:DependsOn(\"org.apache.commons:commons-lang3:3.12.0\")\n",
    "import ai.djl.metric.Metrics\n",
    "\n",
    "fun getLong(nm: String, n: Long): Long {\n",
    "    val name = System.getProperty(nm)\n",
    "    return if (null == name) n.toLong() else name.toLong()\n",
    "}\n",
    "\n",
    "class Accumulator(n: Int) {\n",
    "    val data = FloatArray(n) { 0f }\n",
    "\n",
    "\n",
    "    /* Adds a set of numbers to the array */\n",
    "    fun add(args: FloatArray) {\n",
    "        for (i in 0..args.size - 1) {\n",
    "            data[i] += args[i]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /* Resets the array */\n",
    "    fun reset() {\n",
    "        data.fill(0f)\n",
    "    }\n",
    "\n",
    "    /* Returns the data point at the given index */\n",
    "    fun get(index: Int): Float {\n",
    "        return data[index]\n",
    "    }\n",
    "}\n",
    "\n",
    "class DataPoints(X:NDArray , y:NDArray ) {\n",
    "    private val X = X\n",
    "    private val y = y\n",
    "\n",
    "    fun  getX() : NDArray{\n",
    "        return X\n",
    "    }\n",
    "    \n",
    "    fun getY() :NDArray {\n",
    "        return y\n",
    "    }\n",
    "}\n",
    "\n",
    "fun syntheticData(manager:NDManager , w: NDArray , b : Float, numExamples: Int) : DataPoints {\n",
    "    val X = manager.randomNormal(Shape(numExamples.toLong(), w.size()))\n",
    "    var y = X.matMul(w).add(b)\n",
    "    // Add noise\n",
    "    y = y.add(manager.randomNormal(0f, 0.01f, y.getShape(), DataType.FLOAT32))\n",
    "    return DataPoints(X, y);\n",
    "}\n",
    "\n",
    "object Training {\n",
    "\n",
    "    fun linreg(X: NDArray, w: NDArray, b: NDArray): NDArray {\n",
    "        return X.dot(w).add(b);\n",
    "    }\n",
    "\n",
    "    fun squaredLoss(yHat: NDArray, y: NDArray): NDArray {\n",
    "        return (yHat.sub(y.reshape(yHat.getShape())))\n",
    "            .mul((yHat.sub(y.reshape(yHat.getShape()))))\n",
    "            .div(2);\n",
    "    }\n",
    "\n",
    "    fun sgd(params: NDList, lr: Float, batchSize: Int) {\n",
    "    val lrt = Tracker.fixed(lr);\n",
    "    val opt = Optimizer.sgd().setLearningRateTracker(lrt).build();\n",
    "        for (param in params) {\n",
    "            // Update param in place.\n",
    "            // param = param - param.gradient * lr / batchSize\n",
    "            // val ind = params.indexOf(param)\n",
    "            // params.rep\n",
    "            // params.set(ind, param.sub(param.getGradient().mul(lr).div(batchSize)))\n",
    "            opt.update(param.toString(), param, param.getGradient().div(batchSize))\n",
    "//            param.subi(param.getGradient().mul(lr).div(batchSize));\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * Allows to do gradient calculations on a subManager. This is very useful when you are training\n",
    "     * on a lot of epochs. This subManager could later be closed and all NDArrays generated from the\n",
    "     * calculations in this function will be cleared from memory when subManager is closed. This is\n",
    "     * always a great practice but the impact is most notable when there is lot of data on various\n",
    "     * epochs.\n",
    "     */\n",
    "    fun sgd(params: NDList, lr: Float, batchSize: Int, subManager: NDManager) {\n",
    "        for (param in params) {\n",
    "            // Update param in place.\n",
    "            // param = param - param.gradient * lr / batchSize\n",
    "            val gradient = param.getGradient()\n",
    "            gradient.attach(subManager);\n",
    "            param.subi(gradient.mul(lr).div(batchSize))\n",
    "        }\n",
    "    }\n",
    "\n",
    "    fun accuracy(yHat: NDArray, y: NDArray): Float {\n",
    "        // Check size of 1st dimension greater than 1\n",
    "        // to see if we have multiple samples\n",
    "        if (yHat.getShape().size(1) > 1) {\n",
    "            // Argmax gets index of maximum args for given axis 1\n",
    "            // Convert yHat to same dataType as y (int32)\n",
    "            // Sum up number of true entries\n",
    "            return yHat.argMax(1)\n",
    "                .toType(DataType.INT32, false)\n",
    "                .eq(y.toType(DataType.INT32, false))\n",
    "                .sum()\n",
    "                .toType(DataType.FLOAT32, false)\n",
    "                .getFloat();\n",
    "        }\n",
    "        return yHat.toType(DataType.INT32, false)\n",
    "            .eq(y.toType(DataType.INT32, false))\n",
    "            .sum()\n",
    "            .toType(DataType.FLOAT32, false)\n",
    "            .getFloat();\n",
    "    }\n",
    "\n",
    "    fun trainingChapter6(\n",
    "        trainIter: ArrayDataset,\n",
    "        testIter: ArrayDataset,\n",
    "        numEpochs: Int,\n",
    "        trainer: Trainer,\n",
    "        evaluatorMetrics: MutableMap<String, DoubleArray>\n",
    "    ): Double {\n",
    "\n",
    "        trainer.setMetrics(Metrics())\n",
    "\n",
    "        EasyTrain.fit(trainer, numEpochs, trainIter, testIter)\n",
    "\n",
    "        val metrics = trainer.getMetrics()\n",
    "\n",
    "        trainer.getEvaluators()\n",
    "            .forEach { evaluator ->\n",
    "                {\n",
    "                    evaluatorMetrics.put(\n",
    "                        \"train_epoch_\" + evaluator.getName(),\n",
    "                        metrics.getMetric(\"train_epoch_\" + evaluator.getName()).stream()\n",
    "                            .mapToDouble { x -> x.getValue() }\n",
    "                            .toArray())\n",
    "                    evaluatorMetrics.put(\n",
    "                        \"validate_epoch_\" + evaluator.getName(),\n",
    "                        metrics\n",
    "                            .getMetric(\"validate_epoch_\" + evaluator.getName())\n",
    "                            .stream()\n",
    "                            .mapToDouble { x -> x.getValue() }\n",
    "                            .toArray())\n",
    "                }\n",
    "            }\n",
    "\n",
    "        return metrics.mean(\"epoch\")\n",
    "    }\n",
    "\n",
    "    /* Softmax-regression-scratch */\n",
    "    fun evaluateAccuracy(net: UnaryOperator<NDArray>, dataIterator: Iterable<Batch>): Float {\n",
    "        val metric = Accumulator(2) // numCorrectedExamples, numExamples\n",
    "        for (batch in dataIterator) {\n",
    "            val X = batch.getData().head()\n",
    "            val y = batch.getLabels().head()\n",
    "            metric.add(floatArrayOf(accuracy(net.apply(X), y), y.size().toFloat()))\n",
    "            batch.close()\n",
    "        }\n",
    "        return metric.get(0) / metric.get(1)\n",
    "    }\n",
    "    /* End Softmax-regression-scratch */\n",
    "\n",
    "    /* MLP */\n",
    "    /* Evaluate the loss of a model on the given dataset */\n",
    "    fun evaluateLoss(\n",
    "        net: UnaryOperator<NDArray>,\n",
    "        dataIterator: Iterable<Batch>,\n",
    "        loss: BinaryOperator<NDArray>\n",
    "    ): Float {\n",
    "        val metric = Accumulator(2) // sumLoss, numExamples\n",
    "\n",
    "        for (batch in dataIterator) {\n",
    "            val X = batch . getData ().head();\n",
    "            val y = batch . getLabels ().head();\n",
    "            metric.add(\n",
    "                floatArrayOf(loss.apply(net.apply(X), y).sum().getFloat(), y.size().toFloat()) )\n",
    "            batch.close()\n",
    "        }\n",
    "        return metric.get(0) / metric.get(1)\n",
    "    }\n",
    "    /* End MLP */\n",
    "}\n",
    "\n",
    "// %load ../utils/djl-imports\n",
    "// %load ../utils/plot-utils\n",
    "// %load ../utils/DataPoints.java\n",
    "// %load ../utils/Training.java\n",
    "// %load ../utils/Accumulator.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.commons.lang3.ArrayUtils;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"FD27DE\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"x\":[-8.0,-7.900000095367432,-7.800000190734863,-7.699999809265137,-7.599999904632568,-7.5,-7.400000095367432,-7.300000190734863,-7.199999809265137,-7.099999904632568,-7.0,-6.899999618530273,-6.799999713897705,-6.699999809265137,-6.599999904632568,-6.5,-6.400000095367432,-6.300000190734863,-6.200000286102295,-6.099999904632568,-6.0,-5.900000095367432,-5.800000190734863,-5.700000286102295,-5.599999904632568,-5.5,-5.400000095367432,-5.299999713897705,-5.199999809265137,-5.099999904632568,-5.0,-4.900000095367432,-4.800000190734863,-4.700000286102295,-4.600000381469727,-4.5,-4.400000095367432,-4.300000190734863,-4.200000286102295,-4.100000381469727,-4.0,-3.9000000953674316,-3.799999952316284,-3.700000047683716,-3.5999999046325684,-3.5,-3.4000000953674316,-3.299999952316284,-3.1999998092651367,-3.0999999046325684,-2.999999761581421,-2.8999998569488525,-2.799999713897705,-2.6999998092651367,-2.5999999046325684,-2.499999761581421,-2.3999998569488525,-2.299999952316284,-2.1999998092651367,-2.0999999046325684,-1.9999998807907104,-1.8999998569488525,-1.7999998331069946,-1.6999998092651367,-1.5999999046325684,-1.4999998807907104,-1.3999998569488525,-1.2999999523162842,-1.1999999284744263,-1.0999999046325684,-0.9999998807907104,-0.8999999165534973,-0.7999998927116394,-0.6999998688697815,-0.5999999046325684,-0.49999988079071045,-0.3999998867511749,-0.2999998927116394,-0.1999998837709427,-0.09999988228082657,1.1920928955078125E-7,0.10000012069940567,0.2000001221895218,0.3000001311302185,0.40000012516975403,0.5000001192092896,0.6000001430511475,0.7000001072883606,0.8000001311302185,0.9000001549720764,1.0000001192092896,1.1000001430511475,1.2000001668930054,1.3000001907348633,1.4000000953674316,1.5000001192092896,1.6000001430511475,1.7000001668930054,1.8000001907348633,1.9000000953674316,2.000000238418579,2.1000001430511475,2.200000047683716,2.3000001907348633,2.4000000953674316,2.5,2.6000001430511475,2.700000047683716,2.8000001907348633,2.9000000953674316,3.0,3.1000001430511475,3.200000286102295,3.3000001907348633,3.4000003337860107,3.500000238418579,3.6000003814697266,3.700000286102295,3.8000001907348633,3.9000003337860107,4.0,4.099999904632568,4.199999809265137,4.300000190734863,4.400000095367432,4.5,4.599999904632568,4.699999809265137,4.800000190734863,4.900000095367432,5.0,5.100000381469727,5.200000286102295,5.300000190734863,5.400000095367432,5.5,5.600000381469727,5.700000286102295,5.800000190734863,5.90000057220459,6.000000476837158,6.100000381469727,6.200000286102295,6.300000190734863,6.400000095367432,6.5,6.599999904632568,6.700000286102295,6.800000190734863,6.900000095367432,7.0,7.099999904632568,7.200000286102295,7.300000190734863,7.400000095367432,7.500000476837158,7.600000381469727,7.700000286102295,7.800000190734863,7.900000095367432,-8.0,-7.900000095367432,-7.800000190734863,-7.699999809265137,-7.599999904632568,-7.5,-7.400000095367432,-7.300000190734863,-7.199999809265137,-7.099999904632568,-7.0,-6.899999618530273,-6.799999713897705,-6.699999809265137,-6.599999904632568,-6.5,-6.400000095367432,-6.300000190734863,-6.200000286102295,-6.099999904632568,-6.0,-5.900000095367432,-5.800000190734863,-5.700000286102295,-5.599999904632568,-5.5,-5.400000095367432,-5.299999713897705,-5.199999809265137,-5.099999904632568,-5.0,-4.900000095367432,-4.800000190734863,-4.700000286102295,-4.600000381469727,-4.5,-4.400000095367432,-4.300000190734863,-4.200000286102295,-4.100000381469727,-4.0,-3.9000000953674316,-3.799999952316284,-3.700000047683716,-3.5999999046325684,-3.5,-3.4000000953674316,-3.299999952316284,-3.1999998092651367,-3.0999999046325684,-2.999999761581421,-2.8999998569488525,-2.799999713897705,-2.6999998092651367,-2.5999999046325684,-2.499999761581421,-2.3999998569488525,-2.299999952316284,-2.1999998092651367,-2.0999999046325684,-1.9999998807907104,-1.8999998569488525,-1.7999998331069946,-1.6999998092651367,-1.5999999046325684,-1.4999998807907104,-1.3999998569488525,-1.2999999523162842,-1.1999999284744263,-1.0999999046325684,-0.9999998807907104,-0.8999999165534973,-0.7999998927116394,-0.6999998688697815,-0.5999999046325684,-0.49999988079071045,-0.3999998867511749,-0.2999998927116394,-0.1999998837709427,-0.09999988228082657,1.1920928955078125E-7,0.10000012069940567,0.2000001221895218,0.3000001311302185,0.40000012516975403,0.5000001192092896,0.6000001430511475,0.7000001072883606,0.8000001311302185,0.9000001549720764,1.0000001192092896,1.1000001430511475,1.2000001668930054,1.3000001907348633,1.4000000953674316,1.5000001192092896,1.6000001430511475,1.7000001668930054,1.8000001907348633,1.9000000953674316,2.000000238418579,2.1000001430511475,2.200000047683716,2.3000001907348633,2.4000000953674316,2.5,2.6000001430511475,2.700000047683716,2.8000001907348633,2.9000000953674316,3.0,3.1000001430511475,3.200000286102295,3.3000001907348633,3.4000003337860107,3.500000238418579,3.6000003814697266,3.700000286102295,3.8000001907348633,3.9000003337860107,4.0,4.099999904632568,4.199999809265137,4.300000190734863,4.400000095367432,4.5,4.599999904632568,4.699999809265137,4.800000190734863,4.900000095367432,5.0,5.100000381469727,5.200000286102295,5.300000190734863,5.400000095367432,5.5,5.600000381469727,5.700000286102295,5.800000190734863,5.90000057220459,6.000000476837158,6.100000381469727,6.200000286102295,6.300000190734863,6.400000095367432,6.5,6.599999904632568,6.700000286102295,6.800000190734863,6.900000095367432,7.0,7.099999904632568,7.200000286102295,7.300000190734863,7.400000095367432,7.500000476837158,7.600000381469727,7.700000286102295,7.800000190734863,7.900000095367432],\n",
       "\"y\":[3.3535013790242374E-4,3.706060815602541E-4,4.095670592505485E-4,4.5262230560183525E-4,5.002011894248426E-4,5.52778597921133E-4,6.108792731538415E-4,6.750825559720397E-4,7.460289634764194E-4,8.244247874245048E-4,9.110511746257544E-4,0.0010067712282761931,0.0011125362943857908,0.0012293988838791847,0.0013585201231762767,0.0015011822106316686,0.0016588008729740977,0.0018329385202378035,0.00202531972900033,0.0022378487046808004,0.0024726230185478926,0.0027319604996591806,0.003018415765836835,0.003334806300699711,0.003684240160509944,0.00407013762742281,0.004496272653341293,0.004966802895069122,0.005486299749463797,0.006059801671653986,0.006692850962281227,0.0073915403336286545,0.00816257018595934,0.009013296104967594,0.009951798245310783,0.010986942797899246,0.012128434143960476,0.013386915437877178,0.014774028211832047,0.01630249433219433,0.01798621006309986,0.0198403038084507,0.02188127115368843,0.024127019569277763,0.026596995070576668,0.02931223064661026,0.03229546174407005,0.03557119145989418,0.03916573151946068,0.04310726001858711,0.047425881028175354,0.0521535687148571,0.05732419341802597,0.06297336518764496,0.06913843005895615,0.07585819810628891,0.08317270874977112,0.09112296253442764,0.09975050389766693,0.10909683257341385,0.11920293420553207,0.13010849058628082,0.14185108244419098,0.15446528792381287,0.16798162460327148,0.18242554366588593,0.19781611859798431,0.21416503190994263,0.23147521913051605,0.24973991513252258,0.2689414322376251,0.28905051946640015,0.31002554297447205,0.3318122625350952,0.3543437123298645,0.3775406777858734,0.40131235122680664,0.4255574941635132,0.4501660168170929,0.4750208556652069,0.5000000596046448,0.5249791741371155,0.5498340725898743,0.5744425654411316,0.5986876487731934,0.622459352016449,0.6456563472747803,0.6681877970695496,0.6899744868278503,0.7109495401382446,0.7310585975646973,0.7502601742744446,0.7685248255729675,0.7858350276947021,0.8021839261054993,0.8175745010375977,0.8320184350013733,0.8455348014831543,0.858148992061615,0.8698915839195251,0.8807971477508545,0.8909031748771667,0.9002495408058167,0.9088770747184753,0.9168273210525513,0.9241418242454529,0.9308615922927856,0.9370266199111938,0.9426758289337158,0.9478464722633362,0.9525741338729858,0.956892728805542,0.960834264755249,0.9644288420677185,0.9677045941352844,0.970687747001648,0.9734029769897461,0.9758729934692383,0.9781186580657959,0.9801597595214844,0.9820137619972229,0.9836974740028381,0.9852259755134583,0.9866130948066711,0.987871527671814,0.9890130758285522,0.9900481700897217,0.9909866452217102,0.9918374419212341,0.9926084876060486,0.9933071732521057,0.9939402341842651,0.994513750076294,0.9950331449508667,0.9955037236213684,0.9959298968315125,0.9963157773017883,0.9966651797294617,0.9969815611839294,0.9972680807113647,0.9975274205207825,0.9977620840072632,0.997974693775177,0.9981670379638672,0.9983412027359009,0.998498797416687,0.9986414313316345,0.998770534992218,0.9988874793052673,0.9989932179450989,0.9990890026092529,0.9991756081581116,0.9992539286613464,0.9993249177932739,0.9993890523910522,0.9994471669197083,0.9994997978210449,0.9995473027229309,0.9995904564857483,0.9996293783187866,3.352376807015389E-4,3.7046874058432877E-4,4.093993338756263E-4,4.524174437392503E-4,4.999510128982365E-4,5.524730077013373E-4,6.105061038397253E-4,6.746268481947482E-4,7.454724400304258E-4,8.237450965680182E-4,9.102211333811283E-4,0.001005757600069046,0.0011112985666841269,0.0012278874637559056,0.0013566745910793543,0.0014989286428317428,0.0016560492804273963,0.0018295787740498781,0.0020212179515510798,0.0022328407503664494,0.0024665091186761856,0.002724496880546212,0.0030093048699200153,0.0033236853778362274,0.0036706665996462107,0.004053571727126837,0.004476055968552828,0.0049421340227127075,0.005456200335174799,0.006023080553859472,0.006648056674748659,0.007336905691772699,0.008095942437648773,0.008932056836783886,0.009852760471403599,0.010866230353713036,0.01198133546859026,0.013207705691456795,0.014555756002664566,0.01603672280907631,0.01766270585358143,0.019446665421128273,0.021402480080723763,0.023544907569885254,0.025889594107866287,0.028453022241592407,0.03125246614217758,0.0343058817088604,0.03763177618384361,0.041249021887779236,0.0451766662299633,0.049433574080467224,0.05403812974691391,0.05900771915912628,0.06435830891132355,0.0701037347316742,0.07625500857830048,0.08281956613063812,0.08980034291744232,0.09719471633434296,0.10499359667301178,0.11318027228116989,0.12172935158014297,0.1306057572364807,0.13976380228996277,0.14914646744728088,0.1586848944425583,0.1682983636856079,0.1778944432735443,0.18736989796161652,0.1966119408607483,0.20550031960010529,0.21390970051288605,0.2217128872871399,0.2287842482328415,0.23500370979309082,0.2402607500553131,0.24445831775665283,0.24751658737659454,0.24937602877616882,0.25,0.24937604367733002,0.24751657247543335,0.24445830285549164,0.2402607500553131,0.23500370979309082,0.2287842333316803,0.2217128574848175,0.21390970051288605,0.2055002897977829,0.1966119259595871,0.18736983835697174,0.17789441347122192,0.16829833388328552,0.1586848795413971,0.1491464376449585,0.1397637575864792,0.13060569763183594,0.1217292994260788,0.11318021267652512,0.10499352961778641,0.09719470888376236,0.08980030566453934,0.08281953632831573,0.07625498622655869,0.0701037123799324,0.06435828655958176,0.059007734060287476,0.05403811112046242,0.04943353682756424,0.04517665505409241,0.04124903306365013,0.03763177990913391,0.03430585190653801,0.0312524139881134,0.028453044593334198,0.025889622047543526,0.02354489453136921,0.021402548998594284,0.019446605816483498,0.017662733793258667,0.016036754474043846,0.014555752277374268,0.013207696378231049,0.011981372721493244,0.010866211727261543,0.009852791205048561,0.008932114578783512,0.008095930330455303,0.007336877752095461,0.0066480329260230064,0.006023045163601637,0.005456150975078344,0.004942185245454311,0.004476059693843126,0.004053537268191576,0.0036706491373479366,0.0033236993476748466,0.003009327920153737,0.002724455902352929,0.002466465812176466,0.0022329078055918217,0.002021204447373748,0.0018296022899448872,0.0016560456715524197,0.0014989490155130625,0.0013567229034379125,0.0012279534712433815,0.001111282967031002,0.001005768426693976,9.101674659177661E-4,8.237122092396021E-4,7.45514698792249E-4,6.746264989487827E-4,6.105743232183158E-4,5.525274318642914E-4,4.99952002428472E-4,4.524923278950155E-4,4.0937578887678683E-4,3.704843111336231E-4],\n",
       "\"label\":[\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\",\"gradient\"]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":500.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"x\",\n",
       "\"y\":\"y\",\n",
       "\"color\":\"label\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"FD27DE\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val manager = NDManager.newBaseManager();\n",
    "val x = manager.arange(-8.0f, 8.0f, 0.1f);\n",
    "x.setRequiresGradient(true);\n",
    "\n",
    "val gc = Engine.getInstance().newGradientCollector()\n",
    "    val y = Activation.sigmoid(x)\n",
    "    gc.backward(y)\n",
    "gc.close()\n",
    "\n",
    "val res = x.getGradient();\n",
    "\n",
    "val xLength = x.size();\n",
    "val yLength =  y.size();\n",
    "\n",
    "\n",
    "val X = x.toFloatArray();\n",
    "val Y = y.toFloatArray();\n",
    "val Z = res.toFloatArray();\n",
    "\n",
    "val group1 = Array<String>(xLength.toInt()) { \"sigmoid\" }\n",
    "val group2 = Array<String>(xLength.toInt()) { \"gradient\" }\n",
    "val data = mapOf(\"y\" to Y + Z, \"x\" to X + X, \"label\" to group1 + group2)\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine { x = \"x\" ; y = \"y\" ; color = \"label\"}\n",
    "plot + ggsize(500, 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the sigmoid's gradient vanishes\n",
    "both when its inputs are large and when they are small.\n",
    "Moreover, when backpropagating through many layers,\n",
    "unless we are in the Goldilocks zone, where \n",
    "the inputs to many of the sigmoids are close to zero, \n",
    "the gradients of the overall product may vanish.\n",
    "When our network boasts many layers,\n",
    "unless we are careful, the gradient \n",
    "will likely be cut off at *some* layer.\n",
    "Indeed, this problem used to plague deep network training.\n",
    "Consequently, ReLUs, which are more stable\n",
    "(but less neurally plausible), \n",
    "have emerged as the default choice for practitioners.\n",
    "\n",
    "\n",
    "### Exploding Gradients\n",
    "\n",
    "The opposite problem, when gradients explode,\n",
    "can be similarly vexing.\n",
    "To illustrate this a bit better,\n",
    "we draw $100$ Gaussian random matrices\n",
    "and multiply them with some initial matrix.\n",
    "For the scale that we picked\n",
    "(the choice of the variance $\\sigma^2=1$),\n",
    "the matrix product explodes.\n",
    "When this happens due to the initialization \n",
    "of a deep network, we have no chance of getting\n",
    "a gradient descent optimizer to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single matrix: ND: (4, 4) cpu() float32\n",
      "[[-0.4311, -0.1772,  0.5272,  0.3125],\n",
      " [ 0.0763, -0.5312,  2.5578, -0.2084],\n",
      " [-0.8436,  0.3063, -0.3737,  0.7595],\n",
      " [ 1.4026,  0.8105, -0.9934,  0.2194],\n",
      "]\n",
      "\n",
      "after multiplying 100 matrices: ND: (4, 4) cpu() float32\n",
      "[[ 2.93911678e+26, -3.84331124e+26,  2.63640774e+26, -3.56634482e+26],\n",
      " [ 1.02257755e+27, -1.33716493e+27,  9.17258889e+26, -1.24080283e+27],\n",
      " [-1.45107715e+25,  1.89748870e+25, -1.30162613e+25,  1.76074714e+25],\n",
      " [-5.92655001e+26,  7.74980259e+26, -5.31615426e+26,  7.19131782e+26],\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var M = manager.randomNormal(Shape(4,4));\n",
    "println(\"A single matrix: \" + M);\n",
    "for(i in 0 until 100){\n",
    "    M = M.dot(manager.randomNormal(Shape(4,4)));\n",
    "}\n",
    "\n",
    "println(\"after multiplying 100 matrices: \" + M);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetry\n",
    "\n",
    "Another problem in deep network design\n",
    "is the symmetry inherent in their parametrization.\n",
    "Assume that we have a deep network\n",
    "with one hidden layer and two units, say $h_1$ and $h_2$.\n",
    "In this case, we could permute the weights $\\mathbf{W}_1$\n",
    "of the first layer and likewise permute \n",
    "the weights of the output layer\n",
    "to obtain the same function.\n",
    "There is nothing special differentiating\n",
    "the first hidden unit vs the second hidden unit.\n",
    "In other words, we have permutation symmetry\n",
    "among the hidden units of each layer.\n",
    "\n",
    "This is more than just a theoretical nuisance.\n",
    "Imagine what would happen if we initialized\n",
    "all of the parameters of some layer \n",
    "as $\\mathbf{W}_l = c$ for some constant $c$.\n",
    "In this case, the gradients \n",
    "for all dimensions are identical:\n",
    "thus not only would each unit take the same value,\n",
    "but it would receive the same update.\n",
    "Stochastic gradient descent would \n",
    "never break the symmetry on its own\n",
    "and we might never be able to realize\n",
    "the network's expressive power.\n",
    "The hidden layer would behave\n",
    "as if it had only a single unit.\n",
    "Note that while SGD would not break this symmetry,\n",
    "dropout regularization would!\n",
    "\n",
    "\n",
    "## Parameter Initialization\n",
    "\n",
    "One way of addressing---or at least mitigating---the \n",
    "issues raised above is through careful initialization.\n",
    "Additional care during optimization \n",
    "and suitable regularization can further enhance stability.\n",
    "\n",
    "\n",
    "### Default Initialization\n",
    "\n",
    "In the previous sections,\n",
    "we used `manager.randomNormal()`\n",
    "to initialize the values of our weights.\n",
    "MXNet will use the default random initialization method,\n",
    "sampling each weight parameter from \n",
    "the uniform distribution $U[-0.07, 0.07]$\n",
    "and setting the bias parameters to $0$.\n",
    "Both choices tend to work well in practice \n",
    "for moderate problem sizes.\n",
    "\n",
    "\n",
    "### Xavier Initialization\n",
    "\n",
    "Let us look at the scale distribution of\n",
    "the activations of the hidden units $h_{i}$ for some layer. \n",
    "They are given by\n",
    "\n",
    "$$h_{i} = \\sum_{j=1}^{n_\\mathrm{in}} W_{ij} x_j.$$\n",
    "\n",
    "The weights $W_{ij}$ are all drawn \n",
    "independently from the same distribution.\n",
    "Furthermore, let us assume that this distribution\n",
    "has zero mean and variance $\\sigma^2$\n",
    "(this does not mean that the distribution has to be Gaussian,\n",
    "just that the mean and variance need to exist).\n",
    "For now, let us assume that the inputs to layer $x_j$\n",
    "also have zero mean and variance $\\gamma^2$\n",
    "and that they are independent of $\\mathbf{W}$.\n",
    "In this case, we can compute the mean and variance of $h_i$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[h_i] & = \\sum_{j=1}^{n_\\mathrm{in}} E[W_{ij} x_j] = 0, \\\\\n",
    "    E[h_i^2] & = \\sum_{j=1}^{n_\\mathrm{in}} E[W^2_{ij} x^2_j] \\\\\n",
    "        & = \\sum_{j=1}^{n_\\mathrm{in}} E[W^2_{ij}] E[x^2_j] \\\\\n",
    "        & = n_\\mathrm{in} \\sigma^2 \\gamma^2.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "One way to keep the variance fixed \n",
    "is to set $n_\\mathrm{in} \\sigma^2 = 1$.\n",
    "Now consider backpropagation.\n",
    "There we face a similar problem,\n",
    "albeit with gradients being propagated from the top layers.\n",
    "That is, instead of $\\mathbf{W} \\mathbf{x}$,\n",
    "we need to deal with $\\mathbf{W}^\\top \\mathbf{g}$,\n",
    "where $\\mathbf{g}$ is the incoming gradient from the layer above.\n",
    "Using the same reasoning as for forward propagation,\n",
    "we see that the gradients' variance can blow up\n",
    "unless $n_\\mathrm{out} \\sigma^2 = 1$.\n",
    "This leaves us in a dilemma:\n",
    "we cannot possibly satisfy both conditions simultaneously.\n",
    "Instead, we simply try to satisfy:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{1}{2} (n_\\mathrm{in} + n_\\mathrm{out}) \\sigma^2 = 1 \\text{ or equivalently }\n",
    "\\sigma = \\sqrt{\\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is the reasoning underlying the now-standard\n",
    "and practically beneficial *Xavier* initialization,\n",
    "named for its creator :cite:`Glorot.Bengio.2010`.\n",
    "Typically, the Xavier initialization\n",
    "samples weights from a Gaussian distribution\n",
    "with zero mean and variance\n",
    "$\\sigma^2 = 2/(n_\\mathrm{in} + n_\\mathrm{out})$.\n",
    "We can also adapt Xavier's intuition to \n",
    "choose the variance when sampling weights\n",
    "from a uniform distribution.\n",
    "Note the distribution $U[-a, a]$ has variance $a^2/3$.\n",
    "Plugging $a^2/3$ into our condition on $\\sigma^2$ \n",
    "yields the suggestion to initialize according to\n",
    "$U\\left[-\\sqrt{6/(n_\\mathrm{in} + n_\\mathrm{out})}, \\sqrt{6/(n_\\mathrm{in} + n_\\mathrm{out})}\\right]$.\n",
    "\n",
    "### Beyond\n",
    "\n",
    "The reasoning above barely scratches the surface\n",
    "of modern approaches to parameter initialization.\n",
    "In fact, MXNet has an entire `mxnet.initializer` module\n",
    "implementing over a dozen different heuristics.\n",
    "Moreover, parameter initialization continues to be\n",
    "a hot area of fundamental research in deep learning.\n",
    "Among these are heuristics specialized for \n",
    "tied (shared) parameters, super-resolution, \n",
    "sequence models, and other situations.\n",
    "If the topic interests you we suggest \n",
    "a deep dive into this module's offerings,\n",
    "reading the papers that proposed and analyzed each heuristic,\n",
    "and then exploring the latest publications on the topic.\n",
    "Perhaps you will stumble across (or even invent!) \n",
    "a clever idea and contribute an implementation to MXNet.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "* Vanishing and exploding gradients are common issues in deep networks. Great care in parameter initialization is required to ensure that gradients and parameters remain well controlled.\n",
    "* Initialization heuristics are needed to ensure that the initial gradients are neither too large nor too small.\n",
    "* ReLU activation functions mitigate the vanishing gradient problem. This can accelerate convergence.\n",
    "* Random initialization is key to ensure that symmetry is broken before optimization.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Can you design other cases where a neural network might exhibit symmetry requiring breaking besides the permutation symmetry in a multilayer pereceptron's layers?\n",
    "1. Can we initialize all weight parameters in linear regression or in softmax regression to the same value?\n",
    "1. Look up analytic bounds on the eigenvalues of the product of two matrices. What does this tell you about ensuring that gradients are well conditioned?\n",
    "1. If we know that some terms diverge, can we fix this after the fact? Look at the paper on LARS for inspiration :cite:`You.Gitman.Ginsburg.2017`.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
