{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concise Implementation of Multilayer Perceptron\n",
    "\n",
    ":label:`sec_mlp_djl`\n",
    "\n",
    "\n",
    "As you might expect, by relying on the DJL library,\n",
    "we can implement MLPs even more concisely. <br>\n",
    "Let's setup the relevant libraries first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"NBN0Qe\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "       if(!window.letsPlotCallQueue) {\n",
       "           window.letsPlotCallQueue = [];\n",
       "       }; \n",
       "       window.letsPlotCall = function(f) {\n",
       "           window.letsPlotCallQueue.push(f);\n",
       "       };\n",
       "       (function() {\n",
       "           var script = document.createElement(\"script\");\n",
       "           script.type = \"text/javascript\";\n",
       "           script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.4.0/js-package/distr/lets-plot.min.js\";\n",
       "           script.onload = function() {\n",
       "               window.letsPlotCall = function(f) {f();};\n",
       "               window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "               window.letsPlotCallQueue = [];\n",
       "               \n",
       "               \n",
       "           };\n",
       "           script.onerror = function(event) {\n",
       "               window.letsPlotCall = function(f) {};\n",
       "               window.letsPlotCallQueue = [];\n",
       "               var div = document.createElement(\"div\");\n",
       "               div.style.color = 'darkred';\n",
       "               div.textContent = 'Error loading Lets-Plot JS';\n",
       "               document.getElementById(\"NBN0Qe\").appendChild(div);\n",
       "           };\n",
       "           var e = document.getElementById(\"NBN0Qe\");\n",
       "           e.appendChild(script);\n",
       "       })();\n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%use @file[../djl-pytorch.json]\n",
    "// @file:DependsOn(\"../mxnet-native-cu112mkl-1.9.1-linux-x86_64.jar\")\n",
    "%use lets-plot\n",
    "// %use dataframe\n",
    "//@file:DependsOn(\"org.apache.commons:commons-lang3:3.12.0\")\n",
    "// %load ../utils/djl-imports\n",
    "// %loãƒ»d ../utils/plot-utils\n",
    "\n",
    "fun getLong(nm: String, n: Long): Long {\n",
    "    val name = System.getProperty(nm)\n",
    "    return if (null == name) n.toLong() else name.toLong()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ai.djl.metric.*;\n",
    "import ai.djl.basicdataset.cv.classification.*;\n",
    "// import org.apache.commons.lang3.ArrayUtils;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "As compared to our concise implementation \n",
    "of softmax regression implementation\n",
    "(:numref:`sec_softmax_djl`),\n",
    "the only difference is that we add \n",
    "*two* `Linear` (fully-connected) layers \n",
    "(previously, we added *one*).\n",
    "The first is our hidden layer, \n",
    "which contains *256* hidden units\n",
    "and applies the ReLU activation function.\n",
    "The second is our output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "val net = SequentialBlock();\n",
    "net.add(Blocks.batchFlattenBlock(784));\n",
    "net.add(Linear.builder().setUnits(256).build());\n",
    "net.add(Activation::relu);\n",
    "net.add(Linear.builder().setUnits(10).build());\n",
    "net.setInitializer(NormalInitializer(), Parameter.Type.WEIGHT);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that DJL, as usual, automatically\n",
    "infers the missing input dimensions to each layer.\n",
    "\n",
    "The training loop is *exactly* the same\n",
    "as when we implemented softmax regression.\n",
    "This modularity enables us to separate \n",
    "matters concerning the model architecture\n",
    "from orthogonal considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val batchSize = 256;\n",
    "val  numEpochs = Integer.getInteger(\"MAX_EPOCH\", 10);\n",
    "\n",
    "val trainIter = FashionMnist.builder()\n",
    "        .optUsage(Dataset.Usage.TRAIN)\n",
    "        .setSampling(batchSize, true)\n",
    "        .optLimit(getLong(\"DATASET_LIMIT\", Long.MAX_VALUE))\n",
    "        .build();\n",
    "\n",
    "\n",
    "val testIter = FashionMnist.builder()\n",
    "        .optUsage(Dataset.Usage.TEST)\n",
    "        .setSampling(batchSize, true)\n",
    "        .optLimit(getLong(\"DATASET_LIMIT\", Long.MAX_VALUE))\n",
    "        .build();\n",
    "\n",
    "trainIter.prepare();\n",
    "testIter.prepare();\n",
    "\n",
    "val evaluatorMetrics = mutableMapOf<String, List<Float>>()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    100% |========================================| Accuracy: 0.71, SoftmaxCrossEntropyLoss: 0.78\n",
      "Validating:  100% |========================================|==============                      |=================================== |\n",
      "Training:    100% |========================================| Accuracy: 0.82, SoftmaxCrossEntropyLoss: 0.48\n",
      "Validating:  100% |========================================|================                    |========    |\n",
      "Training:    100% |========================================| Accuracy: 0.84, SoftmaxCrossEntropyLoss: 0.42\n",
      "Validating:  100% |========================================|  12% |=====                                   |=====                  |\n",
      "Training:     55% |=======================                 | Accuracy: 0.86, SoftmaxCrossEntropyLoss: 0.39"
     ]
    }
   ],
   "source": [
    "val lrt = Tracker.fixed(0.5f)\n",
    "val sgd = Optimizer.sgd().setLearningRateTracker(lrt).build();\n",
    "\n",
    "val loss = Loss.softmaxCrossEntropyLoss();\n",
    "\n",
    "val config = DefaultTrainingConfig(loss)\n",
    "                .optOptimizer(sgd) // Optimizer (loss function)\n",
    "                .optDevices(Engine.getInstance().getDevices(1)) // single GPU\n",
    "                .addEvaluator(Accuracy()) // Model Accuracy\n",
    "                .addTrainingListeners(*TrainingListener.Defaults.logging()) // Logging\n",
    "\n",
    "val model = Model.newInstance(\"mlp\").apply {\n",
    "    setBlock(net)\n",
    "}\n",
    "\n",
    "model.newTrainer(config).use { trainer ->\n",
    "    trainer.initialize(Shape(1, 784))\n",
    "    trainer.setMetrics(Metrics())\n",
    "    EasyTrain.fit(trainer, numEpochs, trainIter, testIter)\n",
    "\n",
    "    // Collect results from evaluators\n",
    "    val metrics = trainer.getMetrics()\n",
    "    trainer.getEvaluators().forEach { evaluator ->\n",
    "        val trainMetrics = metrics.getMetric(\"train_epoch_\" + evaluator.getName()).map { it.value.toFloat() }\n",
    "        evaluatorMetrics[\"train_epoch_\" + evaluator.getName()] = trainMetrics\n",
    "\n",
    "        val validateMetrics = metrics.getMetric(\"validate_epoch_\" + evaluator.getName()).map { it.value.toFloat() }\n",
    "        evaluatorMetrics[\"validate_epoch_\" + evaluator.getName()] = validateMetrics\n",
    "    }\n",
    "    trainer.close()\n",
    "}\n",
    "\n",
    "model.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trainLoss = evaluatorMetrics.get(\"train_epoch_SoftmaxCrossEntropyLoss\")\n",
    "val trainAccuracy = evaluatorMetrics.get(\"train_epoch_Accuracy\")\n",
    "val testAccuracy = evaluatorMetrics.get(\"validate_epoch_Accuracy\")\n",
    "val count = listOf(1,2,3,4,5,6,7,8,9,10)\n",
    "\n",
    "// val lossLabel = String[trainLoss.length + testAccuracy.length + trainAccuracy.length];\n",
    "\n",
    "val trainLabel = Array<String>(trainLoss!!.size) { \"train loss\" } \n",
    "val accLabel = Array<String>(trainAccuracy!!.size) { \"train acc\" }\n",
    "val testLabel = Array<String>(testAccuracy!!.size) {\"test acc\"}\n",
    "\n",
    "val data = mapOf( \"epochCount\" to count + count + count,\n",
    "                \"loss\" to trainLoss!! + trainAccuracy!! + testAccuracy!!,\n",
    "                \"lossLabel\" to trainLabel + accLabel + testLabel)\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine { x = \"epochCount\" ; y = \"loss\" ; color = \"lossLabel\"}\n",
    "plot + ggsize(500, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Try adding different numbers of hidden layers. What setting (keeping other parameters and hyperparameters constant) works best? \n",
    "1. Try out different activation functions. Which ones work best?\n",
    "1. Try different schemes for initializing the weights. What method works best?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
