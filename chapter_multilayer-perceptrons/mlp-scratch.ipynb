{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Multilayer Perceptron from Scratch\n",
    "\n",
    ":label:`sec_mlp_scratch`\n",
    "\n",
    "\n",
    "Now that we have characterized \n",
    "multilayer perceptrons (MLPs) mathematically, \n",
    "let us try to implement one ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"YAPOQU\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "       if(!window.letsPlotCallQueue) {\n",
       "           window.letsPlotCallQueue = [];\n",
       "       }; \n",
       "       window.letsPlotCall = function(f) {\n",
       "           window.letsPlotCallQueue.push(f);\n",
       "       };\n",
       "       (function() {\n",
       "           var script = document.createElement(\"script\");\n",
       "           script.type = \"text/javascript\";\n",
       "           script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.4.0/js-package/distr/lets-plot.min.js\";\n",
       "           script.onload = function() {\n",
       "               window.letsPlotCall = function(f) {f();};\n",
       "               window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "               window.letsPlotCallQueue = [];\n",
       "               \n",
       "               \n",
       "           };\n",
       "           script.onerror = function(event) {\n",
       "               window.letsPlotCall = function(f) {};\n",
       "               window.letsPlotCallQueue = [];\n",
       "               var div = document.createElement(\"div\");\n",
       "               div.style.color = 'darkred';\n",
       "               div.textContent = 'Error loading Lets-Plot JS';\n",
       "               document.getElementById(\"YAPOQU\").appendChild(div);\n",
       "           };\n",
       "           var e = document.getElementById(\"YAPOQU\");\n",
       "           e.appendChild(script);\n",
       "       })();\n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%use @file[../djl.json]\n",
    "%use lets-plot\n",
    "@file:DependsOn(\"org.apache.commons:commons-lang3:3.12.0\")\n",
    "import ai.djl.basicdataset.cv.classification.FashionMnist\n",
    "import ai.djl.metric.Metrics\n",
    "\n",
    "class Accumulator(n: Int) {\n",
    "    val data = FloatArray(n) { 0f }\n",
    "\n",
    "\n",
    "    /* Adds a set of numbers to the array */\n",
    "    fun add(args: FloatArray) {\n",
    "        for (i in 0..args.size - 1) {\n",
    "            data[i] += args[i]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /* Resets the array */\n",
    "    fun reset() {\n",
    "        data.fill(0f)\n",
    "    }\n",
    "\n",
    "    /* Returns the data point at the given index */\n",
    "    fun get(index: Int): Float {\n",
    "        return data[index]\n",
    "    }\n",
    "}\n",
    "\n",
    "fun getLong(nm: String, n: Long): Long {\n",
    "    val name = System.getProperty(nm)\n",
    "    return if (null == name) n.toLong() else name.toLong()\n",
    "}\n",
    "\n",
    "object Training {\n",
    "\n",
    "    fun linreg(X: NDArray, w: NDArray, b: NDArray): NDArray {\n",
    "        return X.dot(w).add(b);\n",
    "    }\n",
    "\n",
    "    fun squaredLoss(yHat: NDArray, y: NDArray): NDArray {\n",
    "        return (yHat.sub(y.reshape(yHat.getShape())))\n",
    "            .mul((yHat.sub(y.reshape(yHat.getShape()))))\n",
    "            .div(2);\n",
    "    }\n",
    "\n",
    "    fun sgd(params: NDList, lr: Float, batchSize: Int) {\n",
    "        for (param in params) {\n",
    "            // Update param in place.\n",
    "            // param = param - param.gradient * lr / batchSize\n",
    "            // val ind = params.indexOf(param)\n",
    "            // params.rep\n",
    "            // params.set(ind, param.sub(param.getGradient().mul(lr).div(batchSize)))\n",
    "            param.subi(param.getGradient().mul(lr).div(batchSize));\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * Allows to do gradient calculations on a subManager. This is very useful when you are training\n",
    "     * on a lot of epochs. This subManager could later be closed and all NDArrays generated from the\n",
    "     * calculations in this function will be cleared from memory when subManager is closed. This is\n",
    "     * always a great practice but the impact is most notable when there is lot of data on various\n",
    "     * epochs.\n",
    "     */\n",
    "    fun sgd(params: NDList, lr: Float, batchSize: Int, subManager: NDManager) {\n",
    "        for (param in params) {\n",
    "            // Update param in place.\n",
    "            // param = param - param.gradient * lr / batchSize\n",
    "            val gradient = param.getGradient()\n",
    "            gradient.attach(subManager);\n",
    "            param.subi(gradient.mul(lr).div(batchSize))\n",
    "        }\n",
    "    }\n",
    "\n",
    "    fun accuracy(yHat: NDArray, y: NDArray): Float {\n",
    "        // Check size of 1st dimension greater than 1\n",
    "        // to see if we have multiple samples\n",
    "        if (yHat.getShape().size(1) > 1) {\n",
    "            // Argmax gets index of maximum args for given axis 1\n",
    "            // Convert yHat to same dataType as y (int32)\n",
    "            // Sum up number of true entries\n",
    "            return yHat.argMax(1)\n",
    "                .toType(DataType.INT32, false)\n",
    "                .eq(y.toType(DataType.INT32, false))\n",
    "                .sum()\n",
    "                .toType(DataType.FLOAT32, false)\n",
    "                .getFloat();\n",
    "        }\n",
    "        return yHat.toType(DataType.INT32, false)\n",
    "            .eq(y.toType(DataType.INT32, false))\n",
    "            .sum()\n",
    "            .toType(DataType.FLOAT32, false)\n",
    "            .getFloat();\n",
    "    }\n",
    "\n",
    "    fun trainingChapter6(\n",
    "        trainIter: ArrayDataset,\n",
    "        testIter: ArrayDataset,\n",
    "        numEpochs: Int,\n",
    "        trainer: Trainer,\n",
    "        evaluatorMetrics: MutableMap<String, DoubleArray>\n",
    "    ): Double {\n",
    "\n",
    "        trainer.setMetrics(Metrics())\n",
    "\n",
    "        EasyTrain.fit(trainer, numEpochs, trainIter, testIter)\n",
    "\n",
    "        val metrics = trainer.getMetrics()\n",
    "\n",
    "        trainer.getEvaluators()\n",
    "            .forEach { evaluator ->\n",
    "                {\n",
    "                    evaluatorMetrics.put(\n",
    "                        \"train_epoch_\" + evaluator.getName(),\n",
    "                        metrics.getMetric(\"train_epoch_\" + evaluator.getName()).stream()\n",
    "                            .mapToDouble { x -> x.getValue() }\n",
    "                            .toArray())\n",
    "                    evaluatorMetrics.put(\n",
    "                        \"validate_epoch_\" + evaluator.getName(),\n",
    "                        metrics\n",
    "                            .getMetric(\"validate_epoch_\" + evaluator.getName())\n",
    "                            .stream()\n",
    "                            .mapToDouble { x -> x.getValue() }\n",
    "                            .toArray())\n",
    "                }\n",
    "            }\n",
    "\n",
    "        return metrics.mean(\"epoch\")\n",
    "    }\n",
    "\n",
    "    /* Softmax-regression-scratch */\n",
    "    fun evaluateAccuracy(net: UnaryOperator<NDArray>, dataIterator: Iterable<Batch>): Float {\n",
    "        val metric = Accumulator(2) // numCorrectedExamples, numExamples\n",
    "        for (batch in dataIterator) {\n",
    "            val X = batch.getData().head()\n",
    "            val y = batch.getLabels().head()\n",
    "            metric.add(floatArrayOf(accuracy(net.apply(X), y), y.size().toFloat()))\n",
    "            batch.close()\n",
    "        }\n",
    "        return metric.get(0) / metric.get(1)\n",
    "    }\n",
    "    /* End Softmax-regression-scratch */\n",
    "\n",
    "    /* MLP */\n",
    "    /* Evaluate the loss of a model on the given dataset */\n",
    "    fun evaluateLoss(\n",
    "        net: UnaryOperator<NDArray>,\n",
    "        dataIterator: Iterable<Batch>,\n",
    "        loss: BinaryOperator<NDArray>\n",
    "    ): Float {\n",
    "        val metric = Accumulator(2) // sumLoss, numExamples\n",
    "\n",
    "        for (batch in dataIterator) {\n",
    "            val X = batch . getData ().head();\n",
    "            val y = batch . getLabels ().head();\n",
    "            metric.add(\n",
    "                floatArrayOf(loss.apply(net.apply(X), y).sum().getFloat(), y.size().toFloat()) )\n",
    "            batch.close()\n",
    "        }\n",
    "        return metric.get(0) / metric.get(1)\n",
    "    }\n",
    "    /* End MLP */\n",
    "}\n",
    "\n",
    "// %load ../utils/djl-imports\n",
    "// %load ../utils/plot-utils\n",
    "// %load ../utils/DataPoints.java\n",
    "// %load ../utils/Training.java\n",
    "// %load ../utils/Accumulator.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ai.djl.basicdataset.cv.classification.*\n",
    "import org.apache.commons.lang3.ArrayUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare against our previous results\n",
    "achieved with (linear) softmax regression\n",
    "(:numref:`sec_softmax_scratch`),\n",
    "we will continue work with \n",
    "the Fashion-MNIST image classification dataset \n",
    "(:numref:`sec_fashion_mnist`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "val batchSize = 256;\n",
    "\n",
    "val trainIter = FashionMnist.builder()\n",
    "        .optUsage(Dataset.Usage.TRAIN)\n",
    "        .setSampling(batchSize, true)\n",
    "        .optLimit(getLong(\"DATASET_LIMIT\", Long.MAX_VALUE))\n",
    "        .build();\n",
    "\n",
    "\n",
    "val testIter = FashionMnist.builder()\n",
    "        .optUsage(Dataset.Usage.TEST)\n",
    "        .setSampling(batchSize, true)\n",
    "        .optLimit(getLong(\"DATASET_LIMIT\", Long.MAX_VALUE))\n",
    "        .build();\n",
    "                            \n",
    "trainIter.prepare();\n",
    "testIter.prepare();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Model Parameters\n",
    "\n",
    "Recall that Fashion-MNIST contains $10$ classes,\n",
    "and that each image consists of a $28 \\times 28 = 784$\n",
    "grid of (black and white) pixel values.\n",
    "Again, we will disregard the spatial structure\n",
    "among the pixels (for now),\n",
    "so we can think of this as simply a classification dataset\n",
    "with $784$ input features and $10$ classes.\n",
    "To begin, we will implement an MLP\n",
    "with one hidden layer and $256$ hidden units.\n",
    "Note that we can regard both of these quantities\n",
    "as *hyperparameters* and ought in general\n",
    "to set them based on performance on validation data.\n",
    "Typically, we choose layer widths in powers of $2$,\n",
    "which tend to be computationally efficient because\n",
    "of how memory is alotted and addressed in hardware.\n",
    "\n",
    "Again, we will represent our parameters with several `NDArray`s.\n",
    "Note that *for every layer*, we must keep track of\n",
    "one weight matrix and one bias vector.\n",
    "As always, we call `attachGradient()` to allocate memory\n",
    "for the gradients (of the loss) with respect to these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "val  numInputs = 784L\n",
    "val  numOutputs = 10L\n",
    "val numHiddens = 256L\n",
    "\n",
    "val manager = NDManager.newBaseManager();\n",
    "\n",
    "val W1 = manager.randomNormal(\n",
    "                        0f, 0.01f, Shape(numInputs, numHiddens), DataType.FLOAT32);\n",
    "val b1 = manager.zeros(Shape(numHiddens));\n",
    "val W2 = manager.randomNormal(\n",
    "                        0f, 0.01f, Shape(numHiddens, numOutputs), DataType.FLOAT32);\n",
    "val b2 = manager.zeros(Shape(numOutputs));\n",
    "\n",
    "val params = NDList(W1, b1, W2, b2);\n",
    "\n",
    "for (param in params) {\n",
    "    param.setRequiresGradient(true);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "\n",
    "To make sure we know how everything works,\n",
    "we will implement the ReLU activation ourselves\n",
    "using the `maximum` function rather than \n",
    "invoking `Activation.relu` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "fun relu(X: NDArray): NDArray {\n",
    "    return X.maximum(0f);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "Because we are disregarding spatial structure, \n",
    "we `reshape` each 2D image into \n",
    "a flat vector of length  `numInputs`.\n",
    "Finally, we implement our model \n",
    "with just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "fun net(X: NDArray): NDArray {\n",
    "    val X0 = X.reshape(Shape(-1, numInputs));\n",
    "    val H = relu(X0.dot(W1).add(b1));\n",
    "    return H.dot(W2).add(b2);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Loss Function\n",
    "\n",
    "To ensure numerical stability,\n",
    "and because we already implemented\n",
    "the softmax function from scratch\n",
    "(:numref:`sec_softmax_scratch`),\n",
    "we leverage Gluon's integrated function\n",
    "for calculating the softmax and cross-entropy loss.\n",
    "Recall our earlier discussion of these intricacies \n",
    "(:numref:`sec_mlp`).\n",
    "We encourage the interested reader \n",
    "to examine the source code for `Loss.SoftmaxCrossEntropyLoss`\n",
    "to deepen their knowledge of implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "val loss = Loss.softmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Fortunately, the training loop for MLPs\n",
    "is exactly the same as for softmax regression.\n",
    "\n",
    "We run the training like how we did in Chapter 3, \n",
    "(see :numref:`sec_softmax_scratch`),\n",
    "setting the number of epochs to $10$ \n",
    "and the learning rate to $0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val numEpochs = Integer.getInteger(\"MAX_EPOCH\", 10);\n",
    "val lr = 0.5f;\n",
    "\n",
    "val trainLoss = mutableListOf<Float>()\n",
    "val trainAccuracy = mutableListOf<Float>()\n",
    "val testAccuracy = mutableListOf<Float>()\n",
    "val epochCount = mutableListOf<Int>()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1...... Finished epoch 1\n",
      "Running epoch 2...... Finished epoch 2\n",
      "Running epoch 3...... Finished epoch 3\n",
      "Running epoch 4...... Finished epoch 4\n",
      "Running epoch 5...... Finished epoch 5\n",
      "Running epoch 6...... Finished epoch 6\n",
      "Running epoch 7...... Finished epoch 7\n",
      "Running epoch 8...... Finished epoch 8\n",
      "Running epoch 9...... Finished epoch 9\n",
      "Running epoch 10...... Finished epoch 10\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "var epochLoss = 0f;\n",
    "var accuracyVal = 0f;\n",
    "\n",
    "for (epoch in 1.. numEpochs) {\n",
    "    print(\"Running epoch \" + epoch + \"...... \");\n",
    "    // Iterate over dataset\n",
    "    for (batch in trainIter.getData(manager)) {\n",
    "\n",
    "        val X = batch.getData().head();\n",
    "        val y = batch.getLabels().head();\n",
    "\n",
    "        val gc = Engine.getInstance().newGradientCollector()\n",
    "            val yHat = net(X); // net function call\n",
    "\n",
    "            val lossValue = loss.evaluate(NDList(y), NDList(yHat));\n",
    "            val l = lossValue.mul(batchSize);\n",
    "\n",
    "            accuracyVal += Training.accuracy(yHat, y);\n",
    "            epochLoss += l.sum().getFloat();\n",
    "\n",
    "            gc.backward(l); // gradient calculation\n",
    "        gc.close()\n",
    "\n",
    "        batch.close();\n",
    "        Training.sgd(params, lr, batchSize); // updater\n",
    "    }\n",
    "\n",
    "    trainLoss.add(epochLoss/trainIter.size())\n",
    "    trainAccuracy.add(accuracyVal/trainIter.size())\n",
    "\n",
    "    epochLoss = 0f;\n",
    "    accuracyVal = 0f;    \n",
    "    // testing now\n",
    "    for (batch in testIter.getData(manager)) {\n",
    "\n",
    "        val X = batch.getData().head();\n",
    "        val y = batch.getLabels().head();\n",
    "\n",
    "        val yHat = net(X); // net function call\n",
    "        accuracyVal += Training.accuracy(yHat, y);\n",
    "    }\n",
    "\n",
    "    testAccuracy.add(accuracyVal/testIter.size())\n",
    "    epochCount.add(epoch)\n",
    "    accuracyVal = 0f;\n",
    "    println(\"Finished epoch \" + epoch);\n",
    "}\n",
    "\n",
    "println(\"Finished training!\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"QKjVgF\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"loss\":[0.8107283115386963,0.49431416392326355,0.42597705125808716,0.39121100306510925,0.36855366826057434,0.35323548316955566,0.33905303478240967,0.32643285393714905,0.31755194067955017,0.3064208924770355,0.7002000212669373,0.8180166482925415,0.8446333408355713,0.8559666872024536,0.8647000193595886,0.8696833252906799,0.8750166893005371,0.8805000185966492,0.8827999830245972,0.8859833478927612,0.7450000047683716,0.8108999729156494,0.8270000219345093,0.8234999775886536,0.8414999842643738,0.8427000045776367,0.8586000204086304,0.8536999821662903,0.8614000082015991,0.8507999777793884],\n",
       "\"lossLabel\":[\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train acc\",\"train acc\",\"train acc\",\"train acc\",\"train acc\",\"train acc\",\"train acc\",\"train acc\",\"train acc\",\"train acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\"],\n",
       "\"epochCount\":[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":500.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"epochCount\",\n",
       "\"y\":\"loss\",\n",
       "\"color\":\"lossLabel\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"QKjVgF\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainLabel = Array<String>(trainLoss.size) { \"train loss\" } \n",
    "val accLabel = Array<String>(trainAccuracy.size) { \"train acc\" }\n",
    "val testLabel = Array<String>(testAccuracy.size) {\"test acc\"}\n",
    "\n",
    "val data = mapOf( \"epochCount\" to epochCount + epochCount + epochCount,\n",
    "                \"loss\" to trainLoss + trainAccuracy + testAccuracy,\n",
    "                \"lossLabel\" to trainLabel + accLabel + testLabel)\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine { x = \"epochCount\" ; y = \"loss\" ; color = \"lossLabel\"}\n",
    "plot + ggsize(500, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We saw that implementing a simple MLP is easy, \n",
    "even when done manually.\n",
    "That said, with a large number of layers, \n",
    "this can still get messy \n",
    "(e.g., naming and keeping track of our model's parameters, etc).\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Change the value of the hyperparameter `numHiddens` and see how this hyperparameter influences your results. Determine the best value of this hyperparameter, keeping all others constant.\n",
    "1. Try adding an additional hidden layer to see how it affects the results.\n",
    "1. How does changing the learning rate alter your results? Fixing the model architecture and other hyperparameters (including number of epochs), what learning rate gives you the best results? \n",
    "1. What is the best result you can get by optimizing over all the parameters (learning rate, iterations, number of hidden layers, number of hidden units per layer) jointly? \n",
    "1. Describe why it is much more challenging to deal with multiple hyperparameters. \n",
    "1. What is the smartest strategy you can think of for structuring a search over multiple hyperparameters?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
