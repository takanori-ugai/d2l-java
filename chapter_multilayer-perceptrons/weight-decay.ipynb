{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Decay\n",
    "\n",
    ":label:`sec_weight_decay`\n",
    "\n",
    "\n",
    "Now that we have characterized the problem of overfitting,\n",
    "we can introduce some standard techniques for regularizing models.\n",
    "Recall that we can always mitigate overfitting\n",
    "by going out and collecting more training data.\n",
    "That can be costly, time consuming,\n",
    "or entirely out of our control,\n",
    "making it impossible in the short run.\n",
    "For now, we can assume that we already have\n",
    "as much high-quality data as our resources permit\n",
    "and focus on regularization techniques.\n",
    "\n",
    "Recall that in our\n",
    "polynomial curve-fitting example\n",
    "(:numref:`sec_model_selection`)\n",
    "we could limit our model's capacity\n",
    "simply by tweaking the degree \n",
    "of the fitted polynomial.\n",
    "Indeed, limiting the number of features \n",
    "is a popular technique to avoid overfitting.\n",
    "However, simply tossing aside features\n",
    "can be too blunt an instrument for the job.\n",
    "Sticking with the polynomial curve-fitting\n",
    "example, consider what might happen\n",
    "with high-dimensional inputs.\n",
    "The natural extensions of polynomials\n",
    "to multivariate data are called *monomials*, \n",
    "which are simply products of powers of variables.\n",
    "The degree of a monomial is the sum of the powers.\n",
    "For example, $x_1^2 x_2$, and $x_3 x_5^2$ \n",
    "are both monomials of degree $3$.\n",
    "\n",
    "Note that the number of terms with degree $d$\n",
    "blows up rapidly as $d$ grows larger.\n",
    "Given $k$ variables, the number of monomials \n",
    "of degree $d$ is ${k - 1 + d} \\choose {k - 1}$.\n",
    "Even small changes in degree, say from $2$ to $3$,\n",
    "dramatically increase the complexity of our model.\n",
    "Thus we often need a more fine-grained tool\n",
    "for adjusting function complexity.\n",
    "\n",
    "## Squared Norm Regularization\n",
    "\n",
    "*Weight decay* (commonly called *L2* regularization),\n",
    "might be the most widely-used technique\n",
    "for regularizing parametric machine learning models.\n",
    "The technique is motivated by the basic intuition\n",
    "that among all functions $f$,\n",
    "the function $f = 0$ \n",
    "(assigning the value $0$ to all inputs) \n",
    "is in some sense the *simplest*,\n",
    "and that we can measure the complexity \n",
    "of a function by its distance from zero.\n",
    "But how precisely should we measure\n",
    "the distance between a function and zero?\n",
    "There is no single right answer.\n",
    "In fact, entire branches of mathematics,\n",
    "including parts of functional analysis \n",
    "and the theory of Banach spaces,\n",
    "are devoted to answering this issue.\n",
    "\n",
    "One simple interpretation might be \n",
    "to measure the complexity of a linear function\n",
    "$f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}$\n",
    "by some norm of its weight vector, e.g., $|| \\mathbf{w} ||^2$.\n",
    "The most common method for ensuring a small weight vector\n",
    "is to add its norm as a penalty term\n",
    "to the problem of minimizing the loss.\n",
    "Thus we replace our original objective,\n",
    "*minimize the prediction loss on the training labels*,\n",
    "with new objective,\n",
    "*minimize the sum of the prediction loss and the penalty term*.\n",
    "Now, if our weight vector grows too large,\n",
    "our learning algorithm might *focus* \n",
    "on minimizing the weight norm $|| \\mathbf{w} ||^2$\n",
    "versus minimizing the training error.\n",
    "That is exactly what we want.\n",
    "To illustrate things in code, \n",
    "let us revive our previous example\n",
    "from :numref:`sec_linear_regression` for linear regression.\n",
    "There, our loss was given by\n",
    "\n",
    "$$l(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.$$\n",
    "\n",
    "Recall that $\\mathbf{x}^{(i)}$ are the observations,\n",
    "$y^{(i)}$ are labels, and $(\\mathbf{w}, b)$\n",
    "are the weight and bias parameters respectively.\n",
    "To penalize the size of the weight vector,\n",
    "we must somehow add $|| \\mathbf{w} ||^2$ to the loss function,\n",
    "but how should the model trade off the \n",
    "standard loss for this new additive penalty?\n",
    "In practice, we characterize this tradeoff\n",
    "via the *regularization constant* $\\lambda > 0$, \n",
    "a non-negative hyperparameter \n",
    "that we fit using validation data:\n",
    "\n",
    "$$l(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2.$$\n",
    "\n",
    "For $\\lambda = 0$, we recover our original loss function.\n",
    "For $\\lambda > 0$, we restrict the size of $|| \\mathbf{w} ||$.\n",
    "The astute reader might wonder why we work with the squared\n",
    "norm and not the standard norm (i.e., the Euclidean distance).\n",
    "We do this for computational convenience.\n",
    "By squaring the L2 norm, we remove the square root, \n",
    "leaving the sum of squares of \n",
    "each component of the weight vector.\n",
    "This makes the derivative of the penalty easy to compute\n",
    "(the sum of derivatives equals the derivative of the sum).\n",
    "\n",
    "Moreover, you might ask why we work with the L2 norm \n",
    "in the first place and not, say, the L1 norm.\n",
    "\n",
    "In fact, other choices are valid and \n",
    "popular throughout statistics.\n",
    "While L2-regularized linear models constitute\n",
    "the classic *ridge regression* algorithm,\n",
    "L1-regularized linear regression\n",
    "is a similarly fundamental model in statistics\n",
    "(popularly known as *lasso regression*).\n",
    "\n",
    "More generally, the $\\ell_2$ is just one \n",
    "among an infinite class of norms call p-norms,\n",
    "many of which you might encounter in the future.\n",
    "In general, for some number $p$, \n",
    "the $\\ell_p$ norm is defined as\n",
    "\n",
    "$$\\|\\mathbf{w}\\|_p^p := \\sum_{i=1}^d |w_i|^p.$$\n",
    "\n",
    "\n",
    "One reason to work with the L2 norm\n",
    "is that it places and outsize penalty\n",
    "on large components of the weight vector.\n",
    "This biases our learning algorithm \n",
    "towards models that distribute weight evenly \n",
    "across a larger number of features.\n",
    "In practice, this might make them more robust\n",
    "to measurement error in a single variable.\n",
    "By contrast, L1 penalties lead to models\n",
    "that concentrate weight on a small set of features,\n",
    "which may be desirable for other reasons. \n",
    "\n",
    "The stochastic gradient descent updates \n",
    "for L2-regularized regression follow:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{w} & \\leftarrow \\left(1- \\eta\\lambda \\right) \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "As before, we update $\\mathbf{w}$ based on the amount \n",
    "by which our estimate differs from the observation.\n",
    "However, we also shrink the size of $\\mathbf{w}$ towards $0$.\n",
    "That is why the method is sometimes called \"weight decay\":\n",
    "given the penalty term alone,\n",
    "our optimization algorithm *decays*\n",
    "the weight at each step of training.\n",
    "In contrast to feature selection,\n",
    "weight decay offers us a continuous mechanism\n",
    "for adjusting the complexity of $f$.\n",
    "Small values of $\\lambda$ correspond \n",
    "to unconstrained $\\mathbf{w}$,\n",
    "whereas large values of $\\lambda$ \n",
    "constrain $\\mathbf{w}$ considerably.\n",
    "Whether we include a corresponding bias penalty $b^2$ \n",
    "can vary across implementations, \n",
    "and may vary across layers of a neural network.\n",
    "Often, we do not regularize the bias term\n",
    "of a network's output layer.\n",
    " \n",
    "\n",
    "## High-Dimensional Linear Regression\n",
    "\n",
    "We can illustrate the benefits of \n",
    "weight decay over feature selection\n",
    "through a simple synthetic example.\n",
    "First, we generate some data as before\n",
    "\n",
    "$$y = 0.05 + \\sum_{i = 1}^d 0.01 x_i + \\epsilon \\text{ where }\n",
    "\\epsilon \\sim \\mathcal{N}(0, 0.01).$$\n",
    "\n",
    "choosing our label to be a linear function of our inputs,\n",
    "corrupted by Gaussian noise with zero mean and variance 0.01.\n",
    "To make the effects of overfitting pronounced,\n",
    "we can increase the dimensionality of our problem to $d = 200$\n",
    "and work with a small training set containing only 20 examples.\n",
    "\n",
    "We will now import the relevant libraries for showing weight decay concept in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"sYrCDL\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "       if(!window.letsPlotCallQueue) {\n",
       "           window.letsPlotCallQueue = [];\n",
       "       }; \n",
       "       window.letsPlotCall = function(f) {\n",
       "           window.letsPlotCallQueue.push(f);\n",
       "       };\n",
       "       (function() {\n",
       "           var script = document.createElement(\"script\");\n",
       "           script.type = \"text/javascript\";\n",
       "           script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.4.0/js-package/distr/lets-plot.min.js\";\n",
       "           script.onload = function() {\n",
       "               window.letsPlotCall = function(f) {f();};\n",
       "               window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "               window.letsPlotCallQueue = [];\n",
       "               \n",
       "               \n",
       "           };\n",
       "           script.onerror = function(event) {\n",
       "               window.letsPlotCall = function(f) {};\n",
       "               window.letsPlotCallQueue = [];\n",
       "               var div = document.createElement(\"div\");\n",
       "               div.style.color = 'darkred';\n",
       "               div.textContent = 'Error loading Lets-Plot JS';\n",
       "               document.getElementById(\"sYrCDL\").appendChild(div);\n",
       "           };\n",
       "           var e = document.getElementById(\"sYrCDL\");\n",
       "           e.appendChild(script);\n",
       "       })();\n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%use @file[../djl.json]\n",
    "%use lets-plot\n",
    "@file:DependsOn(\"org.apache.commons:commons-lang3:3.12.0\")\n",
    "import ai.djl.metric.Metrics\n",
    "\n",
    "class Accumulator(n: Int) {\n",
    "    val data = FloatArray(n) { 0f }\n",
    "\n",
    "\n",
    "    /* Adds a set of numbers to the array */\n",
    "    fun add(args: FloatArray) {\n",
    "        for (i in 0..args.size - 1) {\n",
    "            data[i] += args[i]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /* Resets the array */\n",
    "    fun reset() {\n",
    "        data.fill(0f)\n",
    "    }\n",
    "\n",
    "    /* Returns the data point at the given index */\n",
    "    fun get(index: Int): Float {\n",
    "        return data[index]\n",
    "    }\n",
    "}\n",
    "\n",
    "class DataPoints(X:NDArray , y:NDArray ) {\n",
    "    private val X = X\n",
    "    private val y = y\n",
    "\n",
    "    fun  getX() : NDArray{\n",
    "        return X\n",
    "    }\n",
    "    \n",
    "    fun getY() :NDArray {\n",
    "        return y\n",
    "    }\n",
    "}\n",
    "\n",
    "fun syntheticData(manager:NDManager , w: NDArray , b : Float, numExamples: Int) : DataPoints {\n",
    "    val X = manager.randomNormal(Shape(numExamples.toLong(), w.size()))\n",
    "    var y = X.matMul(w).add(b)\n",
    "    // Add noise\n",
    "    y = y.add(manager.randomNormal(0f, 0.01f, y.getShape(), DataType.FLOAT32))\n",
    "    return DataPoints(X, y);\n",
    "}\n",
    "\n",
    "object Training {\n",
    "\n",
    "    fun linreg(X: NDArray, w: NDArray, b: NDArray): NDArray {\n",
    "        return X.dot(w).add(b);\n",
    "    }\n",
    "\n",
    "    fun squaredLoss(yHat: NDArray, y: NDArray): NDArray {\n",
    "        return (yHat.sub(y.reshape(yHat.getShape())))\n",
    "            .mul((yHat.sub(y.reshape(yHat.getShape()))))\n",
    "            .div(2);\n",
    "    }\n",
    "\n",
    "    fun sgd(params: NDList, lr: Float, batchSize: Int) {\n",
    "    val lrt = Tracker.fixed(lr);\n",
    "    val opt = Optimizer.sgd().setLearningRateTracker(lrt).build();\n",
    "        for (param in params) {\n",
    "            // Update param in place.\n",
    "            // param = param - param.gradient * lr / batchSize\n",
    "            // val ind = params.indexOf(param)\n",
    "            // params.rep\n",
    "            // params.set(ind, param.sub(param.getGradient().mul(lr).div(batchSize)))\n",
    "            opt.update(param.toString(), param, param.getGradient().div(batchSize))\n",
    "//            param.subi(param.getGradient().mul(lr).div(batchSize));\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * Allows to do gradient calculations on a subManager. This is very useful when you are training\n",
    "     * on a lot of epochs. This subManager could later be closed and all NDArrays generated from the\n",
    "     * calculations in this function will be cleared from memory when subManager is closed. This is\n",
    "     * always a great practice but the impact is most notable when there is lot of data on various\n",
    "     * epochs.\n",
    "     */\n",
    "    fun sgd(params: NDList, lr: Float, batchSize: Int, subManager: NDManager) {\n",
    "        for (param in params) {\n",
    "            // Update param in place.\n",
    "            // param = param - param.gradient * lr / batchSize\n",
    "            val gradient = param.getGradient()\n",
    "            gradient.attach(subManager);\n",
    "            param.subi(gradient.mul(lr).div(batchSize))\n",
    "        }\n",
    "    }\n",
    "\n",
    "    fun accuracy(yHat: NDArray, y: NDArray): Float {\n",
    "        // Check size of 1st dimension greater than 1\n",
    "        // to see if we have multiple samples\n",
    "        if (yHat.getShape().size(1) > 1) {\n",
    "            // Argmax gets index of maximum args for given axis 1\n",
    "            // Convert yHat to same dataType as y (int32)\n",
    "            // Sum up number of true entries\n",
    "            return yHat.argMax(1)\n",
    "                .toType(DataType.INT32, false)\n",
    "                .eq(y.toType(DataType.INT32, false))\n",
    "                .sum()\n",
    "                .toType(DataType.FLOAT32, false)\n",
    "                .getFloat();\n",
    "        }\n",
    "        return yHat.toType(DataType.INT32, false)\n",
    "            .eq(y.toType(DataType.INT32, false))\n",
    "            .sum()\n",
    "            .toType(DataType.FLOAT32, false)\n",
    "            .getFloat();\n",
    "    }\n",
    "\n",
    "    fun trainingChapter6(\n",
    "        trainIter: ArrayDataset,\n",
    "        testIter: ArrayDataset,\n",
    "        numEpochs: Int,\n",
    "        trainer: Trainer,\n",
    "        evaluatorMetrics: MutableMap<String, DoubleArray>\n",
    "    ): Double {\n",
    "\n",
    "        trainer.setMetrics(Metrics())\n",
    "\n",
    "        EasyTrain.fit(trainer, numEpochs, trainIter, testIter)\n",
    "\n",
    "        val metrics = trainer.getMetrics()\n",
    "\n",
    "        trainer.getEvaluators()\n",
    "            .forEach { evaluator ->\n",
    "                {\n",
    "                    evaluatorMetrics.put(\n",
    "                        \"train_epoch_\" + evaluator.getName(),\n",
    "                        metrics.getMetric(\"train_epoch_\" + evaluator.getName()).stream()\n",
    "                            .mapToDouble { x -> x.getValue() }\n",
    "                            .toArray())\n",
    "                    evaluatorMetrics.put(\n",
    "                        \"validate_epoch_\" + evaluator.getName(),\n",
    "                        metrics\n",
    "                            .getMetric(\"validate_epoch_\" + evaluator.getName())\n",
    "                            .stream()\n",
    "                            .mapToDouble { x -> x.getValue() }\n",
    "                            .toArray())\n",
    "                }\n",
    "            }\n",
    "\n",
    "        return metrics.mean(\"epoch\")\n",
    "    }\n",
    "\n",
    "    /* Softmax-regression-scratch */\n",
    "    fun evaluateAccuracy(net: UnaryOperator<NDArray>, dataIterator: Iterable<Batch>): Float {\n",
    "        val metric = Accumulator(2) // numCorrectedExamples, numExamples\n",
    "        for (batch in dataIterator) {\n",
    "            val X = batch.getData().head()\n",
    "            val y = batch.getLabels().head()\n",
    "            metric.add(floatArrayOf(accuracy(net.apply(X), y), y.size().toFloat()))\n",
    "            batch.close()\n",
    "        }\n",
    "        return metric.get(0) / metric.get(1)\n",
    "    }\n",
    "    /* End Softmax-regression-scratch */\n",
    "\n",
    "    /* MLP */\n",
    "    /* Evaluate the loss of a model on the given dataset */\n",
    "    fun evaluateLoss(\n",
    "        net: UnaryOperator<NDArray>,\n",
    "        dataIterator: Iterable<Batch>,\n",
    "        loss: BinaryOperator<NDArray>\n",
    "    ): Float {\n",
    "        val metric = Accumulator(2) // sumLoss, numExamples\n",
    "\n",
    "        for (batch in dataIterator) {\n",
    "            val X = batch . getData ().head();\n",
    "            val y = batch . getLabels ().head();\n",
    "            metric.add(\n",
    "                floatArrayOf(loss.apply(net.apply(X), y).sum().getFloat(), y.size().toFloat()) )\n",
    "            batch.close()\n",
    "        }\n",
    "        return metric.get(0) / metric.get(1)\n",
    "    }\n",
    "    /* End MLP */\n",
    "}\n",
    "\n",
    "// %load ../utils/djl-imports\n",
    "// %load ../utils/plot-utils\n",
    "// %load ../utils/DataPoints.java\n",
    "// %load ../utils/Training.java\n",
    "// %load ../utils/Accumulator.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.commons.lang3.ArrayUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "val nTrain = 20;\n",
    "val nTest = 100;\n",
    "val numInputs = 200;\n",
    "val batchSize = 5;\n",
    "\n",
    "val trueB = 0.05f;\n",
    "val manager = NDManager.newBaseManager();\n",
    "val trueW = manager.ones(Shape(numInputs.toLong(), 1)).mul(0.01)\n",
    "\n",
    "fun  loadArray(features: NDArray , labels: NDArray , batchSize: Int, shuffle: Boolean) : ArrayDataset {\n",
    "    return ArrayDataset.Builder()\n",
    "                  .setData(features) // set the features\n",
    "                  .optLabels(labels) // set the labels\n",
    "                  .setSampling(batchSize, shuffle) // set the batch size and random sampling\n",
    "                  .build();\n",
    "}\n",
    "\n",
    "val trainData = syntheticData(manager, trueW, trueB, nTrain);\n",
    "\n",
    "val trainIter = loadArray(trainData.getX(), trainData.getY(), batchSize, true);\n",
    "\n",
    "val testData = syntheticData(manager, trueW, trueB, nTest);\n",
    "\n",
    "val testIter = loadArray(testData.getX(), testData.getY(), batchSize, false);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation from Scratch\n",
    "\n",
    "Next, we will implement weight decay from scratch,\n",
    "simply by adding the squared $\\ell_2$ penalty\n",
    "to the original target function.\n",
    "\n",
    "### Initializing Model Parameters\n",
    "\n",
    "First, we will define a function \n",
    "to randomly initialize our model parameters \n",
    "and run `attachGradient()` on each to allocate \n",
    "memory for the gradients we will calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "class InitParams{\n",
    "\n",
    "    private val manager = NDManager.newBaseManager()\n",
    "    private val w = manager.randomNormal(0f, 1.0f, Shape(numInputs.toLong(), 1), DataType.FLOAT32)\n",
    "    private val b = manager.zeros(Shape(1))\n",
    "    \n",
    "    init {\n",
    "        w.setRequiresGradient(true)\n",
    "        b.setRequiresGradient(true)\n",
    "    }\n",
    "    \n",
    "//    private NDList l;\n",
    "    \n",
    "    fun getW(): NDArray{\n",
    "        return this.w;\n",
    "    }\n",
    "    \n",
    "    fun getB(): NDArray {\n",
    "        return this.b;\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining $\\ell_2$ Norm Penalty\n",
    "\n",
    "Perhaps the most convenient way to implement this penalty\n",
    "is to square all terms in place and sum them up.\n",
    "We divide by $2$ by convention\n",
    "(when we take the derivative of a quadratic function,\n",
    "the $2$ and $1/2$ cancel out, ensuring that the expression\n",
    "for the update looks nice and simple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "fun l2Penalty(w: NDArray): NDArray{\n",
    "    return ((w.pow(2)).sum()).div(2);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val l2loss = Loss.l2Loss();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Train and Test Functions\n",
    "\n",
    "The following code fits a model on the training set\n",
    "and evaluates it on the test set.\n",
    "The linear network and the squared loss\n",
    "have not changed since the previous chapter,\n",
    "so we will just import them via `Training.linreg()` and `Training.squaredLoss()`.\n",
    "The only change here is that our loss now includes the penalty term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "val epochCount = mutableListOf<Int>()\n",
    "val trainLoss = mutableListOf<Float>()\n",
    "val testLoss = mutableListOf<Float>()\n",
    "\n",
    "fun train(lambd: Float) {\n",
    "    \n",
    "    val initParams = InitParams();\n",
    "    \n",
    "    val params = NDList(initParams.getW(), initParams.getB());\n",
    "    \n",
    "    val numEpochs = Integer.getInteger(\"MAX_EPOCH\", 100);\n",
    "    val lr = 0.003f;\n",
    "    \n",
    "    for (epoch in 1..numEpochs){\n",
    "        \n",
    "        for (batch in trainIter.getData(manager)){\n",
    "            \n",
    "            val X = batch.getData().head();\n",
    "            val y = batch.getLabels().head();\n",
    "            \n",
    "             val w = params.get(0);\n",
    "             val b = params.get(1);\n",
    "            \n",
    "             Engine.getInstance().newGradientCollector().use { gc ->\n",
    "                // The L2 norm penalty term has been added, and broadcasting\n",
    "                // makes `l2Penalty(w)` a vector whose length is `batch_size`\n",
    "                val l = Training.squaredLoss(Training.linreg(X, w, b), y).add(l2Penalty(w).mul(lambd));\n",
    "                gc.backward(l);  // Compute gradient on l with respect to w and b\n",
    "                \n",
    "            }\n",
    "            \n",
    "            batch.close();\n",
    "            Training.sgd(params, lr, batchSize);  // Update parameters using their gradient\n",
    "        }\n",
    "        \n",
    "        if(epoch % 5 == 0){\n",
    "            val testL = Training.squaredLoss(Training.linreg(testData.getX(), params.get(0), params.get(1)), testData.getY());\n",
    "            val trainL = Training.squaredLoss(Training.linreg(trainData.getX(), params.get(0), params.get(1)), trainData.getY());\n",
    "            \n",
    "            epochCount.add(epoch)  \n",
    "            trainLoss.add(trainL.mean().log10().getFloat())\n",
    "            testLoss.add(testL.mean().log10().getFloat())\n",
    "        }\n",
    "        \n",
    "    }\n",
    "    \n",
    "    println(\"l1 norm of w: \" + params.get(0).abs().sum());\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training without Regularization\n",
    "\n",
    "We now run this code with `lambd = 0`, \n",
    "disabling weight decay.\n",
    "Note that we overfit badly, \n",
    "decreasing the training error but not the \n",
    "test error---a textook case of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 norm of w: ND: () cpu() float32\n",
      "3.7243\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "   <div id=\"6HaWcA\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"loss\":[2.45698618888855,1.7351704835891724,1.0876202583312988,0.4989703595638275,-0.03784336894750595,-0.5275903940200806,-0.9685752987861633,-1.3661940097808838,-1.7289717197418213,-2.0659751892089844,-2.3864073753356934,-2.6957333087921143,-2.9978652000427246,-3.2948904037475586,-3.5885868072509766,-3.879488706588745,-4.168668270111084,-4.455600738525391,-4.740582466125488,-5.024204254150391,1.2936313152313232,0.6323086023330688,0.12694846093654633,-0.16504819691181183,-0.28135013580322266,-0.3161097764968872,-0.3253949284553528,-0.3273281455039978,-0.3276880383491516,-0.3276529014110565,-0.3275519013404846,-0.3275165557861328,-0.32755494117736816,-0.3275931775569916,-0.3276502788066864,-0.3276982307434082,-0.32774055004119873,-0.3277800381183624,-0.32781392335891724,-0.3278411030769348],\n",
       "\"lossLabel\":[\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\"],\n",
       "\"epochCount\":[5.0,10.0,15.0,20.0,25.0,30.0,35.0,40.0,45.0,50.0,55.0,60.0,65.0,70.0,75.0,80.0,85.0,90.0,95.0,100.0,5.0,10.0,15.0,20.0,25.0,30.0,35.0,40.0,45.0,50.0,55.0,60.0,65.0,70.0,75.0,80.0,85.0,90.0,95.0,100.0]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":500.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"epochCount\",\n",
       "\"y\":\"loss\",\n",
       "\"color\":\"lossLabel\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"6HaWcA\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(0f);\n",
    "\n",
    "val trainLabel = Array<String>(trainLoss.size) { \"train loss\" } \n",
    "//val accLabel = Array<String>(trainAccuracy.size) { \"train acc\" }\n",
    "val testLabel = Array<String>(testLoss.size) {\"test acc\"}\n",
    "\n",
    "val data = mapOf( \"epochCount\" to epochCount + epochCount,\n",
    "                \"loss\" to trainLoss + testLoss,\n",
    "                \"lossLabel\" to trainLabel + testLabel)\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine { x = \"epochCount\" ; y = \"loss\" ; color = \"lossLabel\"}\n",
    "plot + ggsize(500, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Weight Decay\n",
    "\n",
    "Below, we run with substantial weight decay.\n",
    "Note that the training error increases\n",
    "but the test error decreases.\n",
    "This is precisely the effect \n",
    "we expect from regularization.\n",
    "As an exercise, you might want to check\n",
    "that the $\\ell_2$ norm of the weights $\\mathbf{w}$\n",
    "has actually decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 norm of w: ND: () cpu() float32\n",
      "1.6956\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "   <div id=\"xjr4OD\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"loss\":[2.45698618888855,1.7351704835891724,1.0876202583312988,0.4989703595638275,-0.03784336894750595,-0.5275903940200806,-0.9685752987861633,-1.3661940097808838,-1.7289717197418213,-2.0659751892089844,-2.3864073753356934,-2.6957333087921143,-2.9978652000427246,-3.2948904037475586,-3.5885868072509766,-3.879488706588745,-4.168668270111084,-4.455600738525391,-4.740582466125488,-5.024204254150391,2.292177677154541,1.4052335023880005,0.5897583961486816,-0.17456287145614624,-0.8987829685211182,-1.5223145484924316,-1.8801801204681396,-1.9591631889343262,-1.9496854543685913,-1.9348138570785522,-1.9289648532867432,-1.9309953451156616,-1.935986042022705,-1.9452732801437378,-1.955380916595459,-1.9658442735671997,-1.9778594970703125,-1.9890981912612915,-2.0018956661224365,-2.0133142471313477,1.2936313152313232,0.6323086023330688,0.12694846093654633,-0.16504819691181183,-0.28135013580322266,-0.3161097764968872,-0.3253949284553528,-0.3273281455039978,-0.3276880383491516,-0.3276529014110565,-0.3275519013404846,-0.3275165557861328,-0.32755494117736816,-0.3275931775569916,-0.3276502788066864,-0.3276982307434082,-0.32774055004119873,-0.3277800381183624,-0.32781392335891724,-0.3278411030769348,1.1285041570663452,0.3155016303062439,-0.26077303290367126,-0.507424533367157,-0.572712779045105,-0.5931220054626465,-0.6060830354690552,-0.6182789206504822,-0.6302817463874817,-0.6425008773803711,-0.6548064947128296,-0.6672958731651306,-0.679529070854187,-0.6918909549713135,-0.7040820717811584,-0.716304361820221,-0.7286391854286194,-0.740622878074646,-0.7527855634689331,-0.7650966048240662],\n",
       "\"lossLabel\":[\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\"],\n",
       "\"epochCount\":[5.0,10.0,15.0,20.0,25.0,30.0,35.0,40.0,45.0,50.0,55.0,60.0,65.0,70.0,75.0,80.0,85.0,90.0,95.0,100.0,5.0,10.0,15.0,20.0,25.0,30.0,35.0,40.0,45.0,50.0,55.0,60.0,65.0,70.0,75.0,80.0,85.0,90.0,95.0,100.0,5.0,10.0,15.0,20.0,25.0,30.0,35.0,40.0,45.0,50.0,55.0,60.0,65.0,70.0,75.0,80.0,85.0,90.0,95.0,100.0,5.0,10.0,15.0,20.0,25.0,30.0,35.0,40.0,45.0,50.0,55.0,60.0,65.0,70.0,75.0,80.0,85.0,90.0,95.0,100.0]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":500.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"epochCount\",\n",
       "\"y\":\"loss\",\n",
       "\"color\":\"lossLabel\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"xjr4OD\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// calling training with weight decay lambda = 3.0\n",
    "train(3f);\n",
    "\n",
    "val trainLabel = Array<String>(trainLoss.size) { \"train loss\" } \n",
    "//val accLabel = Array<String>(trainAccuracy.size) { \"train acc\" }\n",
    "val testLabel = Array<String>(testLoss.size) {\"test acc\"}\n",
    "\n",
    "val data = mapOf( \"epochCount\" to epochCount + epochCount,\n",
    "                \"loss\" to trainLoss + testLoss,\n",
    "                \"lossLabel\" to trainLabel + testLabel)\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine { x = \"epochCount\" ; y = \"loss\" ; color = \"lossLabel\"}\n",
    "plot + ggsize(500, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concise Implementation\n",
    "\n",
    "Because weight decay is ubiquitous \n",
    "in neural network optimization,\n",
    "DJL makes it especially convenient,\n",
    "integrating weight decay into the optimization algorithm itself\n",
    "for easy use in combination with any loss function.\n",
    "Moreover, this integration serves a computational benefit,\n",
    "allowing implementation tricks to add weight decay to the algorithm,\n",
    "without any additional computational overhead.\n",
    "Since the weight decay portion of the update\n",
    "depends only on the current value of each parameter,\n",
    "and the optimizer must touch each parameter once anyway.\n",
    "\n",
    "In the following code, we specify\n",
    "the weight decay hyperparameter directly\n",
    "through `wd` when instantiating our `Trainer`.\n",
    "By default, DJL decays both \n",
    "weights and biases simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "val epochCount = mutableListOf<Int>()\n",
    "val trainLoss = mutableListOf<Float>()\n",
    "val testLoss = mutableListOf<Float>()\n",
    "\n",
    "fun train_djl(wd: Float) {\n",
    "    \n",
    "    val initParams = InitParams();\n",
    "    \n",
    "    val params = NDList(initParams.getW(), initParams.getB());\n",
    "    \n",
    "    val numEpochs = Integer.getInteger(\"MAX_EPOCH\", 100);\n",
    "    val lr = 0.003f;\n",
    "    \n",
    "    val lrt = Tracker.fixed(lr);\n",
    "    val sgd = Optimizer.sgd().setLearningRateTracker(lrt).build();\n",
    "    \n",
    "    val config = DefaultTrainingConfig(l2loss)\n",
    "     .optOptimizer(sgd) // Optimizer (loss function)\n",
    "     .optDevices(Engine.getInstance().getDevices(1)) // single CPU/GPU\n",
    "     .addEvaluator(Accuracy()) // Model Accuracy\n",
    "     .addEvaluator(l2loss)\n",
    "     .addTrainingListeners(*TrainingListener.Defaults.logging()); // Logging\n",
    "    \n",
    "    val model = Model.newInstance(\"mlp\");\n",
    "\n",
    "    val net = SequentialBlock();\n",
    "    val linearBlock = Linear.builder().optBias(true).setUnits(1).build();\n",
    "    net.add(linearBlock);\n",
    "\n",
    "    model.setBlock(net);\n",
    "    val trainer = model.newTrainer(config);\n",
    "        \n",
    "    trainer.initialize(Shape(batchSize.toLong(), 2));\n",
    "    for (epoch in 1..numEpochs){\n",
    "        \n",
    "        for (batch in trainer.iterateDataset(trainIter)){\n",
    "            \n",
    "            val X = batch.getData().head();\n",
    "            val y = batch.getLabels().head();\n",
    "            \n",
    "             val w = params.get(0);\n",
    "             val b = params.get(1);\n",
    "            \n",
    "            Engine.getInstance().newGradientCollector().use { gc ->\n",
    "                // Minibatch loss in X and y\n",
    "                val l = Training.squaredLoss(Training.linreg(X, w, b), y).add(l2Penalty(w).mul(wd));\n",
    "                gc.backward(l);  // Compute gradient on l with respect to w and b\n",
    "                \n",
    "            }\n",
    "            batch.close();\n",
    "            for(param in params) {\n",
    "                sgd.update(param.toString(), param, param.getGradient().div(batchSize))\n",
    "            }\n",
    "//            Training.sgd(params, lr, batchSize);  // Update parameters using their gradient\n",
    "        }\n",
    "        \n",
    "        if(epoch % 5 == 0){\n",
    "            val testL = Training.squaredLoss(Training.linreg(testData.getX(), params.get(0), params.get(1)), testData.getY());\n",
    "            val trainL = Training.squaredLoss(Training.linreg(trainData.getX(), params.get(0), params.get(1)), trainData.getY());\n",
    "            \n",
    "            epochCount.add(epoch)  \n",
    "            trainLoss.add(trainL.mean().log10().getFloat())\n",
    "            testLoss.add(testL.mean().log10().getFloat())\n",
    "        }\n",
    "        \n",
    "    }\n",
    "    println(\"l1 norm of w: \" + params.get(0).abs().sum());\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots look identical to those when \n",
    "we implemented weight decay from scratch.\n",
    "However, they run appreciably faster \n",
    "and are easier to implement,\n",
    "a benefit that will become more\n",
    "pronounced for large problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 norm of w: ND: () cpu() float32\n",
      "3.7242\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "   <div id=\"Q069Op\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"loss\":[2.45812726020813,1.7337517738342285,1.0854029655456543,0.49866706132888794,-0.03840746358036995,-0.5290588140487671,-0.9729793071746826,-1.3723084926605225,-1.7355095148086548,-2.073148488998413,-2.3940391540527344,-2.7032949924468994,-3.0055699348449707,-3.303210973739624,-3.597486972808838,-3.888995409011841,-4.178289890289307,-4.465365886688232,-4.750875473022461,-5.034933090209961,1.2946192026138306,0.6325108408927917,0.1284400224685669,-0.1652105152606964,-0.282325804233551,-0.3169380724430084,-0.32559654116630554,-0.32755500078201294,-0.32783910632133484,-0.3277163803577423,-0.32762768864631653,-0.3275965750217438,-0.32760101556777954,-0.32763028144836426,-0.32766416668891907,-0.32770729064941406,-0.32774850726127625,-0.3277870714664459,-0.32781755924224854,-0.32784295082092285],\n",
       "\"lossLabel\":[\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\"],\n",
       "\"epochCount\":[5.0,10.0,15.0,20.0,25.0,30.0,35.0,40.0,45.0,50.0,55.0,60.0,65.0,70.0,75.0,80.0,85.0,90.0,95.0,100.0,5.0,10.0,15.0,20.0,25.0,30.0,35.0,40.0,45.0,50.0,55.0,60.0,65.0,70.0,75.0,80.0,85.0,90.0,95.0,100.0]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":500.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"epochCount\",\n",
       "\"y\":\"loss\",\n",
       "\"color\":\"lossLabel\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"Q069Op\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_djl(0f);\n",
    "\n",
    "val trainLabel = Array<String>(trainLoss.size) { \"train loss\" } \n",
    "//val accLabel = Array<String>(trainAccuracy.size) { \"train acc\" }\n",
    "val testLabel = Array<String>(testLoss.size) {\"test acc\"}\n",
    "\n",
    "val data = mapOf( \"epochCount\" to epochCount + epochCount,\n",
    "                \"loss\" to trainLoss + testLoss,\n",
    "                \"lossLabel\" to trainLabel + testLabel)\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine { x = \"epochCount\" ; y = \"loss\" ; color = \"lossLabel\"}\n",
    "plot + ggsize(500, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 norm of w: ND: () cpu() float32\n",
      "0.5661\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "   <div id=\"MCi8X7\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"loss\":[2.45812726020813,1.7337517738342285,1.0854029655456543,0.49866706132888794,-0.03840746358036995,-0.5290588140487671,-0.9729793071746826,-1.3723084926605225,-1.7355095148086548,-2.073148488998413,-2.3940391540527344,-2.7032949924468994,-3.0055699348449707,-3.303210973739624,-3.597486972808838,-3.888995409011841,-4.178289890289307,-4.465365886688232,-4.750875473022461,-5.034933090209961,1.8992106914520264,0.6156925559043884,-0.6019889116287231,-1.4893308877944946,-1.6136950254440308,-1.5996795892715454,-1.6082741022109985,-1.626326560974121,-1.6502324342727661,-1.6764410734176636,-1.70091712474823,-1.7279562950134277,-1.752838134765625,-1.777573585510254,-1.801736831665039,-1.8252660036087036,-1.8523499965667725,-1.8747233152389526,-1.9033645391464233,-1.9267927408218384,1.2946192026138306,0.6325108408927917,0.1284400224685669,-0.1652105152606964,-0.282325804233551,-0.3169380724430084,-0.32559654116630554,-0.32755500078201294,-0.32783910632133484,-0.3277163803577423,-0.32762768864631653,-0.3275965750217438,-0.32760101556777954,-0.32763028144836426,-0.32766416668891907,-0.32770729064941406,-0.32774850726127625,-0.3277870714664459,-0.32781755924224854,-0.32784295082092285,0.7359620928764343,-0.38570865988731384,-0.851492702960968,-0.9159529209136963,-0.9400899410247803,-0.9641046524047852,-0.9885772466659546,-1.013414978981018,-1.037851095199585,-1.0619385242462158,-1.0868083238601685,-1.1107370853424072,-1.1345362663269043,-1.1573452949523926,-1.1809803247451782,-1.2040070295333862,-1.2274476289749146,-1.2501927614212036,-1.2731505632400513,-1.295182704925537],\n",
       "\"lossLabel\":[\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"train loss\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\",\"test acc\"],\n",
       "\"epochCount\":[5.0,10.0,15.0,20.0,25.0,30.0,35.0,40.0,45.0,50.0,55.0,60.0,65.0,70.0,75.0,80.0,85.0,90.0,95.0,100.0,5.0,10.0,15.0,20.0,25.0,30.0,35.0,40.0,45.0,50.0,55.0,60.0,65.0,70.0,75.0,80.0,85.0,90.0,95.0,100.0,5.0,10.0,15.0,20.0,25.0,30.0,35.0,40.0,45.0,50.0,55.0,60.0,65.0,70.0,75.0,80.0,85.0,90.0,95.0,100.0,5.0,10.0,15.0,20.0,25.0,30.0,35.0,40.0,45.0,50.0,55.0,60.0,65.0,70.0,75.0,80.0,85.0,90.0,95.0,100.0]\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500.0,\n",
       "\"height\":500.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"mapping\":{\n",
       "\"x\":\"epochCount\",\n",
       "\"y\":\"loss\",\n",
       "\"color\":\"lossLabel\"\n",
       "},\n",
       "\"stat\":\"identity\",\n",
       "\"position\":\"identity\",\n",
       "\"geom\":\"line\",\n",
       "\"data\":{\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"MCi8X7\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_djl(10f);\n",
    "\n",
    "val trainLabel = Array<String>(trainLoss.size) { \"train loss\" } \n",
    "//val accLabel = Array<String>(trainAccuracy.size) { \"train acc\" }\n",
    "val testLabel = Array<String>(testLoss.size) {\"test acc\"}\n",
    "\n",
    "val data = mapOf( \"epochCount\" to epochCount + epochCount,\n",
    "                \"loss\" to trainLoss + testLoss,\n",
    "                \"lossLabel\" to trainLabel + testLabel)\n",
    "var plot = letsPlot(data)\n",
    "plot += geomLine { x = \"epochCount\" ; y = \"loss\" ; color = \"lossLabel\"}\n",
    "plot + ggsize(500, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we only touched upon one notion of\n",
    "what constitutes a simple *linear* function.\n",
    "Moreover, what constitutes a simple *nonlinear* function\n",
    "can be an even more complex question.\n",
    "For instance, [Reproducing Kernel Hilbert Spaces (RKHS)](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space)\n",
    "allows one to apply tools introduced \n",
    "for linear functions in a nonlinear context.\n",
    "Unfortunately, RKHS-based algorithms\n",
    "tend to scale purely to large, high-dimensional data.\n",
    "In this book we will default to the simple heuristic\n",
    "of applying weight decay on all layers of a deep network.\n",
    "\n",
    "## Summary\n",
    "\n",
    "* Regularization is a common method for dealing with overfitting. It adds a penalty term to the loss function on the training set to reduce the complexity of the learned model.\n",
    "* One particular choice for keeping the model simple is weight decay using an $\\ell_2$ penalty. This leads to weight decay in the update steps of the learning algorithm.\n",
    "* DJL provides automatic weight decay functionality in the optimizer by setting the hyperparameter `wd`.\n",
    "* You can have different optimizers within the same training loop, e.g., for different sets of parameters.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Experiment with the value of $\\lambda$ in the estimation problem in this page. Plot training and test accuracy as a function of $\\lambda$. What do you observe?\n",
    "1. Use a validation set to find the optimal value of $\\lambda$. Is it really the optimal value? Does this matter?\n",
    "1. What would the update equations look like if instead of $\\|\\mathbf{w}\\|^2$ we used $\\sum_i |w_i|$ as our penalty of choice (this is called $\\ell_1$ regularization).\n",
    "1. We know that $\\|\\mathbf{w}\\|^2 = \\mathbf{w}^\\top \\mathbf{w}$. Can you find a similar equation for matrices (mathematicians call this the [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm))?\n",
    "1. Review the relationship between training error and generalization error. In addition to weight decay, increased training, and the use of a model of suitable complexity, what other ways can you think of to deal with overfitting?\n",
    "1. In Bayesian statistics we use the product of prior and likelihood to arrive at a posterior via $P(w \\mid x) \\propto P(x \\mid w) P(w)$. How can you identify $P(w)$ with regularization?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
